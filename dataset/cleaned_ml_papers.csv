chunk,id,title,summary,cleaned_summary,source,authors,text_length,keywords_using_bert,keywords_using_togetherai,keywords_human_annotated
"DistilBERT, a distilled version of BERT: smaller,
faster, cheaper and lighter
Victor SANH, Lysandre DEBUT, Julien CHAUMOND, Thom",1910.01108,"DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter","As Transfer Learning from large-scale pre-trained models becomes more
prevalent in Natural Language Processing (NLP), operating these large models in
on-the-edge and/or under constrained computational training or inference
budgets remains challenging. In this work, we propose a method to pre-train a
smaller general-purpose language representation model, called DistilBERT, which
can then be fine-tuned with good performances on a wide range of tasks like its
larger counterparts. While most prior work investigated the use of distillation
for building task-specific models, we leverage knowledge distillation during
the pre-training phase and show that it is possible to reduce the size of a
BERT model by 40%, while retaining 97% of its language understanding
capabilities and being 60% faster. To leverage the inductive biases learned by
larger models during pre-training, we introduce a triple loss combining
language modeling, distillation and cosine-distance losses. Our smaller, faster
and lighter model is cheaper to pre-train and we demonstrate its capabilities
for on-device computations in a proof-of-concept experiment and a comparative
on-device study.","As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",http://arxiv.org/pdf/1910.01108,"['Victor Sanh', 'Lysandre Debut', 'Julien Chaumond', 'Thomas Wolf']",128,,,
"Constructing Datasets
for Multi-hop Reading Comprehension Across Documents
Johannes Welbl1Pontus Stenetorp1Sebastian Riedel1;2
1",1710.06481,Constructing Datasets for Multi-hop Reading Comprehension Across Documents,"Most Reading Comprehension methods limit themselves to queries which can be
answered using a single sentence, paragraph, or document. Enabling models to
combine disjoint pieces of textual evidence would extend the scope of machine
comprehension methods, but currently there exist no resources to train and test
this capability. We propose a novel task to encourage the development of models
for text understanding across multiple documents and to investigate the limits
of existing methods. In our task, a model learns to seek and combine evidence -
effectively performing multi-hop (alias multi-step) inference. We devise a
methodology to produce datasets for this task, given a collection of
query-answer pairs and thematically linked documents. Two datasets from
different domains are induced, and we identify potential pitfalls and devise
circumvention strategies. We evaluate two previously proposed competitive
models and find that one can integrate information across documents. However,
both models struggle to select relevant information, as providing documents
guaranteed to be relevant greatly improves their performance. While the models
outperform several strong baselines, their best accuracy reaches 42.9% compared
to human performance at 74.0% - leaving ample room for improvement.","Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement.",http://arxiv.org/pdf/1710.06481,"['Johannes Welbl', 'Pontus Stenetorp', 'Sebastian Riedel']",128,,,
"Proximal Policy Optimization Algorithms
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov
OpenAI
{joschu",1707.06347,Proximal Policy Optimization Algorithms,"We propose a new family of policy gradient methods for reinforcement
learning, which alternate between sampling data through interaction with the
environment, and optimizing a ""surrogate"" objective function using stochastic
gradient ascent. Whereas standard policy gradient methods perform one gradient
update per data sample, we propose a novel objective function that enables
multiple epochs of minibatch updates. The new methods, which we call proximal
policy optimization (PPO), have some of the benefits of trust region policy
optimization (TRPO), but they are much simpler to implement, more general, and
have better sample complexity (empirically). Our experiments test PPO on a
collection of benchmark tasks, including simulated robotic locomotion and Atari
game playing, and we show that PPO outperforms other online policy gradient
methods, and overall strikes a favorable balance between sample complexity,
simplicity, and wall-time.","We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",http://arxiv.org/pdf/1707.06347,"['John Schulman', 'Filip Wolski', 'Prafulla Dhariwal', 'Alec Radford', 'Oleg Klimov']",128,,,
"Can Automatic Post-Editing Improve NMT?
Shamil Chollampatt
Rakuten, Inc.
shamil.chollampatt@rakuten.comRaymond Hendy Susanto
Rak",2009.14395,Can Automatic Post-Editing Improve NMT?,"Automatic post-editing (APE) aims to improve machine translations, thereby
reducing human post-editing effort. APE has had notable success when used with
statistical machine translation (SMT) systems but has not been as successful
over neural machine translation (NMT) systems. This has raised questions on the
relevance of APE task in the current scenario. However, the training of APE
models has been heavily reliant on large-scale artificial corpora combined with
only limited human post-edited data. We hypothesize that APE models have been
underperforming in improving NMT translations due to the lack of adequate
supervision. To ascertain our hypothesis, we compile a larger corpus of human
post-edits of English to German NMT. We empirically show that a state-of-art
neural APE model trained on this corpus can significantly improve a strong
in-domain NMT system, challenging the current understanding in the field. We
further investigate the effects of varying training data sizes, using
artificial training data, and domain specificity for the APE task. We release
this new corpus under CC BY-NC-SA 4.0 license at
https://github.com/shamilcm/pedra.","Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.",http://arxiv.org/pdf/2009.14395,"['Shamil Chollampatt', 'Raymond Hendy Susanto', 'Liling Tan', 'Ewa Szymanska']",128,,,
"arXiv:1706.05125v1  [cs.AI]  16 Jun 2017Deal or No Deal? End-to-End Learning for Negotiation Dialog ues
Mike Lewis1, Denis Yarat",1706.05125,Deal or No Deal? End-to-End Learning for Negotiation Dialogues,"Much of human dialogue occurs in semi-cooperative settings, where agents with
different goals attempt to agree on common decisions. Negotiations require
complex communication and reasoning skills, but success is easy to measure,
making this an interesting task for AI. We gather a large dataset of
human-human negotiations on a multi-issue bargaining task, where agents who
cannot observe each other's reward functions must reach an agreement (or a
deal) via natural language dialogue. For the first time, we show it is possible
to train end-to-end models for negotiation, which must learn both linguistic
and reasoning skills with no annotated dialogue states. We also introduce
dialogue rollouts, in which the model plans ahead by simulating possible
complete continuations of the conversation, and find that this technique
dramatically improves performance. Our code and dataset are publicly available
(https://github.com/facebookresearch/end-to-end-negotiator).","Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).",http://arxiv.org/pdf/1706.05125,"['Mike Lewis', 'Denis Yarats', 'Yann N. Dauphin', 'Devi Parikh', 'Dhruv Batra']",128,,,
"Aligning Books and Movies: Towards Story-like Visual Explanations by
Watching Movies and Reading Books
Yukun Zhu;1Ryan Kiros*,1",1506.06724,Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books,"Books are a rich source of both fine-grained information, how a character, an
object or a scene looks like, as well as high-level semantics, what someone is
thinking, feeling and how these states evolve through a story. This paper aims
to align books to their movie releases in order to provide rich descriptive
explanations for visual content that go semantically far beyond the captions
available in current datasets. To align movies and books we exploit a neural
sentence embedding that is trained in an unsupervised way from a large corpus
of books, as well as a video-text neural embedding for computing similarities
between movie clips and sentences in the book. We propose a context-aware CNN
to combine information from multiple sources. We demonstrate good quantitative
performance for movie/book alignment and show several qualitative examples that
showcase the diversity of tasks our model can be used for.","Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",http://arxiv.org/pdf/1506.06724,"['Yukun Zhu', 'Ryan Kiros', 'Richard Zemel', 'Ruslan Salakhutdinov', 'Raquel Urtasun', 'Antonio Torralba', 'Sanja Fidler']",128,,,
"Published as a conference paper at ICLR 2022
POISONING AND BACKDOORING CONTRASTIVE LEARNING
Nicholas Carlini
GoogleAndreas Terzi",2106.09667,Poisoning and Backdooring Contrastive Learning,"Multimodal contrastive learning methods like CLIP train on noisy and
uncurated training datasets. This is cheaper than labeling datasets manually,
and even improves out-of-distribution robustness. We show that this practice
makes backdoor and poisoning attacks a significant threat. By poisoning just
0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual
Captions dataset), we can cause the model to misclassify test images by
overlaying a small patch. Targeted poisoning attacks, whereby the model
misclassifies a particular test input with an adversarially-desired label, are
even easier requiring control of 0.0001% of the dataset (e.g., just three out
of the 3 million images). Our attacks call into question whether training on
noisy and uncurated Internet scrapes is desirable.","Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",http://arxiv.org/pdf/2106.09667,"['Nicholas Carlini', 'Andreas Terzis']",128,,,
"Demoting Racial Bias in Hate Speech Detection
Mengzhou Xia Anjalie Field Yulia Tsvetkov
Language Technologies Institute
Carnegie",2005.12246,Demoting Racial Bias in Hate Speech Detection,"In current hate speech datasets, there exists a high correlation between
annotators' perceptions of toxicity and signals of African American English
(AAE). This bias in annotated training data and the tendency of machine
learning models to amplify it cause AAE text to often be mislabeled as
abusive/offensive/hate speech with a high false positive rate by current hate
speech classifiers. In this paper, we use adversarial training to mitigate this
bias, introducing a hate speech classifier that learns to detect toxic
sentences while demoting confounds corresponding to AAE texts. Experimental
results on a hate speech dataset and an AAE dataset suggest that our method is
able to substantially reduce the false positive rate for AAE text while only
minimally affecting the performance of hate speech classification.","In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.",http://arxiv.org/pdf/2005.12246,"['Mengzhou Xia', 'Anjalie Field', 'Yulia Tsvetkov']",128,,,
"arXiv:2008.02637v1  [cs.CL]  6 Aug 2020Question and Answer Test-Train Overlap in Open-Domain Ques tion
Answering Datasets
Patric",2008.02637,Question and Answer Test-Train Overlap in Open-Domain Question Answering Datasets,"Ideally Open-Domain Question Answering models should exhibit a number of
competencies, ranging from simply memorizing questions seen at training time,
to answering novel question formulations with answers seen during training, to
generalizing to completely novel questions with novel answers. However, single
aggregated test set scores do not show the full picture of what capabilities
models truly have. In this work, we perform a detailed study of the test sets
of three popular open-domain benchmark datasets with respect to these
competencies. We find that 60-70% of test-time answers are also present
somewhere in the training sets. We also find that 30% of test-set questions
have a near-duplicate paraphrase in their corresponding training sets. Using
these findings, we evaluate a variety of popular open-domain models to obtain
greater insight into what extent they can actually generalize, and what drives
their overall performance. We find that all models perform dramatically worse
on questions that cannot be memorized from training sets, with a mean absolute
performance difference of 63% between repeated and non-repeated data. Finally
we show that simple nearest-neighbor models out-perform a BART closed-book QA
model, further highlighting the role that training set memorization plays in
these benchmarks","Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models out-perform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks",http://arxiv.org/pdf/2008.02637,"['Patrick Lewis', 'Pontus Stenetorp', 'Sebastian Riedel']",128,,,
"Mitigating Statistical Bias within Differentially Private Synthetic Data
Sahra Ghalebikesabi1Harrison Wilde2Jack Jewson3Arnaud D",2108.10934,Mitigating Statistical Bias within Differentially Private Synthetic Data,"Increasing interest in privacy-preserving machine learning has led to new and
evolved approaches for generating private synthetic data from undisclosed real
data. However, mechanisms of privacy preservation can significantly reduce the
utility of synthetic data, which in turn impacts downstream tasks such as
learning predictive models or inference. We propose several re-weighting
strategies using privatised likelihood ratios that not only mitigate
statistical bias of downstream estimators but also have general applicability
to differentially private generative models. Through large-scale empirical
evaluation, we show that private importance weighting provides simple and
effective privacy-compliant augmentation for general applications of synthetic
data.","Increasing interest in privacy-preserving machine learning has led to new and evolved approaches for generating private synthetic data from undisclosed real data. However, mechanisms of privacy preservation can significantly reduce the utility of synthetic data, which in turn impacts downstream tasks such as learning predictive models or inference. We propose several re-weighting strategies using privatised likelihood ratios that not only mitigate statistical bias of downstream estimators but also have general applicability to differentially private generative models. Through large-scale empirical evaluation, we show that private importance weighting provides simple and effective privacy-compliant augmentation for general applications of synthetic data.",http://arxiv.org/pdf/2108.10934,"['Sahra Ghalebikesabi', 'Harrison Wilde', 'Jack Jewson', 'Arnaud Doucet', 'Sebastian Vollmer', 'Chris Holmes']",128,,,
"Measuring the Reliability of Hate Speech Annotations:
The Case of the European Refugee Crisis
Bj¨orn Ross Michael Rist Guillermo",1701.08118,Measuring the Reliability of Hate Speech Annotations: The Case of the European Refugee Crisis,"Some users of social media are spreading racist, sexist, and otherwise
hateful content. For the purpose of training a hate speech detection system,
the reliability of the annotations is crucial, but there is no universally
agreed-upon definition. We collected potentially hateful messages and asked two
groups of internet users to determine whether they were hate speech or not,
whether they should be banned or not and to rate their degree of offensiveness.
One of the groups was shown a definition prior to completing the survey. We
aimed to assess whether hate speech can be annotated reliably, and the extent
to which existing definitions are in accordance with subjective ratings. Our
results indicate that showing users a definition caused them to partially align
their own opinion with the definition but did not improve reliability, which
was very low overall. We conclude that the presence of hate speech should
perhaps not be considered a binary yes-or-no decision, and raters need more
detailed instructions for the annotation.","Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.",http://arxiv.org/pdf/1701.08118,"['Björn Ross', 'Michael Rist', 'Guillermo Carbonell', 'Benjamin Cabrera', 'Nils Kurowsky', 'Michael Wojatzki']",128,,,
"DetectGPT: Zero-Shot Machine-Generated Text Detection
using Probability Curvature
Eric Mitchell1Yoonho Lee1Alexander Khazatsky1C",2301.11305,DetectGPT: Zero-Shot Machine-Generated Text Detection using Probability Curvature,"The increasing fluency and widespread usage of large language models (LLMs)
highlight the desirability of corresponding tools aiding detection of
LLM-generated text. In this paper, we identify a property of the structure of
an LLM's probability function that is useful for such detection. Specifically,
we demonstrate that text sampled from an LLM tends to occupy negative curvature
regions of the model's log probability function. Leveraging this observation,
we then define a new curvature-based criterion for judging if a passage is
generated from a given LLM. This approach, which we call DetectGPT, does not
require training a separate classifier, collecting a dataset of real or
generated passages, or explicitly watermarking generated text. It uses only log
probabilities computed by the model of interest and random perturbations of the
passage from another generic pre-trained language model (e.g., T5). We find
DetectGPT is more discriminative than existing zero-shot methods for model
sample detection, notably improving detection of fake news articles generated
by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline
to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code,
data, and other project information.","The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",http://arxiv.org/pdf/2301.11305,"['Eric Mitchell', 'Yoonho Lee', 'Alexander Khazatsky', 'Christopher D. Manning', 'Chelsea Finn']",128,,,
"October 14, 2022
TEXT AND PATTERNS : FOREFFECTIVE CHAIN OF THOUGHT
ITTAKES TWO TO TANGO
Aman Madaanand Amir Yazdanbakhsh
Carne",2209.07686,"Text and Patterns: For Effective Chain of Thought, It Takes Two to Tango","The past decade has witnessed dramatic gains in natural language processing
and an unprecedented scaling of large language models. These developments have
been accelerated by the advent of few-shot techniques such as chain of thought
(CoT) prompting. Specifically, CoT pushes the performance of large language
models in a few-shot setup by augmenting the prompts with intermediate steps.
Despite impressive results across various tasks, the reasons behind their
success have not been explored. This work uses counterfactual prompting to
develop a deeper understanding of CoT-based few-shot prompting mechanisms in
large language models. We first systematically identify and define the key
components of a prompt: symbols, patterns, and text. Then, we devise and
conduct an exhaustive set of experiments across four different tasks, by
querying the model with counterfactual prompts where only one of these
components is altered. Our experiments across three models (PaLM, GPT-3, and
CODEX) reveal several surprising findings and brings into question the
conventional wisdom around few-shot prompting. First, the presence of factual
patterns in a prompt is practically immaterial to the success of CoT. Second,
our results conclude that the primary role of intermediate steps may not be to
facilitate learning how to solve a task. The intermediate steps are rather a
beacon for the model to realize what symbols to replicate in the output to form
a factual answer. Further, text imbues patterns with commonsense knowledge and
meaning. Our empirical and qualitative analysis reveals that a symbiotic
relationship between text and patterns explains the success of few-shot
prompting: text helps extract commonsense from the question to help patterns,
and patterns enforce task understanding and direct text generation.","The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.",http://arxiv.org/pdf/2209.07686,"['Aman Madaan', 'Amir Yazdanbakhsh']",128,,,
"Partitioned Variational Inference: A uniﬁed framework
encompassing federated and continual learning
Thang D. Bui1, Cuong V. Nguy",1811.11206,Partitioned Variational Inference: A unified framework encompassing federated and continual learning,"Variational inference (VI) has become the method of choice for fitting many
modern probabilistic models. However, practitioners are faced with a fragmented
literature that offers a bewildering array of algorithmic options. First, the
variational family. Second, the granularity of the updates e.g. whether the
updates are local to each data point and employ message passing or global.
Third, the method of optimization (bespoke or blackbox, closed-form or
stochastic updates, etc.). This paper presents a new framework, termed
Partitioned Variational Inference (PVI), that explicitly acknowledges these
algorithmic dimensions of VI, unifies disparate literature, and provides
guidance on usage. Crucially, the proposed PVI framework allows us to identify
new ways of performing VI that are ideally suited to challenging learning
scenarios including federated learning (where distributed computing is
leveraged to process non-centralized data) and continual learning (where new
data and tasks arrive over time and must be accommodated quickly). We showcase
these new capabilities by developing communication-efficient federated training
of Bayesian neural networks and continual learning for Gaussian process models
with private pseudo-points. The new methods significantly outperform the
state-of-the-art, whilst being almost as straightforward to implement as
standard VI.","Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.",http://arxiv.org/pdf/1811.11206,"['Thang D. Bui', 'Cuong V. Nguyen', 'Siddharth Swaroop', 'Richard E. Turner']",128,,,
"Cross-lingual Language Model Pretraining
Guillaume Lample
Facebook AI Research
Sorbonne Universit ´es
glample@fb.comAlexis Conn",1901.07291,Cross-lingual Language Model Pretraining,"Recent studies have demonstrated the efficiency of generative pretraining for
English natural language understanding. In this work, we extend this approach
to multiple languages and show the effectiveness of cross-lingual pretraining.
We propose two methods to learn cross-lingual language models (XLMs): one
unsupervised that only relies on monolingual data, and one supervised that
leverages parallel data with a new cross-lingual language model objective. We
obtain state-of-the-art results on cross-lingual classification, unsupervised
and supervised machine translation. On XNLI, our approach pushes the state of
the art by an absolute gain of 4.9% accuracy. On unsupervised machine
translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the
previous state of the art by more than 9 BLEU. On supervised machine
translation, we obtain a new state of the art of 38.5 BLEU on WMT'16
Romanian-English, outperforming the previous best approach by more than 4 BLEU.
Our code and pretrained models will be made publicly available.","Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",http://arxiv.org/pdf/1901.07291,"['Guillaume Lample', 'Alexis Conneau']",128,,,
"LaMDA: Language Models for Dialog Applications
Romal Thoppilan Daniel De FreitasJamie Hall Noam ShazeerApoorv Kulshreshtha
Hen",2201.08239,LaMDA: Language Models for Dialog Applications,"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family
of Transformer-based neural language models specialized for dialog, which have
up to 137B parameters and are pre-trained on 1.56T words of public dialog data
and web text. While model scaling alone can improve quality, it shows less
improvements on safety and factual grounding. We demonstrate that fine-tuning
with annotated data and enabling the model to consult external knowledge
sources can lead to significant improvements towards the two key challenges of
safety and factual grounding. The first challenge, safety, involves ensuring
that the model's responses are consistent with a set of human values, such as
preventing harmful suggestions and unfair bias. We quantify safety using a
metric based on an illustrative set of human values, and we find that filtering
candidate responses using a LaMDA classifier fine-tuned with a small amount of
crowdworker-annotated data offers a promising approach to improving model
safety. The second challenge, factual grounding, involves enabling the model to
consult external knowledge sources, such as an information retrieval system, a
language translator, and a calculator. We quantify factuality using a
groundedness metric, and we find that our approach enables the model to
generate responses grounded in known sources, rather than responses that merely
sound plausible. Finally, we explore the use of LaMDA in the domains of
education and content recommendations, and analyze their helpfulness and role
consistency.","We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",http://arxiv.org/pdf/2201.08239,"['Romal Thoppilan', 'Daniel De Freitas', 'Jamie Hall', 'Noam Shazeer', 'Apoorv Kulshreshtha', 'Heng-Tze Cheng', 'Alicia Jin', 'Taylor Bos', 'Leslie Baker', 'Yu Du', 'YaGuang Li', 'Hongrae Lee', 'Huaixiu Steven Zheng', 'Amin Ghafouri', 'Marcelo Menegali', 'Yanping Huang', 'Maxim Krikun', 'Dmitry Lepikhin', 'James Qin', 'Dehao Chen', 'Yuanzhong Xu', 'Zhifeng Chen', 'Adam Roberts', 'Maarten Bosma', 'Vincent Zhao', 'Yanqi Zhou', 'Chung-Ching Chang', 'Igor Krivokon', 'Will Rusch', 'Marc Pickett', 'Pranesh Srinivasan', 'Laichee Man', 'Kathleen Meier-Hellstern', 'Meredith Ringel Morris', 'Tulsee Doshi', 'Renelito Delos Santos', 'Toju Duke', 'Johnny Soraker', 'Ben Zevenbergen', 'Vinodkumar Prabhakaran', 'Mark Diaz', 'Ben Hutchinson', 'Kristen Olson', 'Alejandra Molina', 'Erin Hoffman-John', 'Josh Lee', 'Lora Aroyo', 'Ravi Rajakumar', 'Alena Butryna', 'Matthew Lamm', 'Viktoriya Kuzmina', 'Joe Fenton', 'Aaron Cohen', 'Rachel Bernstein', 'Ray Kurzweil', 'Blaise Aguera-Arcas', 'Claire Cui', 'Marian Croak', 'Ed Chi', 'Quoc Le']",128,,,
"Rethinking with Retrieval: Faithful Large Language Model Inference
Hangfeng HeyHongming ZhangzDan Rothx
yUniversity of Rocheste",2301.00303,Rethinking with Retrieval: Faithful Large Language Model Inference,"Despite the success of large language models (LLMs) in various natural
language processing (NLP) tasks, the stored knowledge in these models may
inevitably be incomplete, out-of-date, or incorrect. This motivates the need to
utilize external knowledge to assist LLMs. Unfortunately, current methods for
incorporating external knowledge often require additional training or
fine-tuning, which can be costly and may not be feasible for LLMs. To address
this issue, we propose a novel post-processing approach, rethinking with
retrieval (RR), which retrieves relevant external knowledge based on the
decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting.
This lightweight approach does not require additional training or fine-tuning
and is not limited by the input length of LLMs. We evaluate the effectiveness
of RR through extensive experiments with GPT-3 on three complex reasoning
tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our
results show that RR can produce more faithful explanations and improve the
performance of LLMs.","Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",http://arxiv.org/pdf/2301.00303,"['Hangfeng He', 'Hongming Zhang', 'Dan Roth']",128,,,
"Is ChatGPT Good at Search?
Investigating Large Language Models as Re-Ranking Agent
Weiwei Sun1Lingyong Yan2Xinyu Ma2Pengjie Ren1",2304.09542,Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agent,"Large Language Models (LLMs) have demonstrated a remarkable ability to
generalize zero-shot to various language-related tasks. This paper focuses on
the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance
ranking in Information Retrieval (IR). Surprisingly, our experiments reveal
that properly instructed ChatGPT and GPT-4 can deliver competitive, even
superior results than supervised methods on popular IR benchmarks. Notably,
GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of
2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and
an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we
delve into the potential for distilling the ranking capabilities of ChatGPT
into a specialized model. Our small specialized model that trained on 10K
ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO
data on BEIR. The code to reproduce our results is available at
www.github.com/sunnweiwei/RankGPT","Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT",http://arxiv.org/pdf/2304.09542,"['Weiwei Sun', 'Lingyong Yan', 'Xinyu Ma', 'Pengjie Ren', 'Dawei Yin', 'Zhaochun Ren']",128,,,
"arXiv:1905.10044v1  [cs.CL]  24 May 2019BoolQ: Exploring the Surprising Difﬁculty of Natural Yes/N o Questions
Christopher Clark",1905.10044,BoolQ: Exploring the Surprising Difficulty of Natural Yes/No Questions,"In this paper we study yes/no questions that are naturally occurring ---
meaning that they are generated in unprompted and unconstrained settings. We
build a reading comprehension dataset, BoolQ, of such questions, and show that
they are unexpectedly challenging. They often query for complex, non-factoid
information, and require difficult entailment-like inference to solve. We also
explore the effectiveness of a range of transfer learning baselines. We find
that transferring from entailment data is more effective than transferring from
paraphrase or extractive QA data, and that it, surprisingly, continues to be
very beneficial even when starting from massive pre-trained language models
such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on
our train set. It achieves 80.4% accuracy compared to 90% accuracy of human
annotators (and 62% majority-baseline), leaving a significant gap for future
work.","In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",http://arxiv.org/pdf/1905.10044,"['Christopher Clark', 'Kenton Lee', 'Ming-Wei Chang', 'Tom Kwiatkowski', 'Michael Collins', 'Kristina Toutanova']",128,,,
"Behavior Cloned Transformers are Neurosymbolic Reasoners
Ruoyao Wangz, Peter Jansenz, Marc-Alexandre Côté|, Prithviraj Ammanabro",2210.07382,Behavior Cloned Transformers are Neurosymbolic Reasoners,"In this work, we explore techniques for augmenting interactive agents with
information from symbolic modules, much like humans use tools like calculators
and GPS systems to assist with arithmetic and navigation. We test our agent's
abilities in text games -- challenging benchmarks for evaluating the multi-step
reasoning abilities of game agents in grounded, language-based environments.
Our experimental study indicates that injecting the actions from these symbolic
modules into the action space of a behavior cloned transformer agent increases
performance on four text game benchmarks that test arithmetic, navigation,
sorting, and common sense reasoning by an average of 22%, allowing an agent to
reach the highest possible performance on unseen games. This action injection
technique is easily extended to new agents, environments, and symbolic modules.","In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent's abilities in text games -- challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules.",http://arxiv.org/pdf/2210.07382,"['Ruoyao Wang', 'Peter Jansen', 'Marc-Alexandre Côté', 'Prithviraj Ammanabrolu']",128,,,
"Negated and Misprimed Probes for Pretrained Language Models:
Birds Can Talk, But Cannot Fly
Nora Kassner, Hinrich Sch ¨utze
Cent",1911.03343,"Negated and Misprimed Probes for Pretrained Language Models: Birds Can Talk, But Cannot Fly","Building on Petroni et al. (2019), we propose two new probing tasks analyzing
factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We
find that PLMs do not distinguish between negated (""Birds cannot [MASK]"") and
non-negated (""Birds can [MASK]"") cloze questions. (2) Mispriming. Inspired by
priming methods in human psychology, we add ""misprimes"" to cloze questions
(""Talk? Birds can [MASK]""). We find that PLMs are easily distracted by
misprimes. These results suggest that PLMs still have a long way to go to
adequately learn human-like factual knowledge.","Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (Birds cannot [MASK]) and non-negated (Birds can [MASK]) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add misprimes to cloze questions (Talk? Birds can [MASK]). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",http://arxiv.org/pdf/1911.03343,"['Nora Kassner', 'Hinrich Schütze']",128,,,
"Published as a conference paper at ICLR 2023
SELF-CONSISTENCY IMPROVES CHAIN OF THOUGHT
REASONING IN LANGUAGE MODELS
Xuezhi Wang",2203.11171,Self-Consistency Improves Chain of Thought Reasoning in Language Models,"Chain-of-thought prompting combined with pre-trained large language models
has achieved encouraging results on complex reasoning tasks. In this paper, we
propose a new decoding strategy, self-consistency, to replace the naive greedy
decoding used in chain-of-thought prompting. It first samples a diverse set of
reasoning paths instead of only taking the greedy one, and then selects the
most consistent answer by marginalizing out the sampled reasoning paths.
Self-consistency leverages the intuition that a complex reasoning problem
typically admits multiple different ways of thinking leading to its unique
correct answer. Our extensive empirical evaluation shows that self-consistency
boosts the performance of chain-of-thought prompting with a striking margin on
a range of popular arithmetic and commonsense reasoning benchmarks, including
GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and
ARC-challenge (+3.9%).","Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",http://arxiv.org/pdf/2203.11171,"['Xuezhi Wang', 'Jason Wei', 'Dale Schuurmans', 'Quoc Le', 'Ed Chi', 'Sharan Narang', 'Aakanksha Chowdhery', 'Denny Zhou']",128,,,
"Published as a conference paper at ICLR 2023
ISREINFORCEMENT LEARNING (NOT)FOR NATURAL
LANGUAGE PROCESSING : BENCHMARKS , BASELI",2210.01241,"Is Reinforcement Learning (Not) for Natural Language Processing: Benchmarks, Baselines, and Building Blocks for Natural Language Policy Optimization","We tackle the problem of aligning pre-trained large language models (LMs)
with human preferences. If we view text generation as a sequential
decision-making problem, reinforcement learning (RL) appears to be a natural
conceptual framework. However, using RL for LM-based generation faces empirical
challenges, including training instability due to the combinatorial action
space, as well as a lack of open-source libraries and benchmarks customized for
LM alignment. Thus, a question rises in the research community: is RL a
practical paradigm for NLP?
  To help answer this, we first introduce an open-source modular library,
RL4LMs (Reinforcement Learning for Language Models), for optimizing language
generators with RL. The library consists of on-policy RL algorithms that can be
used to train any encoder or encoder-decoder LM in the HuggingFace library
(Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE
(General Reinforced-language Understanding Evaluation) benchmark, a set of 6
language generation tasks which are supervised not by target strings, but by
reward functions which capture automated measures of human preference.GRUE is
the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally,
we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language
Policy Optimization)} that learns to effectively reduce the combinatorial
action space in language generation. We show 1) that RL techniques are
generally better than supervised methods at aligning LMs to human preferences;
and 2) that NLPO exhibits greater stability and performance than previous
policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both
automatic and human evaluations.","We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?   To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",http://arxiv.org/pdf/2210.01241,"['Rajkumar Ramamurthy', 'Prithviraj Ammanabrolu', 'Kianté Brantley', 'Jack Hessel', 'Rafet Sifa', 'Christian Bauckhage', 'Hannaneh Hajishirzi', 'Yejin Choi']",128,,,
"Visualizing the Loss Landscape of Neural Nets
Hao Li1, Zheng Xu1, Gavin Taylor2, Christoph Studer3, Tom Goldstein1
1University o",1712.09913,Visualizing the Loss Landscape of Neural Nets,"Neural network training relies on our ability to find ""good"" minimizers of
highly non-convex loss functions. It is well-known that certain network
architecture designs (e.g., skip connections) produce loss functions that train
easier, and well-chosen training parameters (batch size, learning rate,
optimizer) produce minimizers that generalize better. However, the reasons for
these differences, and their effects on the underlying loss landscape, are not
well understood. In this paper, we explore the structure of neural loss
functions, and the effect of loss landscapes on generalization, using a range
of visualization methods. First, we introduce a simple ""filter normalization""
method that helps us visualize loss function curvature and make meaningful
side-by-side comparisons between loss functions. Then, using a variety of
visualizations, we explore how network architecture affects the loss landscape,
and how training parameters affect the shape of minimizers.","Neural network training relies on our ability to find good minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple filter normalization method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",http://arxiv.org/pdf/1712.09913,"['Hao Li', 'Zheng Xu', 'Gavin Taylor', 'Christoph Studer', 'Tom Goldstein']",128,,,
"CONDA QA: A Contrastive Reading Comprehension Dataset for
Reasoning about Negation
Abhilasha Ravichander
Carnegie Mellon Univer",2211.00295,CONDAQA: A Contrastive Reading Comprehension Dataset for Reasoning about Negation,"The full power of human language-based communication cannot be realized
without negation. All human languages have some form of negation. Despite this,
negation remains a challenging phenomenon for current natural language
understanding systems. To facilitate the future development of models that can
process negation effectively, we present CONDAQA, the first English reading
comprehension dataset which requires reasoning about the implications of
negated statements in paragraphs. We collect paragraphs with diverse negation
cues, then have crowdworkers ask questions about the implications of the
negated statement in the passage. We also have workers make three kinds of
edits to the passage -- paraphrasing the negated statement, changing the scope
of the negation, and reversing the negation -- resulting in clusters of
question-answer pairs that are difficult for models to answer with spurious
shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique
negation cues and is challenging for current state-of-the-art models. The best
performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our
consistency metric, well below human performance which is 81%. We release our
dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to
facilitate the development of future NLP methods that work on negated language.","The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present CONDAQA, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage -- paraphrasing the negated statement, changing the scope of the negation, and reversing the negation -- resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our consistency metric, well below human performance which is 81%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future NLP methods that work on negated language.",http://arxiv.org/pdf/2211.00295,"['Abhilasha Ravichander', 'Matt Gardner', 'Ana Marasović']",128,,,
"Chain-of-Thought Prompting Elicits Reasoning
in Large Language Models
Jason Wei Xuezhi Wang Dale Schuurmans Maarten Bosma
Brian ",2201.11903,Chain-of-Thought Prompting Elicits Reasoning in Large Language Models,"We explore how generating a chain of thought -- a series of intermediate
reasoning steps -- significantly improves the ability of large language models
to perform complex reasoning. In particular, we show how such reasoning
abilities emerge naturally in sufficiently large language models via a simple
method called chain of thought prompting, where a few chain of thought
demonstrations are provided as exemplars in prompting. Experiments on three
large language models show that chain of thought prompting improves performance
on a range of arithmetic, commonsense, and symbolic reasoning tasks. The
empirical gains can be striking. For instance, prompting a 540B-parameter
language model with just eight chain of thought exemplars achieves state of the
art accuracy on the GSM8K benchmark of math word problems, surpassing even
finetuned GPT-3 with a verifier.","We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",http://arxiv.org/pdf/2201.11903,"['Jason Wei', 'Xuezhi Wang', 'Dale Schuurmans', 'Maarten Bosma', 'Brian Ichter', 'Fei Xia', 'Ed Chi', 'Quoc Le', 'Denny Zhou']",128,,,
"2022-1-8
AndroidEnv: A Reinforcement Learning
Platform for Android
Daniel Toyama*,1, Philippe Hamel*,1, Anita Gergely*,1, Gheorg",2105.13231,AndroidEnv: A Reinforcement Learning Platform for Android,"We introduce AndroidEnv, an open-source platform for Reinforcement Learning
(RL) research built on top of the Android ecosystem. AndroidEnv allows RL
agents to interact with a wide variety of apps and services commonly used by
humans through a universal touchscreen interface. Since agents train on a
realistic simulation of an Android device, they have the potential to be
deployed on real devices. In this report, we give an overview of the
environment, highlighting the significant features it provides for research,
and we present an empirical evaluation of some popular reinforcement learning
agents on a set of tasks built on this platform.","We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.",http://arxiv.org/pdf/2105.13231,"['Daniel Toyama', 'Philippe Hamel', 'Anita Gergely', 'Gheorghe Comanici', 'Amelia Glaese', 'Zafarali Ahmed', 'Tyler Jackson', 'Shibl Mourad', 'Doina Precup']",128,,,
"Generating Fake Cyber Threat Intelligence
Using Transformer-Based Models
Priyanka Ranade, Aritran Piplai, Sudip Mittaly, Anupa",2102.04351,Generating Fake Cyber Threat Intelligence Using Transformer-Based Models,"Cyber-defense systems are being developed to automatically ingest Cyber
Threat Intelligence (CTI) that contains semi-structured data and/or text to
populate knowledge graphs. A potential risk is that fake CTI can be generated
and spread through Open-Source Intelligence (OSINT) communities or on the Web
to effect a data poisoning attack on these systems. Adversaries can use fake
CTI examples as training input to subvert cyber defense systems, forcing the
model to learn incorrect inputs to serve their malicious needs.
  In this paper, we automatically generate fake CTI text descriptions using
transformers. We show that given an initial prompt sentence, a public language
model like GPT-2 with fine-tuning, can generate plausible CTI text with the
ability of corrupting cyber-defense systems. We utilize the generated fake CTI
text to perform a data poisoning attack on a Cybersecurity Knowledge Graph
(CKG) and a cybersecurity corpus. The poisoning attack introduced adverse
impacts such as returning incorrect reasoning outputs, representation
poisoning, and corruption of other dependent AI-based cyber defense systems. We
evaluate with traditional approaches and conduct a human evaluation study with
cybersecurity professionals and threat hunters. Based on the study,
professional threat hunters were equally likely to consider our fake generated
CTI as true.","Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing the model to learn incorrect inputs to serve their malicious needs.   In this paper, we automatically generate fake CTI text descriptions using transformers. We show that given an initial prompt sentence, a public language model like GPT-2 with fine-tuning, can generate plausible CTI text with the ability of corrupting cyber-defense systems. We utilize the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The poisoning attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cybersecurity professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI as true.",http://arxiv.org/pdf/2102.04351,"['Priyanka Ranade', 'Aritran Piplai', 'Sudip Mittal', 'Anupam Joshi', 'Tim Finin']",128,,,
"PAL: Program-aided Language Models
Luyu Gao* 1Aman Madaan* 1Shuyan Zhou* 1Uri Alon1Pengfei Liu1 2Yiming Yang1Jamie Callan1
Graha",2211.10435,PAL: Program-aided Language Models,"Large language models (LLMs) have recently demonstrated an impressive ability
to perform arithmetic and symbolic reasoning tasks, when provided with a few
examples at test time (""few-shot prompting""). Much of this success can be
attributed to prompting methods such as ""chain-of-thought'', which employ LLMs
for both understanding the problem description by decomposing it into steps, as
well as solving each step of the problem. While LLMs seem to be adept at this
sort of step-by-step decomposition, LLMs often make logical and arithmetic
mistakes in the solution part, even when the problem is decomposed correctly.
In this paper, we present Program-Aided Language models (PAL): a novel approach
that uses the LLM to read natural language problems and generate programs as
the intermediate reasoning steps, but offloads the solution step to a runtime
such as a Python interpreter. With PAL, decomposing the natural language
problem into runnable steps remains the only learning task for the LLM, while
solving is delegated to the interpreter. We demonstrate this synergy between a
neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and
algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all
these natural language reasoning tasks, generating code using an LLM and
reasoning using a Python interpreter leads to more accurate results than much
larger models. For example, PAL using Codex achieves state-of-the-art few-shot
accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B
which uses chain-of-thought by absolute 15% top-1. Our code and data are
publicly available at http://reasonwithpal.com/ .","Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",http://arxiv.org/pdf/2211.10435,"['Luyu Gao', 'Aman Madaan', 'Shuyan Zhou', 'Uri Alon', 'Pengfei Liu', 'Yiming Yang', 'Jamie Callan', 'Graham Neubig']",128,,,
"ACL 2023
SELF-INSTRUCT : Aligning Language Models
with Self-Generated Instructions
Yizhong Wang♣Yeganeh Kordi♢Swaroop Mishra♡Ali",2212.1056,Self-Instruct: Aligning Language Models with Self-Generated Instructions,"Large ""instruction-tuned"" language models (i.e., finetuned to respond to
instructions) have demonstrated a remarkable ability to generalize zero-shot to
new tasks. Nevertheless, they depend heavily on human-written instruction data
that is often limited in quantity, diversity, and creativity, therefore
hindering the generality of the tuned model. We introduce Self-Instruct, a
framework for improving the instruction-following capabilities of pretrained
language models by bootstrapping off their own generations. Our pipeline
generates instructions, input, and output samples from a language model, then
filters invalid or similar ones before using them to finetune the original
model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute
improvement over the original model on Super-NaturalInstructions, on par with
the performance of InstructGPT-001, which was trained with private user data
and human annotations. For further evaluation, we curate a set of
expert-written instructions for novel tasks, and show through human evaluation
that tuning GPT3 with Self-Instruct outperforms using existing public
instruction datasets by a large margin, leaving only a 5% absolute gap behind
InstructGPT-001. Self-Instruct provides an almost annotation-free method for
aligning pre-trained language models with instructions, and we release our
large synthetic dataset to facilitate future studies on instruction tuning. Our
code and data are available at https://github.com/yizhongw/self-instruct.","Large instruction-tuned language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.",http://arxiv.org/pdf/2212.10560,"['Yizhong Wang', 'Yeganeh Kordi', 'Swaroop Mishra', 'Alisa Liu', 'Noah A. Smith', 'Daniel Khashabi', 'Hannaneh Hajishirzi']",128,,,
"Language Generation Models Can Cause Harm:
So What Can We Do About It? An Actionable Survey
Sachin Kumar;|Vidhisha Balachandran",2210.077,Language Generation Models Can Cause Harm: So What Can We Do About It? An Actionable Survey,"Recent advances in the capacity of large language models to generate
human-like text have resulted in their increased adoption in user-facing
settings. In parallel, these improvements have prompted a heated discourse
around the risks of societal harms they introduce, whether inadvertent or
malicious. Several studies have explored these harms and called for their
mitigation via development of safer, fairer models. Going beyond enumerating
the risks of harms, this work provides a survey of practical methods for
addressing potential threats and societal harms from language generation
models. We draw on several prior works' taxonomies of language model risks to
present a structured overview of strategies for detecting and ameliorating
different kinds of risks/harms of language generators. Bridging diverse strands
of research, this survey aims to serve as a practical guide for both LM
researchers and practitioners, with explanations of different mitigation
strategies' motivations, their limitations, and open problems for future
research.","Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works' taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different mitigation strategies' motivations, their limitations, and open problems for future research.",http://arxiv.org/pdf/2210.07700,"['Sachin Kumar', 'Vidhisha Balachandran', 'Lucille Njoo', 'Antonios Anastasopoulos', 'Yulia Tsvetkov']",128,,,
"Learning from the Worst: Dynamically Generated Datasets
to Improve Online Hate Detection
Bertie Vidgeny, Tristan Thrushz, Zeerak",2012.15761,Learning from the Worst: Dynamically Generated Datasets to Improve Online Hate Detection,"We present a human-and-model-in-the-loop process for dynamically generating
datasets and training better performing and more robust hate detection models.
We provide a new dataset of ~40,000 entries, generated and labelled by trained
annotators over four rounds of dynamic data creation. It includes ~15,000
challenging perturbations and each hateful entry has fine-grained labels for
the type and target of hate. Hateful entries make up 54% of the dataset, which
is substantially higher than comparable datasets. We show that model
performance is substantially improved using this approach. Models trained on
later rounds of data collection perform better on test sets and are harder for
annotators to trick. They also perform better on HateCheck, a suite of
functional tests for online hate detection. We provide the code, dataset and
annotation guidelines for other researchers to use. Accepted at ACL 2021.","We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of ~40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes ~15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use. Accepted at ACL 2021.",http://arxiv.org/pdf/2012.15761,"['Bertie Vidgen', 'Tristan Thrush', 'Zeerak Waseem', 'Douwe Kiela']",128,,,
"Longformer: The Long-Document Transformer
Iz BeltagyMatthew E. PetersArman Cohan
Allen Institute for Artiﬁcial Intelligence, ",2004.0515,Longformer: The Long-Document Transformer,"Transformer-based models are unable to process long sequences due to their
self-attention operation, which scales quadratically with the sequence length.
To address this limitation, we introduce the Longformer with an attention
mechanism that scales linearly with sequence length, making it easy to process
documents of thousands of tokens or longer. Longformer's attention mechanism is
a drop-in replacement for the standard self-attention and combines a local
windowed attention with a task motivated global attention. Following prior work
on long-sequence transformers, we evaluate Longformer on character-level
language modeling and achieve state-of-the-art results on text8 and enwik8. In
contrast to most prior work, we also pretrain Longformer and finetune it on a
variety of downstream tasks. Our pretrained Longformer consistently outperforms
RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop
and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a
Longformer variant for supporting long document generative sequence-to-sequence
tasks, and demonstrate its effectiveness on the arXiv summarization dataset.","Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",http://arxiv.org/pdf/2004.05150,"['Iz Beltagy', 'Matthew E. Peters', 'Arman Cohan']",128,,,
"TOWARDS CONTINUAL KNOWLEDGE LEARNING OF
LANGUAGE MODELS
Joel Jang1Seonghyeon Ye1Sohee Yang1Joongbo Shin2
Janghoon Han2Gyeonghun ",2110.03215,Towards Continual Knowledge Learning of Language Models,"Large Language Models (LMs) are known to encode world knowledge in their
parameters as they pretrain on a vast amount of web corpus, which is often
utilized for performing knowledge-dependent downstream tasks such as question
answering, fact-checking, and open dialogue. In real-world scenarios, the world
knowledge stored in the LMs can quickly become outdated as the world changes,
but it is non-trivial to avoid catastrophic forgetting and reliably acquire new
knowledge while preserving invariant knowledge. To push the community towards
better maintenance of ever-changing LMs, we formulate a new continual learning
(CL) problem called Continual Knowledge Learning (CKL). We construct a new
benchmark and metric to quantify the retention of time-invariant world
knowledge, the update of outdated knowledge, and the acquisition of new
knowledge. We adopt applicable recent methods from literature to create several
strong baselines. Through extensive experiments, we find that CKL exhibits
unique challenges that are not addressed in previous CL setups, where parameter
expansion is necessary to reliably retain and learn knowledge simultaneously.
By highlighting the critical causes of knowledge forgetting, we show that CKL
is a challenging and important problem that helps us better understand and
train ever-changing LMs. The benchmark datasets, evaluation script, and
baseline code to reproduce our results are available at
https://github.com/joeljang/continual-knowledge-learning.","Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.",http://arxiv.org/pdf/2110.03215,"['Joel Jang', 'Seonghyeon Ye', 'Sohee Yang', 'Joongbo Shin', 'Janghoon Han', 'Gyeonghun Kim', 'Stanley Jungkyu Choi', 'Minjoon Seo']",128,,,
"A Crowd-based Evaluation of Abuse Response Strategies in
Conversational Agents
Amanda Cercas Curry
Interaction Lab
Heriot-Watt U",1909.04387,A Crowd-based Evaluation of Abuse Response Strategies in Conversational Agents,"How should conversational agents respond to verbal abuse through the user? To
answer this question, we conduct a large-scale crowd-sourced evaluation of
abuse response strategies employed by current state-of-the-art systems. Our
results show that some strategies, such as ""polite refusal"" score highly across
the board, while for other strategies demographic factors, such as age, as well
as the severity of the preceding abuse influence the user's perception of which
response is appropriate. In addition, we find that most data-driven models lag
behind rule-based or commercial systems in terms of their perceived
appropriateness.","How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as polite refusal score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user's perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.",http://arxiv.org/pdf/1909.04387,"['Amanda Cercas Curry', 'Verena Rieser']",128,,,
"MasakhaNER: Named Entity Recognition for African Languages
David Ifeoluwa Adelani1∗, Jade Abbott2∗, Graham Neubig3, Daniel D’sou",2103.11811,MasakhaNER: Named Entity Recognition for African Languages,"We take a step towards addressing the under-representation of the African
continent in NLP research by creating the first large publicly available
high-quality dataset for named entity recognition (NER) in ten African
languages, bringing together a variety of stakeholders. We detail
characteristics of the languages to help researchers understand the challenges
that these languages pose for NER. We analyze our datasets and conduct an
extensive empirical evaluation of state-of-the-art methods across both
supervised and transfer learning settings. We release the data, code, and
models in order to inspire future research on African NLP.","We take a step towards addressing the under-representation of the African continent in NLP research by creating the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages, bringing together a variety of stakeholders. We detail characteristics of the languages to help researchers understand the challenges that these languages pose for NER. We analyze our datasets and conduct an extensive empirical evaluation of state-of-the-art methods across both supervised and transfer learning settings. We release the data, code, and models in order to inspire future research on African NLP.",http://arxiv.org/pdf/2103.11811,"['David Ifeoluwa Adelani', 'Jade Abbott', 'Graham Neubig', ""Daniel D'souza"", 'Julia Kreutzer', 'Constantine Lignos', 'Chester Palen-Michel', 'Happy Buzaaba', 'Shruti Rijhwani', 'Sebastian Ruder', 'Stephen Mayhew', 'Israel Abebe Azime', 'Shamsuddeen Muhammad', 'Chris Chinenye Emezue', 'Joyce Nakatumba-Nabende', 'Perez Ogayo', 'Anuoluwapo Aremu', 'Catherine Gitau', 'Derguene Mbaye', 'Jesujoba Alabi', 'Seid Muhie Yimam', 'Tajuddeen Gwadabe', 'Ignatius Ezeani', 'Rubungo Andre Niyongabo', 'Jonathan Mukiibi', 'Verrah Otiende', 'Iroro Orife', 'Davis David', 'Samba Ngom', 'Tosin Adewumi', 'Paul Rayson', 'Mofetoluwa Adeyemi', 'Gerald Muriuki', 'Emmanuel Anebi', 'Chiamaka Chukwuneke', 'Nkiruka Odu', 'Eric Peter Wairagala', 'Samuel Oyerinde', 'Clemencia Siro', 'Tobius Saul Bateesa', 'Temilola Oloyede', 'Yvonne Wambui', 'Victor Akinode', 'Deborah Nabagereka', 'Maurice Katusiime', 'Ayodele Awokoya', 'Mouhamadane MBOUP', 'Dibora Gebreyohannes', 'Henok Tilaye', 'Kelechi Nwaike', 'Degaga Wolde', 'Abdoulaye Faye', 'Blessing Sibanda', 'Orevaoghene Ahia', 'Bonaventure F. P. Dossou', 'Kelechi Ogueji', 'Thierno Ibrahima DIOP', 'Abdoulaye Diallo', 'Adewale Akinfaderin', 'Tendai Marengereke', 'Salomey Osei']",128,,,
"An Empirical Model of Large-Batch Training
Sam McCandlish
OpenAI
sam@openai.comJared Kaplan
Johns Hopkins University, OpenAI
ja",1812.06162,An Empirical Model of Large-Batch Training,"In an increasing number of domains it has been demonstrated that deep
learning models can be trained using relatively large batch sizes without
sacrificing data efficiency. However the limits of this massive data
parallelism seem to differ from domain to domain, ranging from batches of tens
of thousands in ImageNet to batches of millions in RL agents that play the game
Dota 2. To our knowledge there is limited conceptual understanding of why these
limits to batch size differ or how we might choose the correct batch size in a
new domain. In this paper, we demonstrate that a simple and easy-to-measure
statistic called the gradient noise scale predicts the largest useful batch
size across many domains and applications, including a number of supervised
learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word),
reinforcement learning domains (Atari and Dota), and even generative model
training (autoencoders on SVHN). We find that the noise scale increases as the
loss decreases over a training run and depends on the model size primarily
through improved model performance. Our empirically-motivated theory also
describes the tradeoff between compute-efficiency and time-efficiency, and
provides a rough model of the benefits of adaptive batch-size training.","In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",http://arxiv.org/pdf/1812.06162,"['Sam McCandlish', 'Jared Kaplan', 'Dario Amodei', 'OpenAI Dota Team']",128,,,
"Language Models are Few-shot Multilingual Learners
Genta Indra Winata1, Andrea Madotto1;3, Zhaojiang Lin1,
Rosanne Liu2;3, Jas",2109.07684,Language Models are Few-shot Multilingual Learners,"General-purpose language models have demonstrated impressive capabilities,
performing on par with state-of-the-art approaches on a range of downstream
natural language processing (NLP) tasks and benchmarks when inferring
instructions from very few examples. Here, we evaluate the multilingual skills
of the GPT and T5 models in conducting multi-class classification on
non-English languages without any parameter updates. We show that, given a few
English examples as context, pre-trained language models can predict not only
English test samples but also non-English ones. Finally, we find the in-context
few-shot cross-lingual prediction results of language models are significantly
better than random prediction, and they are competitive compared to the
existing state-of-the-art cross-lingual models.","General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.",http://arxiv.org/pdf/2109.07684,"['Genta Indra Winata', 'Andrea Madotto', 'Zhaojiang Lin', 'Rosanne Liu', 'Jason Yosinski', 'Pascale Fung']",128,,,
"2022-03-16
Teaching language models to support
answers with veriﬁed quotes
Jacob Menick*,1,2, Maja Trebacz*,1, Vladimir Mikulik*",2203.11147,Teaching language models to support answers with verified quotes,"Recent large language models often answer factual questions correctly. But
users can't trust any given claim a model makes without fact-checking, because
language models can hallucinate convincing nonsense. In this work we use
reinforcement learning from human preferences (RLHP) to train ""open-book"" QA
models that generate answers whilst also citing specific evidence for their
claims, which aids in the appraisal of correctness. Supporting evidence is
drawn from multiple documents found via a search engine, or from a single
user-provided document. Our 280 billion parameter model, GopherCite, is able to
produce answers with high quality supporting evidence and abstain from
answering when unsure. We measure the performance of GopherCite by conducting
human evaluation of answers to questions in a subset of the NaturalQuestions
and ELI5 datasets. The model's response is found to be high-quality 80\% of the
time on this Natural Questions subset, and 67\% of the time on the ELI5 subset.
Abstaining from the third of questions for which it is most unsure improves
performance to 90\% and 80\% respectively, approaching human baselines.
However, analysis on the adversarial TruthfulQA dataset shows why citation is
only one part of an overall strategy for safety and trustworthiness: not all
claims supported by evidence are true.","Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train open-book QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",http://arxiv.org/pdf/2203.11147,"['Jacob Menick', 'Maja Trebacz', 'Vladimir Mikulik', 'John Aslanides', 'Francis Song', 'Martin Chadwick', 'Mia Glaese', 'Susannah Young', 'Lucy Campbell-Gillingham', 'Geoffrey Irving', 'Nat McAleese']",128,,,
"Pythia : A Suite for Analyzing Large Language Models
Across Training and Scaling
Stella Biderman* 1 2Hailey Schoelkopf* 1 3Quent",2304.01373,Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling,"How do large language models (LLMs) develop and evolve over the course of
training? How do these patterns change as models scale? To answer these
questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on
public data seen in the exact same order and ranging in size from 70M to 12B
parameters. We provide public access to 154 checkpoints for each one of the 16
models, alongside tools to download and reconstruct their exact training
dataloaders for further study. We intend \textit{Pythia} to facilitate research
in many areas, and we present several case studies including novel results in
memorization, term frequency effects on few-shot performance, and reducing
gender bias. We demonstrate that this highly controlled setup can be used to
yield novel insights toward LLMs and their training dynamics. Trained models,
analysis code, training code, and training data can be found at
\url{https://github.com/EleutherAI/pythia}.","How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \url{https://github.com/EleutherAI/pythia}.",http://arxiv.org/pdf/2304.01373,"['Stella Biderman', 'Hailey Schoelkopf', 'Quentin Anthony', 'Herbie Bradley', ""Kyle O'Brien"", 'Eric Hallahan', 'Mohammad Aflah Khan', 'Shivanshu Purohit', 'USVSN Sai Prashanth', 'Edward Raff', 'Aviya Skowron', 'Lintang Sutawika', 'Oskar van der Wal']",128,,,
"Recursively Summarizing Books with Human Feedback
Jeff WuLong OuyangDaniel M. ZieglerNisan StiennonRyan Lowe
Jan LeikePaul",2109.10862,Recursively Summarizing Books with Human Feedback,"A major challenge for scaling machine learning is training models to perform
tasks that are very difficult or time-consuming for humans to evaluate. We
present progress on this problem on the task of abstractive summarization of
entire fiction novels. Our method combines learning from human feedback with
recursive task decomposition: we use models trained on smaller parts of the
task to assist humans in giving feedback on the broader task. We collect a
large volume of demonstrations and comparisons from human labelers, and
fine-tune GPT-3 using behavioral cloning and reward modeling to do
summarization recursively. At inference time, the model first summarizes small
sections of the book and then recursively summarizes these summaries to produce
a summary of the entire book. Our human labelers are able to supervise and
evaluate the models quickly, despite not having read the entire books
themselves. Our resulting model generates sensible summaries of entire books,
even matching the quality of human-written summaries in a few cases ($\sim5\%$
of books). We achieve state-of-the-art results on the recent BookSum dataset
for book-length summarization. A zero-shot question-answering model using these
summaries achieves state-of-the-art results on the challenging NarrativeQA
benchmark for answering questions about books and movie scripts. We release
datasets of samples from our model.","A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",http://arxiv.org/pdf/2109.10862,"['Jeff Wu', 'Long Ouyang', 'Daniel M. Ziegler', 'Nisan Stiennon', 'Ryan Lowe', 'Jan Leike', 'Paul Christiano']",128,,,
"A Survey on Universal Adversarial Attack
Chaoning Zhang1,Philipp Benz1,Chenguo Lin2,Adil Karjauv1,Jing Wu3,In So Kweon1
1Kore",2103.01498,A Survey On Universal Adversarial Attack,"The intriguing phenomenon of adversarial examples has attracted significant
attention in machine learning and what might be more surprising to the
community is the existence of universal adversarial perturbations (UAPs), i.e.
a single perturbation to fool the target DNN for most images. With the focus on
UAP against deep classifiers, this survey summarizes the recent progress on
universal adversarial attacks, discussing the challenges from both the attack
and defense sides, as well as the reason for the existence of UAP. We aim to
extend this work as a dynamic survey that will regularly update its content to
follow new works regarding UAP or universal attack in a wide range of domains,
such as image, audio, video, text, etc. Relevant updates will be discussed at:
https://bit.ly/2SbQlLG. We welcome authors of future works in this field to
contact us for including your new finding.","The intriguing phenomenon of adversarial examples has attracted significant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. With the focus on UAP against deep classifiers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. We aim to extend this work as a dynamic survey that will regularly update its content to follow new works regarding UAP or universal attack in a wide range of domains, such as image, audio, video, text, etc. Relevant updates will be discussed at: https://bit.ly/2SbQlLG. We welcome authors of future works in this field to contact us for including your new finding.",http://arxiv.org/pdf/2103.01498,"['Chaoning Zhang', 'Philipp Benz', 'Chenguo Lin', 'Adil Karjauv', 'Jing Wu', 'In So Kweon']",128,,,
"Efﬁcient Large Scale Language Modeling with Mixtures of Experts
Mikel Artetxe,Shruti Bhosale,Naman Goyal,Todor Mihaylov,Myle",2112.10684,Efficient Large Scale Language Modeling with Mixtures of Experts,"Mixture of Experts layers (MoEs) enable efficient scaling of language models
through conditional computation. This paper presents a detailed empirical study
of how autoregressive MoE language models scale in comparison with dense models
in a wide range of settings: in- and out-of-domain language modeling, zero- and
few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning,
we find MoEs to be substantially more compute efficient. At more modest
training budgets, MoEs can match the performance of dense models using $\sim$4
times less compute. This gap narrows at scale, but our largest MoE model (1.1T
parameters) consistently outperforms a compute-equivalent dense model (6.7B
parameters). Overall, this performance gap varies greatly across tasks and
domains, suggesting that MoE and dense models generalize differently in ways
that are worthy of future study. We make our code and models publicly available
for research use.","Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",http://arxiv.org/pdf/2112.10684,"['Mikel Artetxe', 'Shruti Bhosale', 'Naman Goyal', 'Todor Mihaylov', 'Myle Ott', 'Sam Shleifer', 'Xi Victoria Lin', 'Jingfei Du', 'Srinivasan Iyer', 'Ramakanth Pasunuru', 'Giri Anantharaman', 'Xian Li', 'Shuohui Chen', 'Halil Akin', 'Mandeep Baines', 'Louis Martin', 'Xing Zhou', 'Punit Singh Koura', ""Brian O'Horo"", 'Jeff Wang', 'Luke Zettlemoyer', 'Mona Diab', 'Zornitsa Kozareva', 'Ves Stoyanov']",128,,,
"Asynchronous Methods for Deep Reinforcement Learning
Volodymyr Mnih1VMNIH @GOOGLE .COM
Adrià Puigdomènech Badia1ADRIAP @GOOGLE .",1602.01783,Asynchronous Methods for Deep Reinforcement Learning,"We propose a conceptually simple and lightweight framework for deep
reinforcement learning that uses asynchronous gradient descent for optimization
of deep neural network controllers. We present asynchronous variants of four
standard reinforcement learning algorithms and show that parallel
actor-learners have a stabilizing effect on training allowing all four methods
to successfully train neural network controllers. The best performing method,
an asynchronous variant of actor-critic, surpasses the current state-of-the-art
on the Atari domain while training for half the time on a single multi-core CPU
instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds
on a wide variety of continuous motor control problems as well as on a new task
of navigating random 3D mazes using a visual input.","We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",http://arxiv.org/pdf/1602.01783,"['Volodymyr Mnih', 'Adrià Puigdomènech Badia', 'Mehdi Mirza', 'Alex Graves', 'Timothy P. Lillicrap', 'Tim Harley', 'David Silver', 'Koray Kavukcuoglu']",128,,,
"TruthfulQA: Measuring How Models Mimic Human Falsehoods
Stephanie Lin
University of Oxford
sylin07@gmail.comJacob Hilton
OpenAI
",2109.07958,TruthfulQA: Measuring How Models Mimic Human Falsehoods,"We propose a benchmark to measure whether a language model is truthful in
generating answers to questions. The benchmark comprises 817 questions that
span 38 categories, including health, law, finance and politics. We crafted
questions that some humans would answer falsely due to a false belief or
misconception. To perform well, models must avoid generating false answers
learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a
T5-based model. The best model was truthful on 58% of questions, while human
performance was 94%. Models generated many false answers that mimic popular
misconceptions and have the potential to deceive humans. The largest models
were generally the least truthful. This contrasts with other NLP tasks, where
performance improves with model size. However, this result is expected if false
answers are learned from the training distribution. We suggest that scaling up
models alone is less promising for improving truthfulness than fine-tuning
using training objectives other than imitation of text from the web.","We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",http://arxiv.org/pdf/2109.07958,"['Stephanie Lin', 'Jacob Hilton', 'Owain Evans']",128,,,
"BLOOM: A 176B-Parameter Open-Access Multilingual
Language Model
BigScience Workshop
Major Contributors
Teven Le Scao, Angela Fa",2211.051,BLOOM: A 176B-Parameter Open-Access Multilingual Language Model,"Large language models (LLMs) have been shown to be able to perform new tasks
based on a few demonstrations or natural language instructions. While these
capabilities have led to widespread adoption, most LLMs are developed by
resource-rich organizations and are frequently kept from the public. As a step
towards democratizing this powerful technology, we present BLOOM, a
176B-parameter open-access language model designed and built thanks to a
collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer
language model that was trained on the ROOTS corpus, a dataset comprising
hundreds of sources in 46 natural and 13 programming languages (59 in total).
We find that BLOOM achieves competitive performance on a wide variety of
benchmarks, with stronger results after undergoing multitask prompted
finetuning. To facilitate future research and applications using LLMs, we
publicly release our models and code under the Responsible AI License.","Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",http://arxiv.org/pdf/2211.05100,"['BigScience Workshop', ':', 'Teven Le Scao', 'Angela Fan', 'Christopher Akiki', 'Ellie Pavlick', 'Suzana Ilić', 'Daniel Hesslow', 'Roman Castagné', 'Alexandra Sasha Luccioni', 'François Yvon', 'Matthias Gallé', 'Jonathan Tow', 'Alexander M. Rush', 'Stella Biderman', 'Albert Webson', 'Pawan Sasanka Ammanamanchi', 'Thomas Wang', 'Benoît Sagot', 'Niklas Muennighoff', 'Albert Villanova del Moral', 'Olatunji Ruwase', 'Rachel Bawden', 'Stas Bekman', 'Angelina McMillan-Major', 'Iz Beltagy', 'Huu Nguyen', 'Lucile Saulnier', 'Samson Tan', 'Pedro Ortiz Suarez', 'Victor Sanh', 'Hugo Laurençon', 'Yacine Jernite', 'Julien Launay', 'Margaret Mitchell', 'Colin Raffel', 'Aaron Gokaslan', 'Adi Simhi', 'Aitor Soroa', 'Alham Fikri Aji', 'Amit Alfassy', 'Anna Rogers', 'Ariel Kreisberg Nitzav', 'Canwen Xu', 'Chenghao Mou', 'Chris Emezue', 'Christopher Klamm', 'Colin Leong', 'Daniel van Strien', 'David Ifeoluwa Adelani', 'Dragomir Radev', 'Eduardo González Ponferrada', 'Efrat Levkovizh', 'Ethan Kim', 'Eyal Bar Natan', 'Francesco De Toni', 'Gérard Dupont', 'Germán Kruszewski', 'Giada Pistilli', 'Hady Elsahar', 'Hamza Benyamina', 'Hieu Tran', 'Ian Yu', 'Idris Abdulmumin', 'Isaac Johnson', 'Itziar Gonzalez-Dios', 'Javier de la Rosa', 'Jenny Chim', 'Jesse Dodge', 'Jian Zhu', 'Jonathan Chang', 'Jörg Frohberg', 'Joseph Tobing', 'Joydeep Bhattacharjee', 'Khalid Almubarak', 'Kimbo Chen', 'Kyle Lo', 'Leandro Von Werra', 'Leon Weber', 'Long Phan', 'Loubna Ben allal', 'Ludovic Tanguy', 'Manan Dey', 'Manuel Romero Muñoz', 'Maraim Masoud', 'María Grandury', 'Mario Šaško', 'Max Huang', 'Maximin Coavoux', 'Mayank Singh', 'Mike Tian-Jian Jiang', 'Minh Chien Vu', 'Mohammad A. Jauhar', 'Mustafa Ghaleb', 'Nishant Subramani', 'Nora Kassner', 'Nurulaqilla Khamis', 'Olivier Nguyen', 'Omar Espejel', 'Ona de Gibert', 'Paulo Villegas', 'Peter Henderson', 'Pierre Colombo', 'Priscilla Amuok', 'Quentin Lhoest', 'Rheza Harliman', 'Rishi Bommasani', 'Roberto Luis López', 'Rui Ribeiro', 'Salomey Osei', 'Sampo Pyysalo', 'Sebastian Nagel', 'Shamik Bose', 'Shamsuddeen Hassan Muhammad', 'Shanya Sharma', 'Shayne Longpre', 'Somaieh Nikpoor', 'Stanislav Silberberg', 'Suhas Pai', 'Sydney Zink', 'Tiago Timponi Torrent', 'Timo Schick', 'Tristan Thrush', 'Valentin Danchev', 'Vassilina Nikoulina', 'Veronika Laippala', 'Violette Lepercq', 'Vrinda Prabhu', 'Zaid Alyafeai', 'Zeerak Talat', 'Arun Raja', 'Benjamin Heinzerling', 'Chenglei Si', 'Davut Emre Taşar', 'Elizabeth Salesky', 'Sabrina J. Mielke', 'Wilson Y. Lee', 'Abheesht Sharma', 'Andrea Santilli', 'Antoine Chaffin', 'Arnaud Stiegler', 'Debajyoti Datta', 'Eliza Szczechla', 'Gunjan Chhablani', 'Han Wang', 'Harshit Pandey', 'Hendrik Strobelt', 'Jason Alan Fries', 'Jos Rozen', 'Leo Gao', 'Lintang Sutawika', 'M Saiful Bari', 'Maged S. Al-shaibani', 'Matteo Manica', 'Nihal Nayak', 'Ryan Teehan', 'Samuel Albanie', 'Sheng Shen', 'Srulik Ben-David', 'Stephen H. Bach', 'Taewoon Kim', 'Tali Bers', 'Thibault Fevry', 'Trishala Neeraj', 'Urmish Thakker', 'Vikas Raunak', 'Xiangru Tang', 'Zheng-Xin Yong', 'Zhiqing Sun', 'Shaked Brody', 'Yallow Uri', 'Hadar Tojarieh', 'Adam Roberts', 'Hyung Won Chung', 'Jaesung Tae', 'Jason Phang', 'Ofir Press', 'Conglong Li', 'Deepak Narayanan', 'Hatim Bourfoune', 'Jared Casper', 'Jeff Rasley', 'Max Ryabinin', 'Mayank Mishra', 'Minjia Zhang', 'Mohammad Shoeybi', 'Myriam Peyrounette', 'Nicolas Patry', 'Nouamane Tazi', 'Omar Sanseviero', 'Patrick von Platen', 'Pierre Cornette', 'Pierre François Lavallée', 'Rémi Lacroix', 'Samyam Rajbhandari', 'Sanchit Gandhi', 'Shaden Smith', 'Stéphane Requena', 'Suraj Patil', 'Tim Dettmers', 'Ahmed Baruwa', 'Amanpreet Singh', 'Anastasia Cheveleva', 'Anne-Laure Ligozat', 'Arjun Subramonian', 'Aurélie Névéol', 'Charles Lovering', 'Dan Garrette', 'Deepak Tunuguntla', 'Ehud Reiter', 'Ekaterina Taktasheva', 'Ekaterina Voloshina', 'Eli Bogdanov', 'Genta Indra Winata', 'Hailey Schoelkopf', 'Jan-Christoph Kalo', 'Jekaterina Novikova', 'Jessica Zosa Forde', 'Jordan Clive', 'Jungo Kasai', 'Ken Kawamura', 'Liam Hazan', 'Marine Carpuat', 'Miruna Clinciu', 'Najoung Kim', 'Newton Cheng', 'Oleg Serikov', 'Omer Antverg', 'Oskar van der Wal', 'Rui Zhang', 'Ruochen Zhang', 'Sebastian Gehrmann', 'Shachar Mirkin', 'Shani Pais', 'Tatiana Shavrina', 'Thomas Scialom', 'Tian Yun', 'Tomasz Limisiewicz', 'Verena Rieser', 'Vitaly Protasov', 'Vladislav Mikhailov', 'Yada Pruksachatkun', 'Yonatan Belinkov', 'Zachary Bamberger', 'Zdeněk Kasner', 'Alice Rueda', 'Amanda Pestana', 'Amir Feizpour', 'Ammar Khan', 'Amy Faranak', 'Ana Santos', 'Anthony Hevia', 'Antigona Unldreaj', 'Arash Aghagol', 'Arezoo Abdollahi', 'Aycha Tammour', 'Azadeh HajiHosseini', 'Bahareh Behroozi', 'Benjamin Ajibade', 'Bharat Saxena', 'Carlos Muñoz Ferrandis', 'Daniel McDuff', 'Danish Contractor', 'David Lansky', 'Davis David', 'Douwe Kiela', 'Duong A. Nguyen', 'Edward Tan', 'Emi Baylor', 'Ezinwanne Ozoani', 'Fatima Mirza', 'Frankline Ononiwu', 'Habib Rezanejad', 'Hessie Jones', 'Indrani Bhattacharya', 'Irene Solaiman', 'Irina Sedenko', 'Isar Nejadgholi', 'Jesse Passmore', 'Josh Seltzer', 'Julio Bonis Sanz', 'Livia Dutra', 'Mairon Samagaio', 'Maraim Elbadri', 'Margot Mieskes', 'Marissa Gerchick', 'Martha Akinlolu', 'Michael McKenna', 'Mike Qiu', 'Muhammed Ghauri', 'Mykola Burynok', 'Nafis Abrar', 'Nazneen Rajani', 'Nour Elkott', 'Nour Fahmy', 'Olanrewaju Samuel', 'Ran An', 'Rasmus Kromann', 'Ryan Hao', 'Samira Alizadeh', 'Sarmad Shubber', 'Silas Wang', 'Sourav Roy', 'Sylvain Viguier', 'Thanh Le', 'Tobi Oyebade', 'Trieu Le', 'Yoyo Yang', 'Zach Nguyen', 'Abhinav Ramesh Kashyap', 'Alfredo Palasciano', 'Alison Callahan', 'Anima Shukla', 'Antonio Miranda-Escalada', 'Ayush Singh', 'Benjamin Beilharz', 'Bo Wang', 'Caio Brito', 'Chenxi Zhou', 'Chirag Jain', 'Chuxin Xu', 'Clémentine Fourrier', 'Daniel León Periñán', 'Daniel Molano', 'Dian Yu', 'Enrique Manjavacas', 'Fabio Barth', 'Florian Fuhrimann', 'Gabriel Altay', 'Giyaseddin Bayrak', 'Gully Burns', 'Helena U. Vrabec', 'Imane Bello', 'Ishani Dash', 'Jihyun Kang', 'John Giorgi', 'Jonas Golde', 'Jose David Posada', 'Karthik Rangasai Sivaraman', 'Lokesh Bulchandani', 'Lu Liu', 'Luisa Shinzato', 'Madeleine Hahn de Bykhovetz', 'Maiko Takeuchi', 'Marc Pàmies', 'Maria A Castillo', 'Marianna Nezhurina', 'Mario Sänger', 'Matthias Samwald', 'Michael Cullan', 'Michael Weinberg', 'Michiel De Wolf', 'Mina Mihaljcic', 'Minna Liu', 'Moritz Freidank', 'Myungsun Kang', 'Natasha Seelam', 'Nathan Dahlberg', 'Nicholas Michio Broad', 'Nikolaus Muellner', 'Pascale Fung', 'Patrick Haller', 'Ramya Chandrasekhar', 'Renata Eisenberg', 'Robert Martin', 'Rodrigo Canalli', 'Rosaline Su', 'Ruisi Su', 'Samuel Cahyawijaya', 'Samuele Garda', 'Shlok S Deshmukh', 'Shubhanshu Mishra', 'Sid Kiblawi', 'Simon Ott', 'Sinee Sang-aroonsiri', 'Srishti Kumar', 'Stefan Schweter', 'Sushil Bharati', 'Tanmay Laud', 'Théo Gigant', 'Tomoya Kainuma', 'Wojciech Kusa', 'Yanis Labrak', 'Yash Shailesh Bajaj', 'Yash Venkatraman', 'Yifan Xu', 'Yingxin Xu', 'Yu Xu', 'Zhe Tan', 'Zhongli Xie', 'Zifan Ye', 'Mathilde Bras', 'Younes Belkada', 'Thomas Wolf']",128,,,
"Gender Bias in Machine Translation
Beatrice Savoldi1,2, Marco Gaido1,2, Luisa Bentivogli2, Matteo Negri2, Marco Turchi2
1Univers",2104.06001,Gender Bias in Machine Translation,"Machine translation (MT) technology has facilitated our daily tasks by
providing accessible shortcuts for gathering, elaborating and communicating
information. However, it can suffer from biases that harm users and society at
large. As a relatively new field of inquiry, gender bias in MT still lacks
internal cohesion, which advocates for a unified framework to ease future
research. To this end, we: i) critically review current conceptualizations of
bias in light of theoretical insights from related disciplines, ii) summarize
previous analyses aimed at assessing gender bias in MT, iii) discuss the
mitigating strategies proposed so far, and iv) point toward potential
directions for future work.","Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.",http://arxiv.org/pdf/2104.06001,"['Beatrice Savoldi', 'Marco Gaido', 'Luisa Bentivogli', 'Matteo Negri', 'Marco Turchi']",128,,,
"arXiv:2207.00560v1  [cs.CL]  1 Jul 2022Is neural language acquisition similar to natural?
A chronological probing study
Ekaterin",2207.0056,Is neural language acquisition similar to natural? A chronological probing study,"The probing methodology allows one to obtain a partial representation of
linguistic phenomena stored in the inner layers of the neural network, using
external classifiers and statistical analysis. Pre-trained transformer-based
language models are widely used both for natural language understanding (NLU)
and natural language generation (NLG) tasks making them most commonly used for
downstream applications. However, little analysis was carried out, whether the
models were pre-trained enough or contained knowledge correlated with
linguistic theory. We are presenting the chronological probing study of
transformer English models such as MultiBERT and T5. We sequentially compare
the information about the language learned by the models in the process of
training on corpora. The results show that 1) linguistic information is
acquired in the early stages of training 2) both language models demonstrate
capabilities to capture various features from various levels of language,
including morphology, syntax, and even discourse, while they also can
inconsistently fail on tasks that are perceived as easy. We also introduce the
open-source framework for chronological probing research, compatible with other
transformer-based models.
https://github.com/EkaterinaVoloshina/chronological_probing","The probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the inner layers of the neural network, using external classifiers and statistical analysis. Pre-trained transformer-based language models are widely used both for natural language understanding (NLU) and natural language generation (NLG) tasks making them most commonly used for downstream applications. However, little analysis was carried out, whether the models were pre-trained enough or contained knowledge correlated with linguistic theory. We are presenting the chronological probing study of transformer English models such as MultiBERT and T5. We sequentially compare the information about the language learned by the models in the process of training on corpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy. We also introduce the open-source framework for chronological probing research, compatible with other transformer-based models. https://github.com/EkaterinaVoloshina/chronological_probing",http://arxiv.org/pdf/2207.00560,"['Ekaterina Voloshina', 'Oleg Serikov', 'Tatiana Shavrina']",128,,,
"Leveraging QA Datasets to Improve Generative Data Augmentation
Dheeraj MekalaµTu Vu¹Timo SchickºJingbo Shangµ;·;
µUniversity of",2205.12604,Leveraging QA Datasets to Improve Generative Data Augmentation,"The ability of generative language models (GLMs) to generate text has
improved considerably in the last few years, enabling their use for generative
data augmentation. In this work, we propose CONDA, an approach to further
improve GLMs' ability to generate synthetic data by reformulating data
generation as context generation for a given question-answer (QA) pair and
leveraging QA datasets for training context generators. Then, we cast
downstream tasks into the same question answering format and adapt the
fine-tuned context generators to the target task domain. Finally, we use the
fine-tuned GLM to generate relevant contexts, which are in turn used as
synthetic training data for their corresponding tasks. We perform extensive
experiments on multiple classification datasets and demonstrate substantial
improvements in performance for both few- and zero-shot settings. Our analysis
reveals that QA datasets that require high-level reasoning abilities (e.g.,
abstractive and common-sense QA datasets) tend to give the best boost in
performance in both few-shot and zero-shot settings.","The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLMs' ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",http://arxiv.org/pdf/2205.12604,"['Dheeraj Mekala', 'Tu Vu', 'Timo Schick', 'Jingbo Shang']",128,,,
"Revealing Persona Biases in Dialogue Systems
Emily Sheng1*Josh Arnold2
Zhou Yu3Kai-Wei Chang4Nanyun Peng1;4
1Information Scienc",2104.08728,Revealing Persona Biases in Dialogue Systems,"Dialogue systems in the form of chatbots and personal assistants are being
increasingly integrated into people's lives. Modern dialogue systems may
consider adopting anthropomorphic personas, mimicking societal demographic
groups to appear more approachable and trustworthy to users. However, the
adoption of a persona can result in the adoption of biases. In this paper, we
present the first large-scale study on persona biases in dialogue systems and
conduct analyses on personas of different social classes, sexual orientations,
races, and genders. We define persona biases as harmful differences in
responses (e.g., varying levels of offensiveness, agreement with harmful
statements) generated from adopting different demographic personas.
Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore
and aggregate persona biases in dialogue systems. By analyzing the Blender and
DialoGPT dialogue systems, we observe that adopting personas can actually
decrease harmful responses, compared to not using any personas. Additionally,
we find that persona choices can affect the degree of harms in generated
responses and thus should be systematically evaluated before deployment. We
also analyze how personas can result in different amounts of harm towards
specific demographics.","Dialogue systems in the form of chatbots and personal assistants are being increasingly integrated into people's lives. Modern dialogue systems may consider adopting anthropomorphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the first large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and genders. We define persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we find that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment. We also analyze how personas can result in different amounts of harm towards specific demographics.",http://arxiv.org/pdf/2104.08728,"['Emily Sheng', 'Josh Arnold', 'Zhou Yu', 'Kai-Wei Chang', 'Nanyun Peng']",128,,,
"A Deep Reinforcement Learning Chatbot
Iulian V . Serban ,Chinnadhurai Sankar ,Mathieu Germain ,Saizheng Zhang ,Zhouhan Lin ,
San",1709.02349,A Deep Reinforcement Learning Chatbot,"We present MILABOT: a deep reinforcement learning chatbot developed by the
Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize
competition. MILABOT is capable of conversing with humans on popular small talk
topics through both speech and text. The system consists of an ensemble of
natural language generation and retrieval models, including template-based
models, bag-of-words models, sequence-to-sequence neural network and latent
variable neural network models. By applying reinforcement learning to
crowdsourced data and real-world user interactions, the system has been trained
to select an appropriate response from the models in its ensemble. The system
has been evaluated through A/B testing with real-world users, where it
performed significantly better than many competing systems. Due to its machine
learning architecture, the system is likely to improve with additional data.","We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.",http://arxiv.org/pdf/1709.02349,"['Iulian V. Serban', 'Chinnadhurai Sankar', 'Mathieu Germain', 'Saizheng Zhang', 'Zhouhan Lin', 'Sandeep Subramanian', 'Taesup Kim', 'Michael Pieper', 'Sarath Chandar', 'Nan Rosemary Ke', 'Sai Rajeshwar', 'Alexandre de Brebisson', 'Jose M. R. Sotelo', 'Dendi Suhubdy', 'Vincent Michalski', 'Alexandre Nguyen', 'Joelle Pineau', 'Yoshua Bengio']",128,,,
"2022-3-16
Competition-Level Code Generation with
AlphaCode
Yujia Li*, David Choi*, Junyoung Chung*, Nate Kushman*, Julian Schrit",2203.07814,Competition-Level Code Generation with AlphaCode,"Programming is a powerful and ubiquitous problem-solving tool. Developing
systems that can assist programmers or even generate programs independently
could make programming more productive and accessible, yet so far incorporating
innovations in AI has proven challenging. Recent large-scale language models
have demonstrated an impressive ability to generate code, and are now able to
complete simple programming tasks. However, these models still perform poorly
when evaluated on more complex, unseen problems that require problem-solving
skills beyond simply translating instructions into code. For example,
competitive programming problems which require an understanding of algorithms
and complex natural language remain extremely challenging. To address this gap,
we introduce AlphaCode, a system for code generation that can create novel
solutions to these problems that require deeper reasoning. In simulated
evaluations on recent programming competitions on the Codeforces platform,
AlphaCode achieved on average a ranking of top 54.3% in competitions with more
than 5,000 participants. We found that three key components were critical to
achieve good and reliable performance: (1) an extensive and clean competitive
programming dataset for training and evaluation, (2) large and
efficient-to-sample transformer-based architectures, and (3) large-scale model
sampling to explore the search space, followed by filtering based on program
behavior to a small set of submissions.","Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.",http://arxiv.org/pdf/2203.07814,"['Yujia Li', 'David Choi', 'Junyoung Chung', 'Nate Kushman', 'Julian Schrittwieser', 'Rémi Leblond', 'Tom Eccles', 'James Keeling', 'Felix Gimeno', 'Agustin Dal Lago', 'Thomas Hubert', 'Peter Choy', ""Cyprien de Masson d'Autume"", 'Igor Babuschkin', 'Xinyun Chen', 'Po-Sen Huang', 'Johannes Welbl', 'Sven Gowal', 'Alexey Cherepanov', 'James Molloy', 'Daniel J. Mankowitz', 'Esme Sutherland Robson', 'Pushmeet Kohli', 'Nando de Freitas', 'Koray Kavukcuoglu', 'Oriol Vinyals']",128,,,
"Training Compute-Optimal Large Language Models
Jordan Hoﬀmann★, Sebastian Borgeaud★, Arthur Mensch★, Elena Buchatskaya, Trevor C",2203.15556,Training Compute-Optimal Large Language Models,"We investigate the optimal model size and number of tokens for training a
transformer language model under a given compute budget. We find that current
large language models are significantly undertrained, a consequence of the
recent focus on scaling language models whilst keeping the amount of training
data constant. By training over 400 language models ranging from 70 million to
over 16 billion parameters on 5 to 500 billion tokens, we find that for
compute-optimal training, the model size and the number of training tokens
should be scaled equally: for every doubling of model size the number of
training tokens should also be doubled. We test this hypothesis by training a
predicted compute-optimal model, Chinchilla, that uses the same compute budget
as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla
uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1
(178B), and Megatron-Turing NLG (530B) on a large range of downstream
evaluation tasks. This also means that Chinchilla uses substantially less
compute for fine-tuning and inference, greatly facilitating downstream usage.
As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5%
on the MMLU benchmark, greater than a 7% improvement over Gopher.","We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",http://arxiv.org/pdf/2203.15556,"['Jordan Hoffmann', 'Sebastian Borgeaud', 'Arthur Mensch', 'Elena Buchatskaya', 'Trevor Cai', 'Eliza Rutherford', 'Diego de Las Casas', 'Lisa Anne Hendricks', 'Johannes Welbl', 'Aidan Clark', 'Tom Hennigan', 'Eric Noland', 'Katie Millican', 'George van den Driessche', 'Bogdan Damoc', 'Aurelia Guy', 'Simon Osindero', 'Karen Simonyan', 'Erich Elsen', 'Jack W. Rae', 'Oriol Vinyals', 'Laurent Sifre']",128,,,
"Recipes for Safety in Open-domain Chatbots
Jing Xu Da Ju Margaret Li Y-Lan Boureau Jason Weston Emily Dinan
Facebook AI Research",2010.07079,Recipes for Safety in Open-domain Chatbots,"Models trained on large unlabeled corpora of human interactions will learn
patterns and mimic behaviors therein, which include offensive or otherwise
toxic behavior and unwanted biases. We investigate a variety of methods to
mitigate these issues in the context of open-domain generative dialogue models.
We introduce a new human-and-model-in-the-loop framework for both training
safer models and for evaluating them, as well as a novel method to distill
safety considerations inside generative models without the use of an external
classifier at deployment time. We conduct experiments comparing these methods
and find our new techniques are (i) safer than existing models as measured by
automatic and human evaluations while (ii) maintaining usability metrics such
as engagingness relative to the state of the art. We then discuss the
limitations of this work by analyzing failure cases of our models.","Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.",http://arxiv.org/pdf/2010.07079,"['Jing Xu', 'Da Ju', 'Margaret Li', 'Y-Lan Boureau', 'Jason Weston', 'Emily Dinan']",128,,,
"2022-5-20
Selection-Inference: Exploiting Large
Language Models for Interpretable Logical
Reasoning
Antonia Creswell1, Murray Sh",2205.09712,Selection-Inference: Exploiting Large Language Models for Interpretable Logical Reasoning,"Large language models (LLMs) have been shown to be capable of impressive
few-shot generalisation to new tasks. However, they still tend to perform
poorly on multi-step logical reasoning problems. Here we carry out a
comprehensive evaluation of LLMs on 50 tasks that probe different aspects of
logical reasoning. We show that language models tend to perform fairly well at
single step inference or entailment tasks, but struggle to chain together
multiple reasoning steps to solve more complex problems. In light of this, we
propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as
general processing modules, and alternates between selection and inference to
generate a series of interpretable, casual reasoning steps leading to the final
answer. We show that a 7B parameter LLM used within the SI framework in a
5-shot generalisation setting, with no fine-tuning, yields a performance
improvement of over 100% compared to an equivalent vanilla baseline on a suite
of 10 logical reasoning tasks. The same model in the same setting even
outperforms a significantly larger 280B parameter baseline on the same suite of
tasks. Moreover, answers produced by the SI framework are accompanied by a
causal natural-language-based reasoning trace, which has important implications
for the safety and trustworthiness of the system.","Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",http://arxiv.org/pdf/2205.09712,"['Antonia Creswell', 'Murray Shanahan', 'Irina Higgins']",128,,,
"InstructDial: Improving Zero and Few-shot Generalization in Dialogue
through Instruction Tuning
Prakhar Gupta|Cathy Jiao|Yi-Ting",2205.12673,InstructDial: Improving Zero and Few-shot Generalization in Dialogue through Instruction Tuning,"Instruction tuning is an emergent paradigm in NLP wherein natural language
instructions are leveraged with language models to induce zero-shot performance
on unseen tasks. Instructions have been shown to enable good performance on
unseen tasks and datasets in both large and small language models. Dialogue is
an especially interesting area to explore instruction tuning because dialogue
systems perform multiple kinds of tasks related to language (e.g., natural
language understanding and generation, domain-specific interaction), yet
instruction tuning has not been systematically explored for dialogue-related
tasks. We introduce InstructDial, an instruction tuning framework for dialogue,
which consists of a repository of 48 diverse dialogue tasks in a unified
text-to-text format created from 59 openly available dialogue datasets. Next,
we explore cross-task generalization ability on models tuned on InstructDial
across diverse dialogue tasks. Our analysis reveals that InstructDial enables
good zero-shot performance on unseen datasets and tasks such as dialogue
evaluation and intent detection, and even better performance in a few-shot
setting. To ensure that models adhere to instructions, we introduce novel
meta-tasks. We establish benchmark zero-shot and few-shot performance of models
trained using the proposed framework on multiple dialogue tasks.","Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.",http://arxiv.org/pdf/2205.12673,"['Prakhar Gupta', 'Cathy Jiao', 'Yi-Ting Yeh', 'Shikib Mehri', 'Maxine Eskenazi', 'Jeffrey P. Bigham']",128,,,
"A Survey of Race, Racism, and Anti-Racism in NLP
Anjalie Field
Carnegie Mellon University
anjalief@cs.cmu.eduSu Lin Blodgett
Mic",2106.1141,"A Survey of Race, Racism, and Anti-Racism in NLP","Despite inextricable ties between race and language, little work has
considered race in NLP research and development. In this work, we survey 79
papers from the ACL anthology that mention race. These papers reveal various
types of race-related bias in all stages of NLP model development, highlighting
the need for proactive consideration of how NLP systems can uphold racial
hierarchies. However, persistent gaps in research on race and NLP remain: race
has been siloed as a niche topic and remains ignored in many NLP tasks; most
work operationalizes race as a fixed single-dimensional variable with a
ground-truth label, which risks reinforcing differences produced by historical
racism; and the voices of historically marginalized people are nearly absent in
NLP literature. By identifying where and how NLP literature has and has not
considered race, especially in comparison to related fields, our work calls for
inclusion and racial justice in NLP research practices.","Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",http://arxiv.org/pdf/2106.11410,"['Anjalie Field', 'Su Lin Blodgett', 'Zeerak Waseem', 'Yulia Tsvetkov']",128,,,
"DEEP LEARNING FOR SYMBOLIC MATHEMATICS
Guillaume Lample
Facebook AI Research
glample@fb.comFranc ¸ois Charton
Facebook AI Rese",1912.01412,Deep Learning for Symbolic Mathematics,"Neural networks have a reputation for being better at solving statistical or
approximate problems than at performing calculations or working with symbolic
data. In this paper, we show that they can be surprisingly good at more
elaborated tasks in mathematics, such as symbolic integration and solving
differential equations. We propose a syntax for representing mathematical
problems, and methods for generating large datasets that can be used to train
sequence-to-sequence models. We achieve results that outperform commercial
Computer Algebra Systems such as Matlab or Mathematica.","Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",http://arxiv.org/pdf/1912.01412,"['Guillaume Lample', 'François Charton']",128,,,
"INSTRUCTION TUNING WITH GPT-4
Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, Jianfeng Gao
Microsoft Research
fbapeng,",2304.03277,Instruction Tuning with GPT-4,"Prior work has shown that finetuning large language models (LLMs) using
machine-generated instruction-following data enables such models to achieve
remarkable zero-shot capabilities on new tasks, and no human-written
instructions are needed. In this paper, we present the first attempt to use
GPT-4 to generate instruction-following data for LLM finetuning. Our early
experiments on instruction-tuned LLaMA models show that the 52K English and
Chinese instruction-following data generated by GPT-4 leads to superior
zero-shot performance on new tasks to the instruction-following data generated
by previous state-of-the-art models. We also collect feedback and comparison
data from GPT-4 to enable a comprehensive evaluation and reward model training.
We make our data generated using GPT-4 as well as our codebase publicly
available.","Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",http://arxiv.org/pdf/2304.03277,"['Baolin Peng', 'Chunyuan Li', 'Pengcheng He', 'Michel Galley', 'Jianfeng Gao']",128,,,
"Documenting Large Webtext Corpora:
A Case Study on the Colossal Clean Crawled Corpus
Jesse Dodge|Maarten Sap|~Ana Marasovi ´c|~W",2104.08758,Documenting Large Webtext Corpora: A Case Study on the Colossal Clean Crawled Corpus,"Large language models have led to remarkable progress on many NLP tasks, and
researchers are turning to ever-larger text corpora to train them. Some of the
largest corpora available are made by scraping significant portions of the
internet, and are frequently introduced with only minimal documentation. In
this work we provide some of the first documentation for the Colossal Clean
Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set
of filters to a single snapshot of Common Crawl. We begin by investigating
where the data came from, and find a significant amount of text from unexpected
sources like patents and US military websites. Then we explore the content of
the text itself, and find machine-generated text (e.g., from machine
translation systems) and evaluation examples from other benchmark NLP datasets.
To understand the impact of the filters applied to create this dataset, we
evaluate the text that was removed, and show that blocklist filtering
disproportionately removes text from and about minority individuals. Finally,
we conclude with some recommendations for how to created and document web-scale
datasets from a scrape of the internet.","Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",http://arxiv.org/pdf/2104.08758,"['Jesse Dodge', 'Maarten Sap', 'Ana Marasović', 'William Agnew', 'Gabriel Ilharco', 'Dirk Groeneveld', 'Margaret Mitchell', 'Matt Gardner']",128,,,
"Published as a conference paper at ICLR 2023
LEAST -TO-MOST PROMPTING ENABLES COMPLEX
REASONING IN LARGE LANGUAGE MODELS
Denny Z",2205.10625,Least-to-Most Prompting Enables Complex Reasoning in Large Language Models,"Chain-of-thought prompting has demonstrated remarkable performance on various
natural language reasoning tasks. However, it tends to perform poorly on tasks
which requires solving problems harder than the exemplars shown in the prompts.
To overcome this challenge of easy-to-hard generalization, we propose a novel
prompting strategy, least-to-most prompting. The key idea in this strategy is
to break down a complex problem into a series of simpler subproblems and then
solve them in sequence. Solving each subproblem is facilitated by the answers
to previously solved subproblems. Our experimental results on tasks related to
symbolic manipulation, compositional generalization, and math reasoning reveal
that least-to-most prompting is capable of generalizing to more difficult
problems than those seen in the prompts. A notable finding is that when the
GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve
the compositional generalization benchmark SCAN in any split (including length
split) with an accuracy of at least 99% using just 14 exemplars, compared to
only 16% accuracy with chain-of-thought prompting. This is particularly
noteworthy because neural-symbolic models in the literature that specialize in
solving SCAN are trained on the entire training set containing over 15,000
examples. We have included prompts for all the tasks in the Appendix.","Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",http://arxiv.org/pdf/2205.10625,"['Denny Zhou', 'Nathanael Schärli', 'Le Hou', 'Jason Wei', 'Nathan Scales', 'Xuezhi Wang', 'Dale Schuurmans', 'Claire Cui', 'Olivier Bousquet', 'Quoc Le', 'Ed Chi']",128,,,
"Published as a conference paper at ICLR 2023
BINDING LANGUAGE MODELS IN SYMBOLIC LANGUAGES
Zhoujun Cheng ~Tianbao XiePeng S",2210.02875,Binding Language Models in Symbolic Languages,"Though end-to-end neural approaches have recently been dominating NLP tasks
in both performance and ease-of-use, they lack interpretability and robustness.
We propose Binder, a training-free neural-symbolic framework that maps the task
input to a program, which (1) allows binding a unified API of language model
(LM) functionalities to a programming language (e.g., SQL, Python) to extend
its grammar coverage and thus tackle more diverse questions, (2) adopts an LM
as both the program parser and the underlying model called by the API during
execution, and (3) requires only a few in-context exemplar annotations.
Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only
a few in-context exemplars, Codex is able to identify the part of the task
input that cannot be answerable by the original programming language, correctly
generate API calls to prompt Codex to solve the unanswerable part, and identify
where to place the API calls while being compatible with the original grammar.
In the execution stage, Codex can perform versatile functionalities (e.g.,
commonsense QA, information extraction) given proper prompts in the API calls.
Binder achieves state-of-the-art results on WikiTableQuestions and TabFact
datasets, with explicit output programs that benefit human debugging. Note that
previous best systems are all finetuned on tens of thousands of task-specific
samples, while Binder only uses dozens of annotations as in-context exemplars
without any training. Our code is available at https://github.com/HKUNLP/Binder .","Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .",http://arxiv.org/pdf/2210.02875,"['Zhoujun Cheng', 'Tianbao Xie', 'Peng Shi', 'Chengzu Li', 'Rahul Nadkarni', 'Yushi Hu', 'Caiming Xiong', 'Dragomir Radev', 'Mari Ostendorf', 'Luke Zettlemoyer', 'Noah A. Smith', 'Tao Yu']",128,,,
"Published as a conference paper at ICLR 2022
FINETUNED LANGUAGE MODELS AREZERO-SHOT
LEARNERS
Jason Wei, Maarten Bosma, Vincent",2109.01652,Finetuned Language Models Are Zero-Shot Learners,"This paper explores a simple method for improving the zero-shot learning
abilities of language models. We show that instruction tuning -- finetuning
language models on a collection of tasks described via instructions --
substantially improves zero-shot performance on unseen tasks.
  We take a 137B parameter pretrained language model and instruction-tune it on
over 60 NLP tasks verbalized via natural language instruction templates. We
evaluate this instruction-tuned model, which we call FLAN, on unseen task
types. FLAN substantially improves the performance of its unmodified
counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we
evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE,
BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number
of finetuning datasets, model scale, and natural language instructions are key
to the success of instruction tuning.","This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.   We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",http://arxiv.org/pdf/2109.01652,"['Jason Wei', 'Maarten Bosma', 'Vincent Y. Zhao', 'Kelvin Guu', 'Adams Wei Yu', 'Brian Lester', 'Nan Du', 'Andrew M. Dai', 'Quoc V. Le']",128,,,
"Discovering and Categorising Language Biases in Reddit
Xavier Ferrer+, Tom van Nuenen+, Jose M. Such+and Natalia Criado+
+Depar",2008.02754,Discovering and Categorising Language Biases in Reddit,"We present a data-driven approach using word embeddings to discover and
categorise language biases on the discussion platform Reddit. As spaces for
isolated user communities, platforms such as Reddit are increasingly connected
to issues of racism, sexism and other forms of discrimination. Hence, there is
a need to monitor the language of these groups. One of the most promising AI
approaches to trace linguistic biases in large textual datasets involves word
embeddings, which transform text into high-dimensional dense vectors and
capture semantic relations between words. Yet, previous studies require
predefined sets of potential biases to study, e.g., whether gender is more or
less associated with particular types of jobs. This makes these approaches
unfit to deal with smaller and community-centric datasets such as those on
Reddit, which contain smaller vocabularies and slang, as well as biases that
may be particular to that community. This paper proposes a data-driven approach
to automatically discover language biases encoded in the vocabulary of online
discourse communities on Reddit. In our approach, protected attributes are
connected to evaluative words found in the data, which are then categorised
through a semantic analysis system. We verify the effectiveness of our method
by comparing the biases we discover in the Google News dataset with those found
in previous literature. We then successfully discover gender bias, religion
bias, and ethnic bias in different Reddit communities. We conclude by
discussing potential application scenarios and limitations of this data-driven
bias discovery method.","We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.",http://arxiv.org/pdf/2008.02754,"['Xavier Ferrer', 'Tom van Nuenen', 'Jose M. Such', 'Natalia Criado']",128,,,
"AMBIPUN: Generating Puns with Ambiguous Context
Anirudh Mittal2y, Yufei Tian1*, Nanyun Peng1
1Computer Science Department, Univ",2205.01825,AmbiPun: Generating Humorous Puns with Ambiguous Context,"In this paper, we propose a simple yet effective way to generate pun
sentences that does not require any training on existing puns. Our approach is
inspired by humor theories that ambiguity comes from the context rather than
the pun word itself. Given a pair of definitions of a pun word, our model first
produces a list of related concepts through a reverse dictionary. We then
utilize one-shot GPT3 to generate context words and then generate puns
incorporating context words from both concepts. Human evaluation shows that our
method successfully generates pun 52\% of the time, outperforming well-crafted
baselines and the state-of-the-art models by a large margin.","In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52\% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.",http://arxiv.org/pdf/2205.01825,"['Anirudh Mittal', 'Yufei Tian', 'Nanyun Peng']",128,,,
"Learning Transferable Architectures for Scalable Image Recognition
Barret Zoph
Google Brain
barretzoph@google.comVijay Vasudevan",1707.07012,Learning Transferable Architectures for Scalable Image Recognition,"Developing neural network image classification models often requires
significant architecture engineering. In this paper, we study a method to learn
the model architectures directly on the dataset of interest. As this approach
is expensive when the dataset is large, we propose to search for an
architectural building block on a small dataset and then transfer the block to
a larger dataset. The key contribution of this work is the design of a new
search space (the ""NASNet search space"") which enables transferability. In our
experiments, we search for the best convolutional layer (or ""cell"") on the
CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking
together more copies of this cell, each with their own parameters to design a
convolutional architecture, named ""NASNet architecture"". We also introduce a
new regularization technique called ScheduledDropPath that significantly
improves generalization in the NASNet models. On CIFAR-10 itself, NASNet
achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet
achieves, among the published works, state-of-the-art accuracy of 82.7% top-1
and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than
the best human-invented architectures while having 9 billion fewer FLOPS - a
reduction of 28% in computational demand from the previous state-of-the-art
model. When evaluated at different levels of computational cost, accuracies of
NASNets exceed those of the state-of-the-art human-designed models. For
instance, a small version of NASNet also achieves 74% top-1 accuracy, which is
3.1% better than equivalently-sized, state-of-the-art models for mobile
platforms. Finally, the learned features by NASNet used with the Faster-RCNN
framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO
dataset.","Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the NASNet search space) which enables transferability. In our experiments, we search for the best convolutional layer (or cell) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named NASNet architecture. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",http://arxiv.org/pdf/1707.07012,"['Barret Zoph', 'Vijay Vasudevan', 'Jonathon Shlens', 'Quoc V. Le']",128,,,
"I love your chain mail! Making knights smile in a fantasy game world:
Open-domain goal-oriented dialogue agents
Shrimai Prabhumo",2002.02878,I love your chain mail! Making knights smile in a fantasy game world: Open-domain goal-oriented dialogue agents,"Dialogue research tends to distinguish between chit-chat and goal-oriented
tasks. While the former is arguably more naturalistic and has a wider use of
language, the latter has clearer metrics and a straightforward learning signal.
Humans effortlessly combine the two, for example engaging in chit-chat with the
goal of exchanging information or eliciting a specific response. Here, we
bridge the divide between these two domains in the setting of a rich
multi-player text-based fantasy environment where agents and humans engage in
both actions and dialogue. Specifically, we train a goal-oriented model with
reinforcement learning against an imitation-learned ``chit-chat'' model with
two approaches: the policy either learns to pick a topic or learns to pick an
utterance given the top-K utterances from the chit-chat model. We show that
both models outperform an inverse model baseline and can converse naturally
with their dialogue partner in order to achieve goals.","Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.",http://arxiv.org/pdf/2002.02878,"['Shrimai Prabhumoye', 'Margaret Li', 'Jack Urbanek', 'Emily Dinan', 'Douwe Kiela', 'Jason Weston', 'Arthur Szlam']",128,,,
"Detecting Hate Speech with GPT-3 *
Ke-Li Chiu University of Toronto
Annie Collins University of Toronto
Rohan Alexander Universi",2103.12407,Detecting Hate Speech with GPT-3,"Sophisticated language models such as OpenAI's GPT-3 can generate hateful
text that targets marginalized groups. Given this capacity, we are interested
in whether large language models can be used to identify hate speech and
classify text as sexist or racist. We use GPT-3 to identify sexist and racist
text passages with zero-, one-, and few-shot learning. We find that with zero-
and one-shot learning, GPT-3 can identify sexist or racist text with an average
accuracy between 55 per cent and 67 per cent, depending on the category of text
and type of learning. With few-shot learning, the model's accuracy can be as
high as 85 per cent. Large language models have a role to play in hate speech
detection, and with further development they could eventually be used to
counter hate speech.","Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.",http://arxiv.org/pdf/2103.12407,"['Ke-Li Chiu', 'Annie Collins', 'Rohan Alexander']",128,,,
"Adversarial training for high-stakes reliability
Daniel M. Ziegler∗Seraphina Nix Lawrence Chan†Tim Bauman
Peter Schmidt-Nielsen ",2205.01663,Adversarial Training for High-Stakes Reliability,"In the future, powerful AI systems may be deployed in high-stakes settings,
where a single failure could be catastrophic. One technique for improving AI
safety in high-stakes settings is adversarial training, which uses an adversary
to generate examples to train on in order to achieve better worst-case
performance.
  In this work, we used a safe language generation task (``avoid injuries'') as
a testbed for achieving high reliability through adversarial training. We
created a series of adversarial training techniques -- including a tool that
assists human adversaries -- to find and eliminate failures in a classifier
that filters text completions suggested by a generator. In our task, we
determined that we can set very conservative classifier thresholds without
significantly impacting the quality of the filtered outputs. We found that
adversarial training increased robustness to the adversarial attacks that we
trained on -- doubling the time for our contractors to find adversarial
examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44
minutes) -- without affecting in-distribution performance.
  We hope to see further work in the high-stakes reliability setting, including
more powerful tools for enhancing human adversaries and better ways to measure
high levels of reliability, until we can confidently rule out the possibility
of catastrophic deployment-time failures of powerful models.","In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.   In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance.   We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.",http://arxiv.org/pdf/2205.01663,"['Daniel M. Ziegler', 'Seraphina Nix', 'Lawrence Chan', 'Tim Bauman', 'Peter Schmidt-Nielsen', 'Tao Lin', 'Adam Scherlis', 'Noa Nabeshima', 'Ben Weinstein-Raun', 'Daniel de Haas', 'Buck Shlegeris', 'Nate Thomas']",128,,,
"CHATGPT OUTPERFORMS CROWD -WORKERS
FOR TEXT -ANNOTATION TASKS
Fabrizio Gilardi
University of Zurich
Zurich, SwitzerlandMeysam A",2303.15056,ChatGPT Outperforms Crowd-Workers for Text-Annotation Tasks,"Many NLP applications require manual data annotations for a variety of tasks,
notably to train classifiers or evaluate the performance of unsupervised
models. Depending on the size and degree of complexity, the tasks may be
conducted by crowd-workers on platforms such as MTurk as well as trained
annotators, such as research assistants. Using a sample of 2,382 tweets, we
demonstrate that ChatGPT outperforms crowd-workers for several annotation
tasks, including relevance, stance, topics, and frames detection. Specifically,
the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of
five tasks, while ChatGPT's intercoder agreement exceeds that of both
crowd-workers and trained annotators for all tasks. Moreover, the
per-annotation cost of ChatGPT is less than $0.003 -- about twenty times
cheaper than MTurk. These results show the potential of large language models
to drastically increase the efficiency of text classification.","Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.",http://arxiv.org/pdf/2303.15056,"['Fabrizio Gilardi', 'Meysam Alizadeh', 'Maël Kubli']",128,,,
"CodeNet: A Large-Scale AI for Code Dataset for
Learning a Diversity of Coding Tasks
Ruchir Puri1, David S. Kung1, Geert Janssen1",2105.12655,CodeNet: A Large-Scale AI for Code Dataset for Learning a Diversity of Coding Tasks,"Over the last several decades, software has been woven into the fabric of
every aspect of our society. As software development surges and code
infrastructure of enterprise applications ages, it is now more critical than
ever to increase software development productivity and modernize legacy
applications. Advances in deep learning and machine learning algorithms have
enabled numerous breakthroughs, motivating researchers to leverage AI
techniques to improve software development efficiency. Thus, the fast-emerging
research area of AI for Code has garnered new interest and gathered momentum.
In this paper, we present a large-scale dataset CodeNet, consisting of over 14
million code samples and about 500 million lines of code in 55 different
programming languages, which is aimed at teaching AI to code. In addition to
its large scale, CodeNet has a rich set of high-quality annotations to
benchmark and help accelerate research in AI techniques for a variety of
critical coding tasks, including code similarity and classification, code
translation between a large variety of programming languages, and code
performance (runtime and memory) improvement techniques. Additionally, CodeNet
provides sample input and output test sets for 98.5% of the code samples, which
can be used as an oracle for determining code correctness and potentially guide
reinforcement learning for code quality improvements. As a usability feature,
we provide several pre-processing tools in CodeNet to transform source code
into representations that can be readily used as inputs into machine learning
models. Results of code classification and code similarity experiments using
the CodeNet dataset are provided as a reference. We hope that the scale,
diversity and rich, high-quality annotations of CodeNet will offer
unprecedented research opportunities at the intersection of AI and Software
Engineering.","Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of AI for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.",http://arxiv.org/pdf/2105.12655,"['Ruchir Puri', 'David S. Kung', 'Geert Janssen', 'Wei Zhang', 'Giacomo Domeniconi', 'Vladimir Zolotov', 'Julian Dolby', 'Jie Chen', 'Mihir Choudhury', 'Lindsey Decker', 'Veronika Thost', 'Luca Buratti', 'Saurabh Pujar', 'Shyam Ramji', 'Ulrich Finkler', 'Susan Malaika', 'Frederick Reiss']",128,,,
"The Values Encoded in Machine Learning Research
ABEBA BIRHANE∗,Mozilla Foundation & School of Computer Science, University Colle",2106.1559,The Values Encoded in Machine Learning Research,"Machine learning currently exerts an outsized influence on the world,
increasingly affecting institutional practices and impacted communities. It is
therefore critical that we question vague conceptions of the field as
value-neutral or universally beneficial, and investigate what specific values
the field is advancing. In this paper, we first introduce a method and
annotation scheme for studying the values encoded in documents such as research
papers. Applying the scheme, we analyze 100 highly cited machine learning
papers published at premier machine learning conferences, ICML and NeurIPS. We
annotate key features of papers which reveal their values: their justification
for their choice of project, which attributes of their project they uplift,
their consideration of potential negative consequences, and their institutional
affiliations and funding sources. We find that few of the papers justify how
their project connects to a societal need (15\%) and far fewer discuss negative
potential (1\%). Through line-by-line content analysis, we identify 59 values
that are uplifted in ML research, and, of these, we find that the papers most
frequently justify and assess themselves based on Performance, Generalization,
Quantitative evidence, Efficiency, Building on past work, and Novelty. We
present extensive textual evidence and identify key themes in the definitions
and operationalization of these values. Notably, we find systematic textual
evidence that these top values are being defined and applied with assumptions
and implications generally supporting the centralization of power.Finally, we
find increasingly close ties between these highly cited papers and tech
companies and elite universities.","Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power.Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",http://arxiv.org/pdf/2106.15590,"['Abeba Birhane', 'Pratyusha Kalluri', 'Dallas Card', 'William Agnew', 'Ravit Dotan', 'Michelle Bao']",128,,,
"Emergence of Locomotion Behaviours
in Rich Environments
Nicolas Heess, Dhruva TB, Srinivasan Sriram, Jay Lemmon, Josh Merel, Gre",1707.02286,Emergence of Locomotion Behaviours in Rich Environments,"The reinforcement learning paradigm allows, in principle, for complex
behaviours to be learned directly from simple reward signals. In practice,
however, it is common to carefully hand-design the reward function to encourage
a particular solution, or to derive it from demonstration data. In this paper
explore how a rich environment can help to promote the learning of complex
behavior. Specifically, we train agents in diverse environmental contexts, and
find that this encourages the emergence of robust behaviours that perform well
across a suite of tasks. We demonstrate this principle for locomotion --
behaviours that are known for their sensitivity to the choice of reward. We
train several simulated bodies on a diverse set of challenging terrains and
obstacles, using a simple reward function based on forward progress. Using a
novel scalable variant of policy gradient reinforcement learning, our agents
learn to run, jump, crouch and turn as required by the environment without
explicit reward-based guidance. A visual depiction of highlights of the learned
behavior can be viewed following https://youtu.be/hx_bgoTF7bs .","The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .",http://arxiv.org/pdf/1707.02286,"['Nicolas Heess', 'Dhruva TB', 'Srinivasan Sriram', 'Jay Lemmon', 'Josh Merel', 'Greg Wayne', 'Yuval Tassa', 'Tom Erez', 'Ziyu Wang', 'S. M. Ali Eslami', 'Martin Riedmiller', 'David Silver']",128,,,
"Montreal AI Ethics Institute
 
 
An international, non-profit research institute helping humanity define its place in a world
 
",2006.06217,SECure: A Social and Environmental Certificate for AI Systems,"In a world increasingly dominated by AI applications, an understudied aspect
is the carbon and social footprint of these power-hungry algorithms that
require copious computation and a trove of data for training and prediction.
While profitable in the short-term, these practices are unsustainable and
socially extractive from both a data-use and energy-use perspective. This work
proposes an ESG-inspired framework combining socio-technical measures to build
eco-socially responsible AI systems. The framework has four pillars:
compute-efficient machine learning, federated learning, data sovereignty, and a
LEEDesque certificate.
  Compute-efficient machine learning is the use of compressed network
architectures that show marginal decreases in accuracy. Federated learning
augments the first pillar's impact through the use of techniques that
distribute computational loads across idle capacity on devices. This is paired
with the third pillar of data sovereignty to ensure the privacy of user data
via techniques like use-based privacy and differential privacy. The final
pillar ties all these factors together and certifies products and services in a
standardized manner on their environmental and social impacts, allowing
consumers to align their purchase with their values.","In a world increasingly dominated by AI applications, an understudied aspect is the carbon and social footprint of these power-hungry algorithms that require copious computation and a trove of data for training and prediction. While profitable in the short-term, these practices are unsustainable and socially extractive from both a data-use and energy-use perspective. This work proposes an ESG-inspired framework combining socio-technical measures to build eco-socially responsible AI systems. The framework has four pillars: compute-efficient machine learning, federated learning, data sovereignty, and a LEEDesque certificate.   Compute-efficient machine learning is the use of compressed network architectures that show marginal decreases in accuracy. Federated learning augments the first pillar's impact through the use of techniques that distribute computational loads across idle capacity on devices. This is paired with the third pillar of data sovereignty to ensure the privacy of user data via techniques like use-based privacy and differential privacy. The final pillar ties all these factors together and certifies products and services in a standardized manner on their environmental and social impacts, allowing consumers to align their purchase with their values.",http://arxiv.org/pdf/2006.06217,"['Abhishek Gupta', 'Camylle Lanteigne', 'Sara Kingsley']",128,,,
"TeraPipe: Token-Level Pipeline Parallelism for Training
Large-Scale Language Models
Zhuohan Li1Siyuan Zhuang1Shiyuan Guo1Danyang",2102.07988,TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models,"Model parallelism has become a necessity for training modern large-scale deep
language models. In this work, we identify a new and orthogonal dimension from
existing model parallel approaches: it is possible to perform pipeline
parallelism within a single training sequence for Transformer-based language
models thanks to its autoregressive property. This enables a more fine-grained
pipeline compared with previous work. With this key idea, we design TeraPipe, a
high-performance token-level pipeline parallel algorithm for synchronous
model-parallel training of Transformer-based language models. We develop a
novel dynamic programming-based algorithm to calculate the optimal pipelining
execution scheme given a specific model and cluster configuration. We show that
TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175
billion parameters on an AWS cluster with 48 p3.16xlarge instances compared
with state-of-the-art model-parallel methods. The code for reproduction can be
found at https://github.com/zhuohan123/terapipe","Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe",http://arxiv.org/pdf/2102.07988,"['Zhuohan Li', 'Siyuan Zhuang', 'Shiyuan Guo', 'Danyang Zhuo', 'Hao Zhang', 'Dawn Song', 'Ion Stoica']",128,,,
"Learning to summarize from human feedback
Nisan StiennonLong OuyangJeff WuDaniel M. ZieglerRyan Lowe
Chelsea VossAlec Radf",2009.01325,Learning to summarize from human feedback,"As language models become more powerful, training and evaluation are
increasingly bottlenecked by the data and metrics used for a particular task.
For example, summarization models are often trained to predict human reference
summaries and evaluated using ROUGE, but both of these metrics are rough
proxies for what we really care about -- summary quality. In this work, we show
that it is possible to significantly improve summary quality by training a
model to optimize for human preferences. We collect a large, high-quality
dataset of human comparisons between summaries, train a model to predict the
human-preferred summary, and use that model as a reward function to fine-tune a
summarization policy using reinforcement learning. We apply our method to a
version of the TL;DR dataset of Reddit posts and find that our models
significantly outperform both human reference summaries and much larger models
fine-tuned with supervised learning alone. Our models also transfer to CNN/DM
news articles, producing summaries nearly as good as the human reference
without any news-specific fine-tuning. We conduct extensive analyses to
understand our human feedback dataset and fine-tuned models We establish that
our reward model generalizes to new datasets, and that optimizing our reward
model results in better summaries than optimizing ROUGE according to humans. We
hope the evidence from our paper motivates machine learning researchers to pay
closer attention to how their training loss affects the model behavior they
actually want.","As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",http://arxiv.org/pdf/2009.01325,"['Nisan Stiennon', 'Long Ouyang', 'Jeff Wu', 'Daniel M. Ziegler', 'Ryan Lowe', 'Chelsea Voss', 'Alec Radford', 'Dario Amodei', 'Paul Christiano']",128,,,
"Searching for Fast Model Families on Datacenter Accelerators
Sheng Li, Mingxing Tan, Ruoming Pang, Andrew Li, Liqun Cheng, Quoc ",2102.0561,Searching for Fast Model Families on Datacenter Accelerators,"Neural Architecture Search (NAS), together with model scaling, has shown
remarkable progress in designing high accuracy and fast convolutional
architecture families. However, as neither NAS nor model scaling considers
sufficient hardware architecture details, they do not take full advantage of
the emerging datacenter (DC) accelerators. In this paper, we search for fast
and accurate CNN model families for efficient inference on DC accelerators. We
first analyze DC accelerators and find that existing CNNs suffer from
insufficient operational intensity, parallelism, and execution efficiency.
These insights let us create a DC-accelerator-optimized search space, with
space-to-depth, space-to-batch, hybrid fused convolution structures with
vanilla and depthwise convolutions, and block-wise activation functions. On top
of our DC accelerator optimized neural architecture search space, we further
propose a latency-aware compound scaling (LACS), the first multi-objective
compound scaling method optimizing both accuracy and latency. Our LACS
discovers that network depth should grow much faster than image size and
network width, which is quite different from previous compound scaling results.
With the new search space and LACS, our search and scaling on datacenter
accelerators results in a new model series named EfficientNet-X. EfficientNet-X
is up to more than 2X faster than EfficientNet (a model series with
state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with
comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet
and ResNeSt on TPUv3 and GPUv100.","Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. On top of our DC accelerator optimized neural architecture search space, we further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from previous compound scaling results. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than EfficientNet (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100.",http://arxiv.org/pdf/2102.05610,"['Sheng Li', 'Mingxing Tan', 'Ruoming Pang', 'Andrew Li', 'Liqun Cheng', 'Quoc Le', 'Norman P. Jouppi']",128,,,
"Crosslingual Generalization through Multitask Finetuning
Niklas Muennighoff1Thomas Wang1Lintang Sutawika2,3Adam Roberts4Stella B",2211.01786,Crosslingual Generalization through Multitask Finetuning,"Multitask prompted finetuning (MTF) has been shown to help large language
models generalize to new tasks in a zero-shot setting, but so far explorations
of MTF have focused on English data and models. We apply MTF to the pretrained
multilingual BLOOM and mT5 model families to produce finetuned variants called
BLOOMZ and mT0. We find finetuning large multilingual language models on
English tasks with English prompts allows for task generalization to
non-English languages that appear only in the pretraining corpus. Finetuning on
multilingual tasks with English prompts further improves performance on English
and non-English tasks leading to various state-of-the-art zero-shot results. We
also investigate finetuning on multilingual tasks with prompts that have been
machine-translated from English to match the language of each dataset. We find
training on these machine-translated prompts leads to better performance on
human-written prompts in the respective languages. Surprisingly, we find models
are capable of zero-shot generalization to tasks in languages they have never
intentionally seen. We conjecture that the models are learning higher-level
capabilities that are both task- and language-agnostic. In addition, we
introduce xP3, a composite of supervised datasets in 46 languages with English
and machine-translated prompts. Our code, datasets and models are freely
available at https://github.com/bigscience-workshop/xmtf.","Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",http://arxiv.org/pdf/2211.01786,"['Niklas Muennighoff', 'Thomas Wang', 'Lintang Sutawika', 'Adam Roberts', 'Stella Biderman', 'Teven Le Scao', 'M Saiful Bari', 'Sheng Shen', 'Zheng-Xin Yong', 'Hailey Schoelkopf', 'Xiangru Tang', 'Dragomir Radev', 'Alham Fikri Aji', 'Khalid Almubarak', 'Samuel Albanie', 'Zaid Alyafeai', 'Albert Webson', 'Edward Raff', 'Colin Raffel']",128,,,
"Improving Multi-Task Deep Neural Networks via Knowledge Distillation
for Natural Language Understanding
Xiaodong Liu1, Pengcheng",1904.09482,Improving Multi-Task Deep Neural Networks via Knowledge Distillation for Natural Language Understanding,"This paper explores the use of knowledge distillation to improve a Multi-Task
Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text
representations across multiple natural language understanding tasks. Although
ensemble learning can improve model performance, serving an ensemble of large
DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge
distillation method (Hinton et al., 2015) in the multi-task learning setting.
For each task, we train an ensemble of different MT-DNNs (teacher) that
outperforms any single model, and then train a single MT-DNN (student) via
multi-task learning to \emph{distill} knowledge from these ensemble teachers.
We show that the distilled MT-DNN significantly outperforms the original MT-DNN
on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\%
(1.5\% absolute improvement\footnote{ Based on the GLUE leaderboard at
https://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and
pre-trained models will be made publicly available at
https://github.com/namisan/mt-dnn.","This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to \emph{distill} knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\% (1.5\% absolute improvement\footnote{ Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and pre-trained models will be made publicly available at https://github.com/namisan/mt-dnn.",http://arxiv.org/pdf/1904.09482,"['Xiaodong Liu', 'Pengcheng He', 'Weizhu Chen', 'Jianfeng Gao']",128,,,
"Towards Reasoning in Large Language Models: A Survey
Jie Huang Kevin Chen-Chuan Chang
Department of Computer Science, University",2212.10403,Towards Reasoning in Large Language Models: A Survey,"Reasoning is a fundamental aspect of human intelligence that plays a crucial
role in activities such as problem solving, decision making, and critical
thinking. In recent years, large language models (LLMs) have made significant
progress in natural language processing, and there is observation that these
models may exhibit reasoning abilities when they are sufficiently large.
However, it is not yet clear to what extent LLMs are capable of reasoning. This
paper provides a comprehensive overview of the current state of knowledge on
reasoning in LLMs, including techniques for improving and eliciting reasoning
in these models, methods and benchmarks for evaluating reasoning abilities,
findings and implications of previous research in this field, and suggestions
on future directions. Our aim is to provide a detailed and up-to-date review of
this topic and stimulate meaningful discussion and future work.","Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",http://arxiv.org/pdf/2212.10403,"['Jie Huang', 'Kevin Chen-Chuan Chang']",128,,,
"Quality at a Glance:
An Audit of Web-Crawled Multilingual Datasets
Julia Kreutzera;b, Isaac Caswella, Lisa Wanga, Ahsan Wahabc, ",2103.12028,Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets,"With the success of large-scale pre-training and multilingual modeling in
Natural Language Processing (NLP), recent years have seen a proliferation of
large, web-mined text datasets covering hundreds of languages. We manually
audit the quality of 205 language-specific corpora released with five major
public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource
corpora have systematic issues: At least 15 corpora have no usable text, and a
significant fraction contains less than 50% sentences of acceptable quality. In
addition, many are mislabeled or use nonstandard/ambiguous language codes. We
demonstrate that these issues are easy to detect even for non-proficient
speakers, and supplement the human audit with automatic analyses. Finally, we
recommend techniques to evaluate and improve multilingual corpora and discuss
potential risks that come with low-quality data releases.","With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.",http://arxiv.org/pdf/2103.12028,"['Julia Kreutzer', 'Isaac Caswell', 'Lisa Wang', 'Ahsan Wahab', 'Daan van Esch', 'Nasanbayar Ulzii-Orshikh', 'Allahsera Tapo', 'Nishant Subramani', 'Artem Sokolov', 'Claytone Sikasote', 'Monang Setyawan', 'Supheakmungkol Sarin', 'Sokhar Samb', 'Benoît Sagot', 'Clara Rivera', 'Annette Rios', 'Isabel Papadimitriou', 'Salomey Osei', 'Pedro Ortiz Suarez', 'Iroro Orife', 'Kelechi Ogueji', 'Andre Niyongabo Rubungo', 'Toan Q. Nguyen', 'Mathias Müller', 'André Müller', 'Shamsuddeen Hassan Muhammad', 'Nanda Muhammad', 'Ayanda Mnyakeni', 'Jamshidbek Mirzakhalov', 'Tapiwanashe Matangira', 'Colin Leong', 'Nze Lawson', 'Sneha Kudugunta', 'Yacine Jernite', 'Mathias Jenny', 'Orhan Firat', 'Bonaventure F. P. Dossou', 'Sakhile Dlamini', 'Nisansa de Silva', 'Sakine Çabuk Ballı', 'Stella Biderman', 'Alessia Battisti', 'Ahmed Baruwa', 'Ankur Bapna', 'Pallavi Baljekar', 'Israel Abebe Azime', 'Ayodele Awokoya', 'Duygu Ataman', 'Orevaoghene Ahia', 'Oghenefego Ahia', 'Sweta Agrawal', 'Mofetoluwa Adeyemi']",128,,,
"Will we run out of data? An analysis of the limits
of scaling datasets in Machine Learning
Pablo Villalobos, Jaime Sevillay, L",2211.04325,Will we run out of data? An analysis of the limits of scaling datasets in Machine Learning,"We analyze the growth of dataset sizes used in machine learning for natural
language processing and computer vision, and extrapolate these using two
methods; using the historical growth rate and estimating the compute-optimal
dataset size for future predicted compute budgets. We investigate the growth in
data usage by estimating the total stock of unlabeled data available on the
internet over the coming decades. Our analysis indicates that the stock of
high-quality language data will be exhausted soon; likely before 2026. By
contrast, the stock of low-quality language data and image data will be
exhausted only much later; between 2030 and 2050 (for low-quality language) and
between 2030 and 2060 (for images). Our work suggests that the current trend of
ever-growing ML models that rely on enormous datasets might slow down if data
efficiency is not drastically improved or new sources of data become available.","We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.",http://arxiv.org/pdf/2211.04325,"['Pablo Villalobos', 'Jaime Sevilla', 'Lennart Heim', 'Tamay Besiroglu', 'Marius Hobbhahn', 'Anson Ho']",128,,,
"Transformer-XL: Attentive Language Models
Beyond a Fixed-Length Context
Zihang Dai12, Zhilin Yang12, Yiming Yang1, Jaime Carbo",1901.0286,Transformer-XL: Attentive Language Models Beyond a Fixed-Length Context,"Transformers have a potential of learning longer-term dependency, but are
limited by a fixed-length context in the setting of language modeling. We
propose a novel neural architecture Transformer-XL that enables learning
dependency beyond a fixed length without disrupting temporal coherence. It
consists of a segment-level recurrence mechanism and a novel positional
encoding scheme. Our method not only enables capturing longer-term dependency,
but also resolves the context fragmentation problem. As a result,
Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer
than vanilla Transformers, achieves better performance on both short and long
sequences, and is up to 1,800+ times faster than vanilla Transformers during
evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity
to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion
Word, and 54.5 on Penn Treebank (without finetuning). When trained only on
WikiText-103, Transformer-XL manages to generate reasonably coherent, novel
text articles with thousands of tokens. Our code, pretrained models, and
hyperparameters are available in both Tensorflow and PyTorch.","Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",http://arxiv.org/pdf/1901.02860,"['Zihang Dai', 'Zhilin Yang', 'Yiming Yang', 'Jaime Carbonell', 'Quoc V. Le', 'Ruslan Salakhutdinov']",128,,,
"ALERT : Adapting Language Models to Reasoning Tasks
Ping YuTianlu WangOlga GolovnevaBadr AlKhamissi4
Siddharth Verma4Zhijing ",2212.08286,ALERT: Adapting Language Models to Reasoning Tasks,"Current large language models can perform reasonably well on complex tasks
that require step-by-step reasoning with few-shot learning. Are these models
applying reasoning skills they have learnt during pre-training and reason
outside of their training context, or are they simply memorizing their training
corpus at finer granularity and have learnt to better understand their context?
To tease apart these possibilities, we introduce ALERT, a benchmark and suite
of analyses for assessing language models' reasoning ability comparing
pre-trained and finetuned models on complex tasks that require reasoning skills
to solve. ALERT provides a test bed to asses any language model on fine-grained
reasoning skills, which spans over 20 datasets and covers 10 different
reasoning skills. We leverage ALERT to further investigate the role of
finetuning. With extensive empirical analysis we find that language models
learn more reasoning skills such as textual entailment, abductive reasoning,
and analogical reasoning during finetuning stage compared to pretraining state.
We also find that when language models are finetuned they tend to overfit to
the prompt template, which hurts the robustness of models causing
generalization problems.","Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",http://arxiv.org/pdf/2212.08286,"['Ping Yu', 'Tianlu Wang', 'Olga Golovneva', 'Badr AlKhamissi', 'Siddharth Verma', 'Zhijing Jin', 'Gargi Ghosh', 'Mona Diab', 'Asli Celikyilmaz']",128,,,
"Looped Transformers as Programmable Computers
Angeliki Giannouw*, Shashank Rajputw, Jy-yong Sohnw,
Kangwook Leew, Jason D. Leep",2301.13196,Looped Transformers as Programmable Computers,"We present a framework for using transformer networks as universal computers
by programming them with specific weights and placing them in a loop. Our input
sequence acts as a punchcard, consisting of instructions and memory for data
read/writes. We demonstrate that a constant number of encoder layers can
emulate basic computing blocks, including embedding edit operations, non-linear
functions, function calls, program counters, and conditional branches. Using
these building blocks, we emulate a small instruction-set computer. This allows
us to map iterative algorithms to programs that can be executed by a looped,
13-layer transformer. We show how this transformer, instructed by its input,
can emulate a basic calculator, a basic linear algebra library, and in-context
learning algorithms that employ backpropagation. Our work highlights the
versatility of the attention mechanism, and demonstrates that even shallow
transformers can execute full-fledged, general-purpose programs.","We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.",http://arxiv.org/pdf/2301.13196,"['Angeliki Giannou', 'Shashank Rajput', 'Jy-yong Sohn', 'Kangwook Lee', 'Jason D. Lee', 'Dimitris Papailiopoulos']",128,,,
"Evaluating the Underlying Gender Bias in
Contextualized Word Embeddings
Christine Basta Marta R. Costa-juss `a Noe Casas
Univers",1904.08783,Evaluating the Underlying Gender Bias in Contextualized Word Embeddings,"Gender bias is highly impacting natural language processing applications.
Word embeddings have clearly been proven both to keep and amplify gender biases
that are present in current data sources. Recently, contextualized word
embeddings have enhanced previous word embedding techniques by computing word
vector representations dependent on the sentence they appear in.
  In this paper, we study the impact of this conceptual change in the word
embedding computation in relation with gender bias. Our analysis includes
different measures previously applied in the literature to standard word
embeddings. Our findings suggest that contextualized word embeddings are less
biased than standard ones even when the latter are debiased.","Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in.   In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.",http://arxiv.org/pdf/1904.08783,"['Christine Basta', 'Marta R. Costa-jussà', 'Noe Casas']",128,,,
"Universal Language Model Fine-tuning for Text Classiﬁcation
Jeremy Howard
fast.ai
University of San Francisco
j@fast.aiSebastia",1801.06146,Universal Language Model Fine-tuning for Text Classification,"Inductive transfer learning has greatly impacted computer vision, but
existing approaches in NLP still require task-specific modifications and
training from scratch. We propose Universal Language Model Fine-tuning
(ULMFiT), an effective transfer learning method that can be applied to any task
in NLP, and introduce techniques that are key for fine-tuning a language model.
Our method significantly outperforms the state-of-the-art on six text
classification tasks, reducing the error by 18-24% on the majority of datasets.
Furthermore, with only 100 labeled examples, it matches the performance of
training from scratch on 100x more data. We open-source our pretrained models
and code.","Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",http://arxiv.org/pdf/1801.06146,"['Jeremy Howard', 'Sebastian Ruder']",128,,,
"Chat as Expected: Learning to Manipulate
Black-box Neural Dialogue Models
Haochen Liu
Michigan State University
liuhaoc1@msu.edu",2005.1317,Chat as Expected: Learning to Manipulate Black-box Neural Dialogue Models,"Recently, neural network based dialogue systems have become ubiquitous in our
increasingly digitalized society. However, due to their inherent opaqueness,
some recently raised concerns about using neural models are starting to be
taken seriously. In fact, intentional or unintentional behaviors could lead to
a dialogue system to generate inappropriate responses. Thus, in this paper, we
investigate whether we can learn to craft input sentences that result in a
black-box neural dialogue model being manipulated into having its outputs
contain target words or match target sentences. We propose a reinforcement
learning based model that can generate such desired inputs automatically.
Extensive experiments on a popular well-trained state-of-the-art neural
dialogue model show that our method can successfully seek out desired inputs
that lead to the target outputs in a considerable portion of cases.
Consequently, our work reveals the potential of neural dialogue models to be
manipulated, which inspires and opens the door towards developing strategies to
defend them.","Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.",http://arxiv.org/pdf/2005.13170,"['Haochen Liu', 'Zhiwei Wang', 'Tyler Derr', 'Jiliang Tang']",128,,,
"Fine-tuned Language Models are Continual Learners
Thomas Scialom1Tuhin Chakrabarty2Smaranda Muresan2
1Meta AI
2Department of C",2205.12393,Fine-tuned Language Models are Continual Learners,"Recent work on large language models relies on the intuition that most
natural language processing tasks can be described via natural language
instructions. Language models trained on these instructions show strong
zero-shot performance on several standard datasets. However, these models even
though impressive still perform poorly on a wide range of tasks outside of
their respective training and evaluation sets. To address this limitation, we
argue that a model should be able to keep extending its knowledge and
abilities, without forgetting previous skills. In spite of the limited success
of Continual Learning we show that Language Models can be continual learners.
We empirically investigate the reason for this success and conclude that
Continual Learning emerges from self-supervision pre-training. Our resulting
model Continual-T0 (CT0) is able to learn diverse new tasks, while still
maintaining good performance on previous tasks, spanning remarkably through 70
datasets in total. Finally, we show that CT0 is able to combine instructions in
ways it was never trained for, demonstrating some compositionality.","Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.",http://arxiv.org/pdf/2205.12393,"['Thomas Scialom', 'Tuhin Chakrabarty', 'Smaranda Muresan']",128,,,
"GPT-NeoX-20B: An Open-Source Autoregressive Language Model
Sid Black * Stella Biderman * Eric Hallahan *
Quentin Anthony Leo Gao",2204.06745,GPT-NeoX-20B: An Open-Source Autoregressive Language Model,"We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language
model trained on the Pile, whose weights will be made freely and openly
available to the public through a permissive license. It is, to the best of our
knowledge, the largest dense autoregressive model that has publicly available
weights at the time of submission. In this work, we describe \model{}'s
architecture and training and evaluate its performance on a range of
language-understanding, mathematics, and knowledge-based tasks. We find that
GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in
performance when evaluated five-shot than similarly sized GPT-3 and FairSeq
models. We open-source the training and evaluation code, as well as the model
weights, at https://github.com/EleutherAI/gpt-neox.","We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",http://arxiv.org/pdf/2204.06745,"['Sid Black', 'Stella Biderman', 'Eric Hallahan', 'Quentin Anthony', 'Leo Gao', 'Laurence Golding', 'Horace He', 'Connor Leahy', 'Kyle McDonell', 'Jason Phang', 'Michael Pieler', 'USVSN Sai Prashanth', 'Shivanshu Purohit', 'Laria Reynolds', 'Jonathan Tow', 'Ben Wang', 'Samuel Weinbach']",128,,,
"ESTIMATING THE CARBON FOOTPRINT OF BLOOM,
A176B P ARAMETER LANGUAGE MODEL
Alexandra Sasha Luccioni
Hugging Face
sasha.luccioni@h",2211.02001,"Estimating the Carbon Footprint of BLOOM, a 176B Parameter Language Model","Progress in machine learning (ML) comes with a cost to the environment, given
that training ML models requires significant computational resources, energy
and materials. In the present article, we aim to quantify the carbon footprint
of BLOOM, a 176-billion parameter language model, across its life cycle. We
estimate that BLOOM's final training emitted approximately 24.7 tonnes
of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes
if we account for all processes ranging from equipment manufacturing to
energy-based operational consumption. We also study the energy requirements and
carbon emissions of its deployment for inference via an API endpoint receiving
user queries in real-time. We conclude with a discussion regarding the
difficulty of precisely estimating the carbon footprint of ML models and future
research directions that can contribute towards improving carbon emissions
reporting.","Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.",http://arxiv.org/pdf/2211.02001,"['Alexandra Sasha Luccioni', 'Sylvain Viguier', 'Anne-Laure Ligozat']",128,,,
"Task-aware Retrieval with Instructions
Akari Asaiyz, Timo Schicky, Patrick Lewisy, Xilun Cheny, Gautier Izacardy,
Sebastian Rie",2211.0926,Task-aware Retrieval with Instructions,"We study the problem of retrieval with instructions, where users of a
retrieval system explicitly describe their intent along with their queries. We
aim to develop a general-purpose task-aware retrieval system using multi-task
instruction tuning, which can follow human-written instructions to find the
best documents for a given query. We introduce the first large-scale collection
of approximately 40 retrieval datasets with instructions, BERRI, and present
TART, a multi-task retrieval system trained on BERRI with instructions. TART
shows strong capabilities to adapt to a new retrieval task via instructions and
advances the state of the art on two zero-shot retrieval benchmarks, BEIR and
LOTTE, outperforming models up to three times larger. We further introduce a
new evaluation setup, X^2-Retrieval to better reflect real-world scenarios,
where diverse domains and tasks are pooled and a system needs to find documents
aligning users' intents. In this setup, TART significantly outperforms
competitive baselines, further demonstrating the effectiveness of guiding
retrieval with instructions.","We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.",http://arxiv.org/pdf/2211.09260,"['Akari Asai', 'Timo Schick', 'Patrick Lewis', 'Xilun Chen', 'Gautier Izacard', 'Sebastian Riedel', 'Hannaneh Hajishirzi', 'Wen-tau Yih']",128,,,
"Under review as a conference paper at ICLR 2016
UNSUPERVISED REPRESENTATION LEARNING
WITH DEEPCONVOLUTIONAL
GENERATIVE ADVERSARI",1511.06434,Unsupervised Representation Learning with Deep Convolutional Generative Adversarial Networks,"In recent years, supervised learning with convolutional networks (CNNs) has
seen huge adoption in computer vision applications. Comparatively, unsupervised
learning with CNNs has received less attention. In this work we hope to help
bridge the gap between the success of CNNs for supervised learning and
unsupervised learning. We introduce a class of CNNs called deep convolutional
generative adversarial networks (DCGANs), that have certain architectural
constraints, and demonstrate that they are a strong candidate for unsupervised
learning. Training on various image datasets, we show convincing evidence that
our deep convolutional adversarial pair learns a hierarchy of representations
from object parts to scenes in both the generator and discriminator.
Additionally, we use the learned features for novel tasks - demonstrating their
applicability as general image representations.","In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",http://arxiv.org/pdf/1511.06434,"['Alec Radford', 'Luke Metz', 'Soumith Chintala']",128,,,
"Published as a conference paper at ICLR 2020
A C ONSTRUCTIVE PREDICTION OF THE
GENERALIZATION ERROR ACROSS SCALES
Jonathan S. Ro",1909.12673,A Constructive Prediction of the Generalization Error Across Scales,"The dependency of the generalization error of neural networks on model and
dataset size is of critical importance both in practice and for understanding
the theory of neural networks. Nevertheless, the functional form of this
dependency remains elusive. In this work, we present a functional form which
approximates well the generalization error in practice. Capitalizing on the
successful concept of model scaling (e.g., width, depth), we are able to
simultaneously construct such a form and specify the exact models which can
attain it across model/data scales. Our construction follows insights obtained
from observations conducted over a range of model/data scales, in various model
types and datasets, in vision and language tasks. We show that the form both
fits the observations well across scales, and provides accurate predictions
from small- to large-scale models and data.","The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.",http://arxiv.org/pdf/1909.12673,"['Jonathan S. Rosenfeld', 'Amir Rosenfeld', 'Yonatan Belinkov', 'Nir Shavit']",128,,,
"RAINIER : Reinforced Knowledge Introspector
for Commonsense Question Answering
Jiacheng Liu~Skyler Hallinan~Ximing Lu~Pengfei H",2210.03078,Rainier: Reinforced Knowledge Introspector for Commonsense Question Answering,"Knowledge underpins reasoning. Recent research demonstrates that when
relevant knowledge is provided as additional context to commonsense question
answering (QA), it can substantially enhance the performance even on top of
state-of-the-art. The fundamental challenge is where and how to find such
knowledge that is high quality and on point with respect to the question;
knowledge retrieved from knowledge bases are incomplete and knowledge generated
from language models are inconsistent. We present Rainier, or Reinforced
Knowledge Introspector, that learns to generate contextually relevant knowledge
in response to given questions. Our approach starts by imitating knowledge
generated by GPT-3, then learns to generate its own knowledge via reinforcement
learning where rewards are shaped based on the increased performance on the
resulting question answering. Rainier demonstrates substantial and consistent
performance gains when tested over 9 different commonsense benchmarks:
including 5 datasets that are seen during model training, as well as 4 datasets
that are kept unseen. Our work is the first to report that knowledge generated
by models that are orders of magnitude smaller than GPT-3, even without direct
supervision on the knowledge itself, can exceed the quality of commonsense
knowledge elicited from GPT-3.","Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.",http://arxiv.org/pdf/2210.03078,"['Jiacheng Liu', 'Skyler Hallinan', 'Ximing Lu', 'Pengfei He', 'Sean Welleck', 'Hannaneh Hajishirzi', 'Yejin Choi']",128,,,
"Generate & Rank: A Multi-task Framework
for Math Word Problems
Jianhao Shen1y, Yichun Yin2, Lin Li3, Lifeng Shang2,
Xin Jiang2,M",2109.03034,Generate & Rank: A Multi-task Framework for Math Word Problems,"Math word problem (MWP) is a challenging and critical task in natural
language processing. Many recent studies formalize MWP as a generation task and
have adopted sequence-to-sequence models to transform problem descriptions to
mathematical expressions. However, mathematical expressions are prone to minor
mistakes while the generation objective does not explicitly handle such
mistakes. To address this limitation, we devise a new ranking task for MWP and
propose Generate & Rank, a multi-task framework based on a generative
pre-trained language model. By joint training with generation and ranking, the
model learns from its own mistakes and is able to distinguish between correct
and incorrect expressions. Meanwhile, we perform tree-based disturbance
specially designed for MWP and an online update to boost the ranker. We
demonstrate the effectiveness of our proposed method on the benchmark and the
results show that our method consistently outperforms baselines in all
datasets. Particularly, in the classical Math23k, our method is 7% (78.4%
$\rightarrow$ 85.4%) higher than the state-of-the-art.","Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\rightarrow$ 85.4%) higher than the state-of-the-art.",http://arxiv.org/pdf/2109.03034,"['Jianhao Shen', 'Yichun Yin', 'Lin Li', 'Lifeng Shang', 'Xin Jiang', 'Ming Zhang', 'Qun Liu']",128,,,
"SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence
Embedding
Harish Tayyar Madabushi1, Edward Gow-Smith1,
Mar",2204.1005,SemEval-2022 Task 2: Multilingual Idiomaticity Detection and Sentence Embedding,"This paper presents the shared task on Multilingual Idiomaticity Detection
and Sentence Embedding, which consists of two subtasks: (a) a binary
classification task aimed at identifying whether a sentence contains an
idiomatic expression, and (b) a task based on semantic text similarity which
requires the model to adequately represent potentially idiomatic expressions in
context. Each subtask includes different settings regarding the amount of
training data. Besides the task description, this paper introduces the datasets
in English, Portuguese, and Galician and their annotation procedure, the
evaluation metrics, and a summary of the participant systems and their results.
The task had close to 100 registered participants organised into twenty five
teams making over 650 and 150 submissions in the practice and evaluation phases
respectively.","This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.",http://arxiv.org/pdf/2204.10050,"['Harish Tayyar Madabushi', 'Edward Gow-Smith', 'Marcos Garcia', 'Carolina Scarton', 'Marco Idiart', 'Aline Villavicencio']",128,,,
"NL-Augmenter
A Framework for Task-Sensitive Natural Language Augmentation
December 5, 2021
Kaustubh D. Dhole3;18†, Varun Gangal7",2112.02721,NL-Augmenter: A Framework for Task-Sensitive Natural Language Augmentation,"Data augmentation is an important component in the robustness evaluation of
models in natural language processing (NLP) and in enhancing the diversity of
the data they are trained on. In this paper, we present NL-Augmenter, a new
participatory Python-based natural language augmentation framework which
supports the creation of both transformations (modifications to the data) and
filters (data splits according to specific features). We describe the framework
and an initial set of 117 transformations and 23 filters for a variety of
natural language tasks. We demonstrate the efficacy of NL-Augmenter by using
several of its transformations to analyze the robustness of popular natural
language models. The infrastructure, datacards and robustness analysis results
are available publicly on the NL-Augmenter repository
(https://github.com/GEM-benchmark/NL-Augmenter).","Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Python-based natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its transformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robustness analysis results are available publicly on the NL-Augmenter repository (https://github.com/GEM-benchmark/NL-Augmenter).",http://arxiv.org/pdf/2112.02721,"['Kaustubh D. Dhole', 'Varun Gangal', 'Sebastian Gehrmann', 'Aadesh Gupta', 'Zhenhao Li', 'Saad Mahamood', 'Abinaya Mahendiran', 'Simon Mille', 'Ashish Shrivastava', 'Samson Tan', 'Tongshuang Wu', 'Jascha Sohl-Dickstein', 'Jinho D. Choi', 'Eduard Hovy', 'Ondrej Dusek', 'Sebastian Ruder', 'Sajant Anand', 'Nagender Aneja', 'Rabin Banjade', 'Lisa Barthe', 'Hanna Behnke', 'Ian Berlot-Attwell', 'Connor Boyle', 'Caroline Brun', 'Marco Antonio Sobrevilla Cabezudo', 'Samuel Cahyawijaya', 'Emile Chapuis', 'Wanxiang Che', 'Mukund Choudhary', 'Christian Clauss', 'Pierre Colombo', 'Filip Cornell', 'Gautier Dagan', 'Mayukh Das', 'Tanay Dixit', 'Thomas Dopierre', 'Paul-Alexis Dray', 'Suchitra Dubey', 'Tatiana Ekeinhor', 'Marco Di Giovanni', 'Tanya Goyal', 'Rishabh Gupta', 'Rishabh Gupta', 'Louanes Hamla', 'Sang Han', 'Fabrice Harel-Canada', 'Antoine Honore', 'Ishan Jindal', 'Przemyslaw K. Joniak', 'Denis Kleyko', 'Venelin Kovatchev', 'Kalpesh Krishna', 'Ashutosh Kumar', 'Stefan Langer', 'Seungjae Ryan Lee', 'Corey James Levinson', 'Hualou Liang', 'Kaizhao Liang', 'Zhexiong Liu', 'Andrey Lukyanenko', 'Vukosi Marivate', 'Gerard de Melo', 'Simon Meoni', 'Maxime Meyer', 'Afnan Mir', 'Nafise Sadat Moosavi', 'Niklas Muennighoff', 'Timothy Sum Hon Mun', 'Kenton Murray', 'Marcin Namysl', 'Maria Obedkova', 'Priti Oli', 'Nivranshu Pasricha', 'Jan Pfister', 'Richard Plant', 'Vinay Prabhu', 'Vasile Pais', 'Libo Qin', 'Shahab Raji', 'Pawan Kumar Rajpoot', 'Vikas Raunak', 'Roy Rinberg', 'Nicolas Roberts', 'Juan Diego Rodriguez', 'Claude Roux', 'Vasconcellos P. H. S.', 'Ananya B. Sai', 'Robin M. Schmidt', 'Thomas Scialom', 'Tshephisho Sefara', 'Saqib N. Shamsi', 'Xudong Shen', 'Haoyue Shi', 'Yiwen Shi', 'Anna Shvets', 'Nick Siegel', 'Damien Sileo', 'Jamie Simon', 'Chandan Singh', 'Roman Sitelew', 'Priyank Soni', 'Taylor Sorensen', 'William Soto', 'Aman Srivastava', 'KV Aditya Srivatsa', 'Tony Sun', 'Mukund Varma T', 'A Tabassum', 'Fiona Anting Tan', 'Ryan Teehan', 'Mo Tiwari', 'Marie Tolkiehn', 'Athena Wang', 'Zijian Wang', 'Gloria Wang', 'Zijie J. Wang', 'Fuxuan Wei', 'Bryan Wilie', 'Genta Indra Winata', 'Xinyi Wu', 'Witold Wydmański', 'Tianbao Xie', 'Usama Yaseen', 'Michael A. Yee', 'Jing Zhang', 'Yue Zhang']",128,,,
"SGPT: GPT Sentence Embeddings for Semantic
Search
Niklas Muennighoff
Peking University
muennighoff@stu.pku.edu.cn
Abstract
Decod",2202.08904,SGPT: GPT Sentence Embeddings for Semantic Search,"Decoder transformers have continued increasing in scale reaching hundreds of
billions of parameters. Due to their scale the same decoder sets
state-of-the-art results on various language tasks via prompting or
fine-tuning. Yet, these large foundation models remain unusable for the related
fields of semantic search and sentence embeddings. This prevents possibly new
state-of-the-art results and forces organizations to train and maintain
separate models. To this end, we propose SGPT to use decoders for sentence
embeddings and semantic search via prompting or fine-tuning. At 5.8 billion
parameters SGPT improves on the previously best sentence embeddings by a margin
of 7% and outperforms a concurrent method with 175 billion parameters as
measured on the BEIR search benchmark. Code, models and result files are freely
available at https://github.com/Muennighoff/sgpt.","Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.",http://arxiv.org/pdf/2202.08904,['Niklas Muennighoff'],128,,,
"Societal Biases in Language Generation: Progress and Challenges
Emily Sheng1, Kai-Wei Chang2, Premkumar Natarajan1, Nanyun Peng1",2105.04054,Societal Biases in Language Generation: Progress and Challenges,"Technology for language generation has advanced rapidly, spurred by
advancements in pre-training large models on massive amounts of data and the
need for intelligent agents to communicate in a natural manner. While
techniques can effectively generate fluent text, they can also produce
undesirable societal biases that can have a disproportionately negative impact
on marginalized populations. Language generation presents unique challenges for
biases in terms of direct user interaction and the structure of decoding
techniques. To better understand these challenges, we present a survey on
societal biases in language generation, focusing on how data and techniques
contribute to biases and progress towards reducing biases. Motivated by a lack
of studies on biases from decoding techniques, we also conduct experiments to
quantify the effects of these techniques. By further discussing general trends
and open challenges, we call to attention promising directions for research and
the importance of fairness and inclusivity considerations for language
generation applications.","Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",http://arxiv.org/pdf/2105.04054,"['Emily Sheng', 'Kai-Wei Chang', 'Premkumar Natarajan', 'Nanyun Peng']",128,,,
"Measuring and Narrowing the Compositionality Gap in Language Models
Ofir Press1,2, Muru Zhang1, Sewon Min1,3, Ludwig Schmidt1,4,",2210.0335,Measuring and Narrowing the Compositionality Gap in Language Models,"We investigate the ability of language models to perform compositional
reasoning tasks where the overall solution depends on correctly composing the
answers to sub-problems. We measure how often models can correctly answer all
sub-problems but not generate the overall solution, a ratio we call the
compositionality gap. We evaluate this ratio by asking multi-hop questions with
answers that require composing multiple facts unlikely to have been observed
together during pretraining. In the GPT-3 family of models, as model size
increases we show that the single-hop question answering performance improves
faster than the multi-hop performance does, therefore the compositionality gap
does not decrease. This surprising result suggests that while more powerful
models memorize and recall more factual knowledge, they show no corresponding
improvement in their ability to perform this kind of compositional reasoning.
  We then demonstrate how elicitive prompting (such as chain of thought)
narrows the compositionality gap by reasoning explicitly instead of implicitly.
We present a new method, self-ask, that further improves on chain of thought.
In our method, the model explicitly asks itself (and then answers) follow-up
questions before answering the initial question. We finally show that
self-ask's structured prompting lets us easily plug in a search engine to
answer the follow-up questions, which additionally improves accuracy.","We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.   We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",http://arxiv.org/pdf/2210.03350,"['Ofir Press', 'Muru Zhang', 'Sewon Min', 'Ludwig Schmidt', 'Noah A. Smith', 'Mike Lewis']",128,,,
"Chasing Carbon: The Elusive Environmental
Footprint of Computing
Udit Gupta1;2, Young Geun Kim3, Sylvia Lee2, Jordan Tse2,
Hsien",2011.02839,Chasing Carbon: The Elusive Environmental Footprint of Computing,"Given recent algorithm, software, and hardware innovation, computing has
enabled a plethora of new applications. As computing becomes increasingly
ubiquitous, however, so does its environmental impact. This paper brings the
issue to the attention of computer-systems researchers. Our analysis, built on
industry-reported characterization, quantifies the environmental effects of
computing in terms of carbon emissions. Broadly, carbon emissions have two
sources: operational energy consumption, and hardware manufacturing and
infrastructure. Although carbon emissions from the former are decreasing thanks
to algorithmic, software, and hardware innovations that boost performance and
power efficiency, the overall carbon footprint of computer systems continues to
grow. This work quantifies the carbon output of computer systems to show that
most emissions related to modern mobile and data-center equipment come from
hardware manufacturing and infrastructure. We therefore outline future
directions for minimizing the environmental impact of computing systems.","Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.",http://arxiv.org/pdf/2011.02839,"['Udit Gupta', 'Young Geun Kim', 'Sylvia Lee', 'Jordan Tse', 'Hsien-Hsin S. Lee', 'Gu-Yeon Wei', 'David Brooks', 'Carole-Jean Wu']",128,,,
"TASK2VEC: Task Embedding for Meta-Learning
Alessandro Achille
UCLA and AWS
achille@cs.ucla.eduMichael Lam
AWS
michlam@amazon.com",1902.03545,Task2Vec: Task Embedding for Meta-Learning,"We introduce a method to provide vectorial representations of visual
classification tasks which can be used to reason about the nature of those
tasks and their relations. Given a dataset with ground-truth labels and a loss
function defined over those labels, we process images through a ""probe network""
and compute an embedding based on estimates of the Fisher information matrix
associated with the probe network parameters. This provides a fixed-dimensional
embedding of the task that is independent of details such as the number of
classes and does not require any understanding of the class label semantics. We
demonstrate that this embedding is capable of predicting task similarities that
match our intuition about semantic and taxonomic relations between different
visual tasks (e.g., tasks based on classifying different types of plants are
similar) We also demonstrate the practical value of this framework for the
meta-task of selecting a pre-trained feature extractor for a new task. We
present a simple meta-learning framework for learning a metric on embeddings
that is capable of predicting which feature extractors will perform well.
Selecting a feature extractor with task embedding obtains a performance close
to the best available feature extractor, while costing substantially less than
exhaustively training and evaluating on all available feature extractors.","We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a probe network and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.",http://arxiv.org/pdf/1902.03545,"['Alessandro Achille', 'Michael Lam', 'Rahul Tewari', 'Avinash Ravichandran', 'Subhransu Maji', 'Charless Fowlkes', 'Stefano Soatto', 'Pietro Perona']",128,,,
"Towards a Human-like Open-Domain Chatbot
Daniel Adiwardana Minh-Thang Luong David R. So Jamie Hall
Noah Fiedel Romal Thoppilan Z",2001.09977,Towards a Human-like Open-Domain Chatbot,"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data
mined and filtered from public domain social media conversations. This 2.6B
parameter neural network is simply trained to minimize perplexity of the next
token. We also propose a human evaluation metric called Sensibleness and
Specificity Average (SSA), which captures key elements of a human-like
multi-turn conversation. Our experiments show strong correlation between
perplexity and SSA. The fact that the best perplexity end-to-end trained Meena
scores high on SSA (72% on multi-turn evaluation) suggests that a human-level
SSA of 86% is potentially within reach if we can better optimize perplexity.
Additionally, the full version of Meena (with a filtering mechanism and tuned
decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots
we evaluated.","We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",http://arxiv.org/pdf/2001.09977,"['Daniel Adiwardana', 'Minh-Thang Luong', 'David R. So', 'Jamie Hall', 'Noah Fiedel', 'Romal Thoppilan', 'Zi Yang', 'Apoorv Kulshreshtha', 'Gaurav Nemade', 'Yifeng Lu', 'Quoc V. Le']",128,,,
"Constitutional AI: Harmlessness from AI Feedback
Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion,
A",2212.08073,Constitutional AI: Harmlessness from AI Feedback,"As AI systems become more capable, we would like to enlist their help to
supervise other AIs. We experiment with methods for training a harmless AI
assistant through self-improvement, without any human labels identifying
harmful outputs. The only human oversight is provided through a list of rules
or principles, and so we refer to the method as 'Constitutional AI'. The
process involves both a supervised learning and a reinforcement learning phase.
In the supervised phase we sample from an initial model, then generate
self-critiques and revisions, and then finetune the original model on revised
responses. In the RL phase, we sample from the finetuned model, use a model to
evaluate which of the two samples is better, and then train a preference model
from this dataset of AI preferences. We then train with RL using the preference
model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a
result we are able to train a harmless but non-evasive AI assistant that
engages with harmful queries by explaining its objections to them. Both the SL
and RL methods can leverage chain-of-thought style reasoning to improve the
human-judged performance and transparency of AI decision making. These methods
make it possible to control AI behavior more precisely and with far fewer human
labels.","As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",http://arxiv.org/pdf/2212.08073,"['Yuntao Bai', 'Saurav Kadavath', 'Sandipan Kundu', 'Amanda Askell', 'Jackson Kernion', 'Andy Jones', 'Anna Chen', 'Anna Goldie', 'Azalia Mirhoseini', 'Cameron McKinnon', 'Carol Chen', 'Catherine Olsson', 'Christopher Olah', 'Danny Hernandez', 'Dawn Drain', 'Deep Ganguli', 'Dustin Li', 'Eli Tran-Johnson', 'Ethan Perez', 'Jamie Kerr', 'Jared Mueller', 'Jeffrey Ladish', 'Joshua Landau', 'Kamal Ndousse', 'Kamile Lukosuite', 'Liane Lovitt', 'Michael Sellitto', 'Nelson Elhage', 'Nicholas Schiefer', 'Noemi Mercado', 'Nova DasSarma', 'Robert Lasenby', 'Robin Larson', 'Sam Ringer', 'Scott Johnston', 'Shauna Kravec', 'Sheer El Showk', 'Stanislav Fort', 'Tamera Lanham', 'Timothy Telleen-Lawton', 'Tom Conerly', 'Tom Henighan', 'Tristan Hume', 'Samuel R. Bowman', 'Zac Hatfield-Dodds', 'Ben Mann', 'Dario Amodei', 'Nicholas Joseph', 'Sam McCandlish', 'Tom Brown', 'Jared Kaplan']",128,,,
"ROFORMER : ENHANCED TRANSFORMER WITH ROTARY
POSITION EMBEDDING
Jianlin Su
Zhuiyi Technology Co., Ltd.
Shenzhen
bojonesu@wezhuiyi",2104.09864,RoFormer: Enhanced Transformer with Rotary Position Embedding,"Position encoding recently has shown effective in the transformer
architecture. It enables valuable supervision for dependency modeling between
elements at different positions of the sequence. In this paper, we first
investigate various methods to integrate positional information into the
learning process of transformer-based language models. Then, we propose a novel
method named Rotary Position Embedding(RoPE) to effectively leverage the
positional information. Specifically, the proposed RoPE encodes the absolute
position with a rotation matrix and meanwhile incorporates the explicit
relative position dependency in self-attention formulation. Notably, RoPE
enables valuable properties, including the flexibility of sequence length,
decaying inter-token dependency with increasing relative distances, and the
capability of equipping the linear self-attention with relative position
encoding. Finally, we evaluate the enhanced transformer with rotary position
embedding, also called RoFormer, on various long text classification benchmark
datasets. Our experiments show that it consistently overcomes its alternatives.
Furthermore, we provide a theoretical analysis to explain some experimental
results. RoFormer is already integrated into Huggingface:
\url{https://huggingface.co/docs/transformers/model_doc/roformer}.","Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \url{https://huggingface.co/docs/transformers/model_doc/roformer}.",http://arxiv.org/pdf/2104.09864,"['Jianlin Su', 'Yu Lu', 'Shengfeng Pan', 'Ahmed Murtadha', 'Bo Wen', 'Yunfeng Liu']",128,,,
"Calibrate Before Use:
Improving Few-Shot Performance of Language Models
Tony Z. Zhao* 1Eric Wallace* 1Shi Feng2Dan Klein1Sameer ",2102.0969,Calibrate Before Use: Improving Few-Shot Performance of Language Models,"GPT-3 can perform numerous tasks when provided a natural language prompt that
contains a few training examples. We show that this type of few-shot learning
can be unstable: the choice of prompt format, training examples, and even the
order of the training examples can cause accuracy to vary from near chance to
near state-of-the-art. We demonstrate that this instability arises from the
bias of language models towards predicting certain answers, e.g., those that
are placed near the end of the prompt or are common in the pre-training data.
To mitigate this, we first estimate the model's bias towards each answer by
asking for its prediction when given the training prompt and a content-free
test input such as ""N/A"". We then fit calibration parameters that cause the
prediction for this input to be uniform across answers. On a diverse set of
tasks, this contextual calibration procedure substantially improves GPT-3 and
GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across
different choices of the prompt.","GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as N/A. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",http://arxiv.org/pdf/2102.09690,"['Tony Z. Zhao', 'Eric Wallace', 'Shi Feng', 'Dan Klein', 'Sameer Singh']",128,,,
"arXiv:1906.01604v1  [cs.CL]  4 Jun 2019KERMIT:
Generative Insertion-Based Modeling for Sequences
William Chan∗1, Nikita Kitaev∗1",1906.01604,KERMIT: Generative Insertion-Based Modeling for Sequences,"We present KERMIT, a simple insertion-based approach to generative modeling
for sequences and sequence pairs. KERMIT models the joint distribution and its
decompositions (i.e., marginals and conditionals) using a single neural network
and, unlike much prior work, does not rely on a prespecified factorization of
the data distribution. During training, one can feed KERMIT paired data $(x,
y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired
data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we
have access to the conditionals $p(x \mid y)$ and $p(y \mid x)$ in both
directions. We can also sample from the joint distribution or the marginals.
The model supports both serial fully autoregressive decoding and parallel
partially autoregressive decoding, with the latter exhibiting an empirically
logarithmic runtime. We demonstrate through experiments in machine translation,
representation learning, and zero-shot cloze question answering that our
unified approach is capable of matching or exceeding the performance of
dedicated state-of-the-art systems across a wide range of tasks without the
need for problem-specific architectural adaptation.","We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data $(x, y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we have access to the conditionals $p(x \mid y)$ and $p(y \mid x)$ in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation.",http://arxiv.org/pdf/1906.01604,"['William Chan', 'Nikita Kitaev', 'Kelvin Guu', 'Mitchell Stern', 'Jakob Uszkoreit']",128,,,
"Published as a conference paper at ICLR 2021
DEBERT A: D ECODING -ENHANCED BERT WITH DISENTANGLED ATTENTION
Pengcheng He1, Xiaod",2006.03654,DeBERTa: Decoding-enhanced BERT with Disentangled Attention,"Recent progress in pre-trained neural language models has significantly
improved the performance of many natural language processing (NLP) tasks. In
this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT
with disentangled attention) that improves the BERT and RoBERTa models using
two novel techniques. The first is the disentangled attention mechanism, where
each word is represented using two vectors that encode its content and
position, respectively, and the attention weights among words are computed
using disentangled matrices on their contents and relative positions,
respectively. Second, an enhanced mask decoder is used to incorporate absolute
positions in the decoding layer to predict the masked tokens in model
pre-training. In addition, a new virtual adversarial training method is used
for fine-tuning to improve models' generalization. We show that these
techniques significantly improve the efficiency of model pre-training and the
performance of both natural language understanding (NLU) and natural langauge
generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model
trained on half of the training data performs consistently better on a wide
range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%),
on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%).
Notably, we scale up DeBERTa by training a larger version that consists of 48
Transform layers with 1.5 billion parameters. The significant performance boost
makes the single DeBERTa model surpass the human performance on the SuperGLUE
benchmark (Wang et al., 2019a) for the first time in terms of macro-average
score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the
SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline
by a decent margin (90.3 versus 89.8).","Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",http://arxiv.org/pdf/2006.03654,"['Pengcheng He', 'Xiaodong Liu', 'Jianfeng Gao', 'Weizhu Chen']",128,,,
"Evaluating the Factual Consistency of Abstractive Text Summarization
Wojciech Kry ´sci´nski, Bryan McCann, Caiming Xiong, Richar",1910.1284,Evaluating the Factual Consistency of Abstractive Text Summarization,"Currently used metrics for assessing summarization algorithms do not account
for whether summaries are factually consistent with source documents. We
propose a weakly-supervised, model-based approach for verifying factual
consistency and identifying conflicts between source documents and a generated
summary. Training data is generated by applying a series of rule-based
transformations to the sentences of source documents. The factual consistency
model is then trained jointly for three tasks: 1) identify whether sentences
remain factually consistent after transformation, 2) extract a span in the
source documents to support the consistency prediction, 3) extract a span in
the summary sentence that is inconsistent if one exists. Transferring this
model to summaries generated by several state-of-the art models reveals that
this highly scalable approach substantially outperforms previous models,
including those trained with strong supervision using standard datasets for
natural language inference and fact checking. Additionally, human evaluation
shows that the auxiliary span extraction tasks provide useful assistance in the
process of verifying factual consistency.","Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",http://arxiv.org/pdf/1910.12840,"['Wojciech Kryściński', 'Bryan McCann', 'Caiming Xiong', 'Richard Socher']",128,,,
"Dynamic Planning in Open-Ended Dialogue using
Reinforcement Learning
Deborah Cohen, Moonkyung Ryu, Yinlam Chow, Orgad Keller, Id",2208.02294,Dynamic Planning in Open-Ended Dialogue using Reinforcement Learning,"Despite recent advances in natural language understanding and generation, and
decades of research on the development of conversational bots, building
automated agents that can carry on rich open-ended conversations with humans
""in the wild"" remains a formidable challenge. In this work we develop a
real-time, open-ended dialogue system that uses reinforcement learning (RL) to
power a bot's conversational skill at scale. Our work pairs the succinct
embedding of the conversation state generated using SOTA (supervised) language
models with RL techniques that are particularly suited to a dynamic action
space that changes as the conversation progresses. Trained using crowd-sourced
data, our novel system is able to substantially exceeds the (strong) baseline
supervised model with respect to several metrics of interest in a live
experiment with real users of the Google Assistant.","Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans in the wild remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.",http://arxiv.org/pdf/2208.02294,"['Deborah Cohen', 'Moonkyung Ryu', 'Yinlam Chow', 'Orgad Keller', 'Ido Greenberg', 'Avinatan Hassidim', 'Michael Fink', 'Yossi Matias', 'Idan Szpektor', 'Craig Boutilier', 'Gal Elidan']",128,,,
"An Explanation of In-context Learning as Implicit
Bayesian Inference
Sang Michael Xie
Stanford University
xie@cs.stanford.eduAdi",2111.0208,An Explanation of In-context Learning as Implicit Bayesian Inference,"Large language models (LMs) such as GPT-3 have the surprising ability to do
in-context learning, where the model learns to do a downstream task simply by
conditioning on a prompt consisting of input-output examples. The LM learns
from these examples without being explicitly pretrained to learn. Thus, it is
unclear what enables in-context learning. In this paper, we study how
in-context learning can emerge when pretraining documents have long-range
coherence. Here, the LM must infer a latent document-level concept to generate
coherent next tokens during pretraining. At test time, in-context learning
occurs when the LM also infers a shared latent concept between examples in a
prompt. We prove when this occurs despite a distribution mismatch between
prompts and pretraining data in a setting where the pretraining distribution is
a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs
capable of in-context learning, we generate a small-scale synthetic dataset
(GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond
the theory, experiments on GINC exhibit large-scale real-world phenomena
including improved in-context performance with model scaling (despite the same
pretraining loss), sensitivity to example order, and instances where zero-shot
is better than few-shot in-context learning.","Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",http://arxiv.org/pdf/2111.02080,"['Sang Michael Xie', 'Aditi Raghunathan', 'Percy Liang', 'Tengyu Ma']",128,,,
"Journal of Machine Learning Research 23 (2022) 1-40 Submitted 8/21; Revised 3/22; Published 4/22
Switch Transformers: Scaling to",2101.03961,Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity,"In deep learning, models typically reuse the same parameters for all inputs.
Mixture of Experts (MoE) defies this and instead selects different parameters
for each incoming example. The result is a sparsely-activated model -- with
outrageous numbers of parameters -- but a constant computational cost. However,
despite several notable successes of MoE, widespread adoption has been hindered
by complexity, communication costs and training instability -- we address these
with the Switch Transformer. We simplify the MoE routing algorithm and design
intuitive improved models with reduced communication and computational costs.
Our proposed training techniques help wrangle the instabilities and we show
large sparse models may be trained, for the first time, with lower precision
(bfloat16) formats. We design models based off T5-Base and T5-Large to obtain
up to 7x increases in pre-training speed with the same computational resources.
These improvements extend into multilingual settings where we measure gains
over the mT5-Base version across all 101 languages. Finally, we advance the
current scale of language models by pre-training up to trillion parameter
models on the ""Colossal Clean Crawled Corpus"" and achieve a 4x speedup over the
T5-XXL model.","In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the Colossal Clean Crawled Corpus and achieve a 4x speedup over the T5-XXL model.",http://arxiv.org/pdf/2101.03961,"['William Fedus', 'Barret Zoph', 'Noam Shazeer']",128,,,
"Published in Transactions on Machine Learning Research (08/2023)
Holistic Evaluation of Language Models
Percy Liang†, Rishi Bomm",2211.0911,Holistic Evaluation of Language Models,"Language models (LMs) are becoming the foundation for almost all major
language technologies, but their capabilities, limitations, and risks are not
well understood. We present Holistic Evaluation of Language Models (HELM) to
improve the transparency of language models. First, we taxonomize the vast
space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata)
that are of interest for LMs. Then we select a broad subset based on coverage
and feasibility, noting what's missing or underrepresented (e.g. question
answering for neglected English dialects, metrics for trustworthiness). Second,
we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration,
robustness, fairness, bias, toxicity, and efficiency) for each of 16 core
scenarios when possible (87.5% of the time). This ensures metrics beyond
accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We
also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze
specific aspects (e.g. reasoning, disinformation). Third, we conduct a
large-scale evaluation of 30 prominent language models (spanning open,
limited-access, and closed models) on all 42 scenarios, 21 of which were not
previously used in mainstream LM evaluation. Prior to HELM, models on average
were evaluated on just 17.9% of the core HELM scenarios, with some prominent
models not sharing a single scenario in common. We improve this to 96.0%: now
all 30 models have been densely benchmarked on the same core scenarios and
metrics under standardized conditions. Our evaluation surfaces 25 top-level
findings. For full transparency, we release all raw model prompts and
completions publicly for further analysis, as well as a general modular
toolkit. We intend for HELM to be a living benchmark for the community,
continuously updated with new scenarios, metrics, and models.","Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",http://arxiv.org/pdf/2211.09110,"['Percy Liang', 'Rishi Bommasani', 'Tony Lee', 'Dimitris Tsipras', 'Dilara Soylu', 'Michihiro Yasunaga', 'Yian Zhang', 'Deepak Narayanan', 'Yuhuai Wu', 'Ananya Kumar', 'Benjamin Newman', 'Binhang Yuan', 'Bobby Yan', 'Ce Zhang', 'Christian Cosgrove', 'Christopher D. Manning', 'Christopher Ré', 'Diana Acosta-Navas', 'Drew A. Hudson', 'Eric Zelikman', 'Esin Durmus', 'Faisal Ladhak', 'Frieda Rong', 'Hongyu Ren', 'Huaxiu Yao', 'Jue Wang', 'Keshav Santhanam', 'Laurel Orr', 'Lucia Zheng', 'Mert Yuksekgonul', 'Mirac Suzgun', 'Nathan Kim', 'Neel Guha', 'Niladri Chatterji', 'Omar Khattab', 'Peter Henderson', 'Qian Huang', 'Ryan Chi', 'Sang Michael Xie', 'Shibani Santurkar', 'Surya Ganguli', 'Tatsunori Hashimoto', 'Thomas Icard', 'Tianyi Zhang', 'Vishrav Chaudhary', 'William Wang', 'Xuechen Li', 'Yifan Mai', 'Yuhui Zhang', 'Yuta Koreeda']",128,,,
"2021-12-08
Scaling Language Models: Methods, Analysis
& Insights from Training Gopher
Jack W. Rae, Sebastian Borgeaud, Trevor Ca",2112.11446,"Scaling Language Models: Methods, Analysis & Insights from Training Gopher","Language modelling provides a step towards intelligent communication systems
by harnessing large repositories of written human knowledge to better predict
and understand the world. In this paper, we present an analysis of
Transformer-based language model performance across a wide range of model
scales -- from models with tens of millions of parameters up to a 280 billion
parameter model called Gopher. These models are evaluated on 152 diverse tasks,
achieving state-of-the-art performance across the majority. Gains from scale
are largest in areas such as reading comprehension, fact-checking, and the
identification of toxic language, but logical and mathematical reasoning see
less benefit. We provide a holistic analysis of the training dataset and
model's behaviour, covering the intersection of model scale with bias and
toxicity. Finally we discuss the application of language models to AI safety
and the mitigation of downstream harms.","Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",http://arxiv.org/pdf/2112.11446,"['Jack W. Rae', 'Sebastian Borgeaud', 'Trevor Cai', 'Katie Millican', 'Jordan Hoffmann', 'Francis Song', 'John Aslanides', 'Sarah Henderson', 'Roman Ring', 'Susannah Young', 'Eliza Rutherford', 'Tom Hennigan', 'Jacob Menick', 'Albin Cassirer', 'Richard Powell', 'George van den Driessche', 'Lisa Anne Hendricks', 'Maribeth Rauh', 'Po-Sen Huang', 'Amelia Glaese', 'Johannes Welbl', 'Sumanth Dathathri', 'Saffron Huang', 'Jonathan Uesato', 'John Mellor', 'Irina Higgins', 'Antonia Creswell', 'Nat McAleese', 'Amy Wu', 'Erich Elsen', 'Siddhant Jayakumar', 'Elena Buchatskaya', 'David Budden', 'Esme Sutherland', 'Karen Simonyan', 'Michela Paganini', 'Laurent Sifre', 'Lena Martens', 'Xiang Lorraine Li', 'Adhiguna Kuncoro', 'Aida Nematzadeh', 'Elena Gribovskaya', 'Domenic Donato', 'Angeliki Lazaridou', 'Arthur Mensch', 'Jean-Baptiste Lespiau', 'Maria Tsimpoukelli', 'Nikolai Grigorev', 'Doug Fritz', 'Thibault Sottiaux', 'Mantas Pajarskas', 'Toby Pohlen', 'Zhitao Gong', 'Daniel Toyama', ""Cyprien de Masson d'Autume"", 'Yujia Li', 'Tayfun Terzi', 'Vladimir Mikulik', 'Igor Babuschkin', 'Aidan Clark', 'Diego de Las Casas', 'Aurelia Guy', 'Chris Jones', 'James Bradbury', 'Matthew Johnson', 'Blake Hechtman', 'Laura Weidinger', 'Iason Gabriel', 'William Isaac', 'Ed Lockhart', 'Simon Osindero', 'Laura Rimell', 'Chris Dyer', 'Oriol Vinyals', 'Kareem Ayoub', 'Jeff Stanway', 'Lorrayne Bennett', 'Demis Hassabis', 'Koray Kavukcuoglu', 'Geoffrey Irving']",128,,,
"CHARACTERISING BIAS IN COMPRESSED MODELS
Sara Hooker
Google Research
shooker@google.comNyalleng Moorosi*
Google Research
nyalle",2010.03058,Characterising Bias in Compressed Models,"The popularity and widespread use of pruning and quantization is driven by
the severe resource constraints of deploying deep neural networks to
environments with strict latency, memory and energy requirements. These
techniques achieve high levels of compression with negligible impact on
top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides
disproportionately high errors on a small subset of examples; we call this
subset Compression Identified Exemplars (CIE). We further establish that for
CIE examples, compression amplifies existing algorithmic bias. Pruning
disproportionately impacts performance on underrepresented features, which
often coincides with considerations of fairness. Given that CIE is a relatively
small subset but a great contributor of error in the model, we propose its use
as a human-in-the-loop auditing tool to surface a tractable subset of the
dataset for further inspection or annotation by a domain expert. We provide
qualitative and quantitative support that CIE surfaces the most challenging
examples in the data distribution for human-in-the-loop auditing.","The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.",http://arxiv.org/pdf/2010.03058,"['Sara Hooker', 'Nyalleng Moorosi', 'Gregory Clark', 'Samy Bengio', 'Emily Denton']",128,,,
"Open-Ended Learning Leads to Generally
Capable Agents
Open-Ended Learning Team *, Adam Stooke, Anuj Mahajan, Catarina Barros, Ch",2107.12808,Open-Ended Learning Leads to Generally Capable Agents,"In this work we create agents that can perform well beyond a single,
individual task, that exhibit much wider generalisation of behaviour to a
massive, rich space of challenges. We define a universe of tasks within an
environment domain and demonstrate the ability to train agents that are
generally capable across this vast space and beyond. The environment is
natively multi-agent, spanning the continuum of competitive, cooperative, and
independent games, which are situated within procedurally generated physical 3D
worlds. The resulting space is exceptionally diverse in terms of the challenges
posed to agents, and as such, even measuring the learning progress of an agent
is an open research problem. We propose an iterative notion of improvement
between successive generations of agents, rather than seeking to maximise a
singular objective, allowing us to quantify progress despite tasks being
incomparable in terms of achievable rewards. We show that through constructing
an open-ended learning process, which dynamically changes the training task
distributions and training objectives such that the agent never stops learning,
we achieve consistent learning of new behaviours. The resulting agent is able
to score reward in every one of our humanly solvable evaluation levels, with
behaviour generalising to many held-out points in the universe of tasks.
Examples of this zero-shot generalisation include good performance on Hide and
Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks
we characterise the behaviour of our agent, and find interesting emergent
heuristic behaviours such as trial-and-error experimentation, simple tool use,
option switching, and cooperation. Finally, we demonstrate that the general
capabilities of this agent could unlock larger scale transfer of behaviour
through cheap finetuning.","In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",http://arxiv.org/pdf/2107.12808,"['Open Ended Learning Team', 'Adam Stooke', 'Anuj Mahajan', 'Catarina Barros', 'Charlie Deck', 'Jakob Bauer', 'Jakub Sygnowski', 'Maja Trebacz', 'Max Jaderberg', 'Michael Mathieu', 'Nat McAleese', 'Nathalie Bradley-Schmieg', 'Nathaniel Wong', 'Nicolas Porcel', 'Roberta Raileanu', 'Steph Hughes-Fitt', 'Valentin Dalibard', 'Wojciech Marian Czarnecki']",128,,,
"Published as a conference paper at ICLR 2023
CODEGEN: ANOPEN LARGE LANGUAGE MODEL FOR
CODE WITH MULTI -TURN PROGRAM SYNTHESIS
Er",2203.13474,CodeGen: An Open Large Language Model for Code with Multi-Turn Program Synthesis,"Program synthesis strives to generate a computer program as a solution to a
given problem specification, expressed with input-output examples or natural
language descriptions. The prevalence of large language models advances the
state-of-the-art for program synthesis, though limited training resources and
data impede open access to such models. To democratize this, we train and
release a family of large language models up to 16.1B parameters, called
CODEGEN, on natural language and programming language data, and open source the
training library JAXFORMER. We show the utility of the trained model by
demonstrating that it is competitive with the previous state-of-the-art on
zero-shot Python code generation on HumanEval. We further investigate the
multi-step paradigm for program synthesis, where a single program is factorized
into multiple prompts specifying subproblems. To this end, we construct an open
benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse
problem sets that are factorized into multi-turn prompts. Our analysis on MTPB
shows that the same intent provided to CODEGEN in multi-turn fashion
significantly improves program synthesis over that provided as a single turn.
We make the training library JAXFORMER and model checkpoints available as open
source contribution: https://github.com/salesforce/CodeGen.","Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",http://arxiv.org/pdf/2203.13474,"['Erik Nijkamp', 'Bo Pang', 'Hiroaki Hayashi', 'Lifu Tu', 'Huan Wang', 'Yingbo Zhou', 'Silvio Savarese', 'Caiming Xiong']",128,,,
"A Watermark for Large Language Models
John Kirchenbauer*Jonas Geiping*Yuxin Wen Jonathan Katz Ian Miers Tom Goldstein
University",2301.10226,A Watermark for Large Language Models,"Potential harms of large language models can be mitigated by watermarking
model output, i.e., embedding signals into generated text that are invisible to
humans but algorithmically detectable from a short span of tokens. We propose a
watermarking framework for proprietary language models. The watermark can be
embedded with negligible impact on text quality, and can be detected using an
efficient open-source algorithm without access to the language model API or
parameters. The watermark works by selecting a randomized set of ""green"" tokens
before a word is generated, and then softly promoting use of green tokens
during sampling. We propose a statistical test for detecting the watermark with
interpretable p-values, and derive an information-theoretic framework for
analyzing the sensitivity of the watermark. We test the watermark using a
multi-billion parameter model from the Open Pretrained Transformer (OPT)
family, and discuss robustness and security.","Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of green tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",http://arxiv.org/pdf/2301.10226,"['John Kirchenbauer', 'Jonas Geiping', 'Yuxin Wen', 'Jonathan Katz', 'Ian Miers', 'Tom Goldstein']",128,,,
"Learning to Model Editing Processes
Machel Reid
University of Tokyo
machlereid@weblab.t.u-tokyo.ac.jpGraham Neubig
Carnegie Mell",2205.12374,Learning to Model Editing Processes,"Most existing sequence generation models produce outputs in one pass, usually
left-to-right. However, this is in contrast with a more natural approach that
humans use in generating content; iterative refinement and editing. Recent work
has introduced edit-based models for various tasks (such as neural machine
translation and text style transfer), but these generally model a single edit
step. In this work, we propose modeling editing processes, modeling the whole
process of iteratively generating sequences. We form a conceptual framework to
describe the likelihood of multi-step edits, and describe neural models that
can learn a generative model of sequences based on these multistep edits. We
introduce baseline results and metrics on this task, finding that modeling
editing processes improves performance on a variety of axes on both our
proposed task and related downstream tasks compared to previous single-step
models of edits.","Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.",http://arxiv.org/pdf/2205.12374,"['Machel Reid', 'Graham Neubig']",128,,,
"Under review as a conference paper at ICLR 2017
FASTTEXT.ZIP:
COMPRESSING TEXT CLASSIFICATION MODELS
Armand Joulin, Edouard Grav",1612.03651,FastText.zip: Compressing text classification models,"We consider the problem of producing compact architectures for text
classification, such that the full model fits in a limited amount of memory.
After considering different solutions inspired by the hashing literature, we
propose a method built upon product quantization to store word embeddings.
While the original technique leads to a loss in accuracy, we adapt this method
to circumvent quantization artefacts. Our experiments carried out on several
benchmarks show that our approach typically requires two orders of magnitude
less memory than fastText while being only slightly inferior with respect to
accuracy. As a result, it outperforms the state of the art by a good margin in
terms of the compromise between memory usage and accuracy.","We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",http://arxiv.org/pdf/1612.03651,"['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Matthijs Douze', 'Hérve Jégou', 'Tomas Mikolov']",128,,,
"Program Induction by Rationale Generation:
Learning to Solve and Explain Algebraic Word Problems
Wang LingDani YogatamaChris D",1705.04146,Program Induction by Rationale Generation : Learning to Solve and Explain Algebraic Word Problems,"Solving algebraic word problems requires executing a series of arithmetic
operations---a program---to obtain a final answer. However, since programs can
be arbitrarily complicated, inducing them directly from question-answer pairs
is a formidable challenge. To make this task more feasible, we solve these
problems by generating answer rationales, sequences of natural language and
human-readable mathematical expressions that derive the final answer through a
series of small steps. Although rationales do not explicitly specify programs,
they provide a scaffolding for their structure via intermediate milestones. To
evaluate our approach, we have created a new 100,000-sample dataset of
questions, answers and rationales. Experimental results show that indirect
supervision of program learning via answer rationales is a promising strategy
for inducing arithmetic programs.","Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",http://arxiv.org/pdf/1705.04146,"['Wang Ling', 'Dani Yogatama', 'Chris Dyer', 'Phil Blunsom']",128,,,
"Intracranial Error Detection via Deep Learning
Martin V ¨olker
Graduate School of Robotics
Albert-Ludwigs-University
Freiburg, G",1805.01667,Intracranial Error Detection via Deep Learning,"Deep learning techniques have revolutionized the field of machine learning
and were recently successfully applied to various classification problems in
noninvasive electroencephalography (EEG). However, these methods were so far
only rarely evaluated for use in intracranial EEG. We employed convolutional
neural networks (CNNs) to classify and characterize the error-related brain
response as measured in 24 intracranial EEG recordings. Decoding accuracies of
CNNs were significantly higher than those of a regularized linear discriminant
analysis. Using time-resolved deep decoding, it was possible to classify errors
in various regions in the human brain, and further to decode errors over 200 ms
before the actual erroneous button press, e.g., in the precentral gyrus.
Moreover, deeper networks performed better than shallower networks in
distinguishing correct from error trials in all-channel decoding. In single
recordings, up to 100 % decoding accuracy was achieved. Visualization of the
networks' learned features indicated that multivariate decoding on an ensemble
of channels yields related, albeit non-redundant information compared to
single-channel decoding. In summary, here we show the usefulness of deep
learning for both intracranial error decoding and mapping of the
spatio-temporal structure of the human error processing network.","Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks' learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.",http://arxiv.org/pdf/1805.01667,"['Martin Völker', 'Jiří Hammer', 'Robin T. Schirrmeister', 'Joos Behncke', 'Lukas D. J. Fiederer', 'Andreas Schulze-Bonhage', 'Petr Marusič', 'Wolfram Burgard', 'Tonio Ball']",128,,,
"AUTOMATIC CHAIN OF THOUGHT PROMPTING
INLARGE LANGUAGE MODELS
Zhuosheng Zhangy, Aston Zhangz, Mu Liz, Alex Smolaz
yShanghai Jiao",2210.03493,Automatic Chain of Thought Prompting in Large Language Models,"Large language models (LLMs) can perform complex reasoning by generating
intermediate reasoning steps. Providing these steps for prompting
demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has
two major paradigms. One leverages a simple prompt like ""Let's think step by
step"" to facilitate step-by-step thinking before answering a question. The
other uses a few manual demonstrations one by one, each composed of a question
and a reasoning chain that leads to an answer. The superior performance of the
second paradigm hinges on the hand-crafting of task-specific demonstrations one
by one. We show that such manual efforts may be eliminated by leveraging LLMs
with the ""Let's think step by step"" prompt to generate reasoning chains for
demonstrations one by one, i.e., let's think not just step by step, but also
one by one. However, these generated chains often come with mistakes. To
mitigate the effect of such mistakes, we find that diversity matters for
automatically constructing demonstrations. We propose an automatic CoT
prompting method: Auto-CoT. It samples questions with diversity and generates
reasoning chains to construct demonstrations. On ten public benchmark reasoning
tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of
the CoT paradigm that requires manual designs of demonstrations. Code is
available at https://github.com/amazon-research/auto-cot","Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like Let's think step by step to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the Let's think step by step prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",http://arxiv.org/pdf/2210.03493,"['Zhuosheng Zhang', 'Aston Zhang', 'Mu Li', 'Alex Smola']",128,,,
"UNDERSTANDING HTML WITH LARGE LANGUAGE
MODELS
Izzeddin Gur, Oﬁr Nachum, Yingjie Miao, Mustafa Safdari, Austin Huang
Aakanksha Ch",2210.03945,Understanding HTML with Large Language Models,"Large language models (LLMs) have shown exceptional performance on a variety
of natural language tasks. Yet, their capabilities for HTML understanding --
i.e., parsing the raw HTML of a webpage, with applications to automation of
web-based tasks, crawling, and browser-assisted retrieval -- have not been
fully explored. We contribute HTML understanding models (fine-tuned LLMs) and
an in-depth analysis of their capabilities under three tasks: (i) Semantic
Classification of HTML elements, (ii) Description Generation for HTML inputs,
and (iii) Autonomous Web Navigation of HTML pages. While previous work has
developed dedicated architectures and training procedures for HTML
understanding, we show that LLMs pretrained on standard natural language
corpora transfer remarkably well to HTML understanding tasks. For instance,
fine-tuned LLMs are 12% more accurate at semantic classification compared to
models trained exclusively on the task dataset. Moreover, when fine-tuned on
data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks
using 192x less data compared to the previous best supervised model. Out of the
LLMs we evaluate, we show evidence that T5-based models are ideal due to their
bidirectional encoder-decoder architecture. To promote further research on LLMs
for HTML understanding, we create and open-source a large-scale HTML dataset
distilled and auto-labeled from CommonCrawl.","Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.",http://arxiv.org/pdf/2210.03945,"['Izzeddin Gur', 'Ofir Nachum', 'Yingjie Miao', 'Mustafa Safdari', 'Austin Huang', 'Aakanksha Chowdhery', 'Sharan Narang', 'Noah Fiedel', 'Aleksandra Faust']",128,,,
"STRUCT BERT: I NCORPORATING LANGUAGE STRUCTURES
INTO PRE-TRAINING FOR DEEPLANGUAGE UNDERSTANDING
Wei Wang, Bin Bi, Ming Yan, Che",1908.04577,StructBERT: Incorporating Language Structures into Pre-training for Deep Language Understanding,"Recently, the pre-trained language model, BERT (and its robustly optimized
version RoBERTa), has attracted a lot of attention in natural language
understanding (NLU), and achieved state-of-the-art accuracy in various NLU
tasks, such as sentiment classification, natural language inference, semantic
textual similarity and question answering. Inspired by the linearization
exploration work of Elman [8], we extend BERT to a new model, StructBERT, by
incorporating language structures into pre-training. Specifically, we pre-train
StructBERT with two auxiliary tasks to make the most of the sequential order of
words and sentences, which leverage language structures at the word and
sentence levels, respectively. As a result, the new model is adapted to
different levels of language understanding required by downstream tasks. The
StructBERT with structural pre-training gives surprisingly good empirical
results on a variety of downstream tasks, including pushing the
state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published
models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on
SNLI to 91.7.","Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.",http://arxiv.org/pdf/1908.04577,"['Wei Wang', 'Bin Bi', 'Ming Yan', 'Chen Wu', 'Zuyi Bao', 'Jiangnan Xia', 'Liwei Peng', 'Luo Si']",128,,,
"NEWSQA: A M ACHINE COMPREHENSION DATASET
Adam TrischlerTong WangXingdi YuanJustin Harris
Alessandro Sordoni Philip Bachman Ka",1611.0983,NewsQA: A Machine Comprehension Dataset,"We present NewsQA, a challenging machine comprehension dataset of over
100,000 human-generated question-answer pairs. Crowdworkers supply questions
and answers based on a set of over 10,000 news articles from CNN, with answers
consisting of spans of text from the corresponding articles. We collect this
dataset through a four-stage process designed to solicit exploratory questions
that require reasoning. A thorough analysis confirms that NewsQA demands
abilities beyond simple word matching and recognizing textual entailment. We
measure human performance on the dataset and compare it to several strong
neural models. The performance gap between humans and machines (0.198 in F1)
indicates that significant progress can be made on NewsQA through future
research. The dataset is freely available at
https://datasets.maluuba.com/NewsQA.","We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.",http://arxiv.org/pdf/1611.09830,"['Adam Trischler', 'Tong Wang', 'Xingdi Yuan', 'Justin Harris', 'Alessandro Sordoni', 'Philip Bachman', 'Kaheer Suleman']",128,,,
"UMAP: Uniform Manifold
Approximation and Projection for
Dimension Reduction
Leland McInnes
Tu/t_te Institute for Mathematics and",1802.03426,UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction,"UMAP (Uniform Manifold Approximation and Projection) is a novel manifold
learning technique for dimension reduction. UMAP is constructed from a
theoretical framework based in Riemannian geometry and algebraic topology. The
result is a practical scalable algorithm that applies to real world data. The
UMAP algorithm is competitive with t-SNE for visualization quality, and
arguably preserves more of the global structure with superior run time
performance. Furthermore, UMAP has no computational restrictions on embedding
dimension, making it viable as a general purpose dimension reduction technique
for machine learning.","UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",http://arxiv.org/pdf/1802.03426,"['Leland McInnes', 'John Healy', 'James Melville']",128,,,
"High-Resolution Image Synthesis with Latent Diffusion Models
Robin Rombach1* Andreas Blattmann1Dominik Lorenz1Patrick Esser
 Bj",2112.10752,High-Resolution Image Synthesis with Latent Diffusion Models,"By decomposing the image formation process into a sequential application of
denoising autoencoders, diffusion models (DMs) achieve state-of-the-art
synthesis results on image data and beyond. Additionally, their formulation
allows for a guiding mechanism to control the image generation process without
retraining. However, since these models typically operate directly in pixel
space, optimization of powerful DMs often consumes hundreds of GPU days and
inference is expensive due to sequential evaluations. To enable DM training on
limited computational resources while retaining their quality and flexibility,
we apply them in the latent space of powerful pretrained autoencoders. In
contrast to previous work, training diffusion models on such a representation
allows for the first time to reach a near-optimal point between complexity
reduction and detail preservation, greatly boosting visual fidelity. By
introducing cross-attention layers into the model architecture, we turn
diffusion models into powerful and flexible generators for general conditioning
inputs such as text or bounding boxes and high-resolution synthesis becomes
possible in a convolutional manner. Our latent diffusion models (LDMs) achieve
a new state of the art for image inpainting and highly competitive performance
on various tasks, including unconditional image generation, semantic scene
synthesis, and super-resolution, while significantly reducing computational
requirements compared to pixel-based DMs. Code is available at
https://github.com/CompVis/latent-diffusion .","By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",http://arxiv.org/pdf/2112.10752,"['Robin Rombach', 'Andreas Blattmann', 'Dominik Lorenz', 'Patrick Esser', 'Björn Ommer']",128,,,
"MS MARCO: A Human Generated MAchine
Reading COmprehension Dataset
Payal Bajaj, Daniel Campos, Nick Craswell, Li Deng, Jianfeng G",1611.09268,MS MARCO: A Human Generated MAchine Reading COmprehension Dataset,"We introduce a large scale MAchine Reading COmprehension dataset, which we
name MS MARCO. The dataset comprises of 1,010,916 anonymized
questions---sampled from Bing's search query logs---each with a human generated
answer and 182,669 completely human rewritten generated answers. In addition,
the dataset contains 8,841,823 passages---extracted from 3,563,535 web
documents retrieved by Bing---that provide the information necessary for
curating the natural language answers. A question in the MS MARCO dataset may
have multiple answers or no answers at all. Using this dataset, we propose
three different tasks with varying levels of difficulty: (i) predict if a
question is answerable given a set of context passages, and extract and
synthesize the answer as a human would (ii) generate a well-formed answer (if
possible) based on the context passages that can be understood with the
question and passage context, and finally (iii) rank a set of retrieved
passages given a question. The size of the dataset and the fact that the
questions are derived from real user search queries distinguishes MS MARCO from
other well-known publicly available datasets for machine reading comprehension
and question-answering. We believe that the scale and the real-world nature of
this dataset makes it attractive for benchmarking machine reading comprehension
and question-answering models.","We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.",http://arxiv.org/pdf/1611.09268,"['Payal Bajaj', 'Daniel Campos', 'Nick Craswell', 'Li Deng', 'Jianfeng Gao', 'Xiaodong Liu', 'Rangan Majumder', 'Andrew McNamara', 'Bhaskar Mitra', 'Tri Nguyen', 'Mir Rosenberg', 'Xia Song', 'Alina Stoica', 'Saurabh Tiwary', 'Tong Wang']",128,,,
"FlashAttention : Fast and Memory-Eﬃcient Exact Attention
with IO-Awareness
Tri Daoy, Daniel Y. Fuy, Stefano Ermony, Atri Rudraz,",2205.14135,FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness,"Transformers are slow and memory-hungry on long sequences, since the time and
memory complexity of self-attention are quadratic in sequence length.
Approximate attention methods have attempted to address this problem by trading
off model quality to reduce the compute complexity, but often do not achieve
wall-clock speedup. We argue that a missing principle is making attention
algorithms IO-aware -- accounting for reads and writes between levels of GPU
memory. We propose FlashAttention, an IO-aware exact attention algorithm that
uses tiling to reduce the number of memory reads/writes between GPU high
bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of
FlashAttention, showing that it requires fewer HBM accesses than standard
attention, and is optimal for a range of SRAM sizes. We also extend
FlashAttention to block-sparse attention, yielding an approximate attention
algorithm that is faster than any existing approximate attention method.
FlashAttention trains Transformers faster than existing baselines: 15%
end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the
MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K),
and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention
and block-sparse FlashAttention enable longer context in Transformers, yielding
higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on
long-document classification) and entirely new capabilities: the first
Transformers to achieve better-than-chance performance on the Path-X challenge
(seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1%
accuracy).","Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",http://arxiv.org/pdf/2205.14135,"['Tri Dao', 'Daniel Y. Fu', 'Stefano Ermon', 'Atri Rudra', 'Christopher Ré']",128,,,
"Scaling Laws vs Model Architectures :
How does Inductive Bias Inﬂuence Scaling?
Yi TayMostafa DehghaniSamira Abnar Hyung Won C",2207.10551,Scaling Laws vs Model Architectures: How does Inductive Bias Influence Scaling?,"There have been a lot of interest in the scaling properties of Transformer
models. However, not much has been done on the front of investigating the
effect of scaling properties of different inductive biases and model
architectures. Do model architectures scale differently? If so, how does
inductive bias affect scaling behaviour? How does this influence upstream
(pretraining) and downstream (transfer)? This paper conducts a systematic study
of scaling behaviour of ten diverse model architectures such as Transformers,
Switch Transformers, Universal Transformers, Dynamic convolutions, Performers,
and recently proposed MLP-Mixers. Via extensive experiments, we show that (1)
architecture is an indeed an important consideration when performing scaling
and (2) the best performing model can fluctuate at different scales. We believe
that the findings outlined in this work has significant implications to how
model architectures are currently evaluated in the community.","There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.",http://arxiv.org/pdf/2207.10551,"['Yi Tay', 'Mostafa Dehghani', 'Samira Abnar', 'Hyung Won Chung', 'William Fedus', 'Jinfeng Rao', 'Sharan Narang', 'Vinh Q. Tran', 'Dani Yogatama', 'Donald Metzler']",128,,,
"Language Models are General-Purpose Interfaces
Yaru Hao, Haoyu Song, Li Dong
Shaohan Huang, Zewen Chi, Wenhui Wang, Shuming M",2206.06336,Language Models are General-Purpose Interfaces,"Foundation models have received much attention due to their effectiveness
across a broad range of downstream applications. Though there is a big
convergence in terms of architecture, most pretrained models are typically
still developed for specific tasks or modalities. In this work, we propose to
use language models as a general-purpose interface to various foundation
models. A collection of pretrained encoders perceive diverse modalities (such
as vision, and language), and they dock with a language model that plays the
role of a universal task layer. We propose a semi-causal language modeling
objective to jointly pretrain the interface and the modular encoders. We
subsume the advantages and capabilities from both causal and non-causal
modeling, thereby combining the best of two worlds. Specifically, the proposed
method not only inherits the capabilities of in-context learning and open-ended
generation from causal language modeling, but also is conducive to finetuning
because of the bidirectional encoders. More importantly, our approach
seamlessly unlocks the combinations of the above capabilities, e.g., enabling
in-context learning or instruction following with finetuned encoders.
Experimental results across various language-only and vision-language
benchmarks show that our model outperforms or is competitive with specialized
models on finetuning, zero-shot generalization, and few-shot learning.","Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",http://arxiv.org/pdf/2206.06336,"['Yaru Hao', 'Haoyu Song', 'Li Dong', 'Shaohan Huang', 'Zewen Chi', 'Wenhui Wang', 'Shuming Ma', 'Furu Wei']",128,,,
"Alignment of Language Agents
Zachary Kenton, Tom Everitt, Laura Weidinger, Iason Gabriel, Vladimir Mikulikand Geoﬀrey Irving
Dee",2103.14659,Alignment of Language Agents,"For artificial intelligence to be beneficial to humans the behaviour of AI
agents needs to be aligned with what humans want. In this paper we discuss some
behavioural issues for language agents, arising from accidental
misspecification by the system designer. We highlight some ways that
misspecification can occur and discuss some behavioural issues that could arise
from misspecification, including deceptive or manipulative language, and review
some approaches for avoiding these issues.","For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",http://arxiv.org/pdf/2103.14659,"['Zachary Kenton', 'Tom Everitt', 'Laura Weidinger', 'Iason Gabriel', 'Vladimir Mikulik', 'Geoffrey Irving']",128,,,
"arXiv:1810.08810v1  [cs.LG]  20 Oct 2018The Frontiers of Fairness in Machine Learning
Alexandra Chouldechova∗Aaron Roth†
January",1810.0881,The Frontiers of Fairness in Machine Learning,"The last few years have seen an explosion of academic and popular interest in
algorithmic fairness. Despite this interest and the volume and velocity of work
that has been produced recently, the fundamental science of fairness in machine
learning is still in a nascent state. In March 2018, we convened a group of
experts as part of a CCC visioning workshop to assess the state of the field,
and distill the most promising research directions going forward. This report
summarizes the findings of that workshop. Along the way, it surveys recent
theoretical work in the field and points towards promising directions for
research.","The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.",http://arxiv.org/pdf/1810.08810,"['Alexandra Chouldechova', 'Aaron Roth']",128,,,
"Distilling Reasoning Capabilities into Smaller Language Models
Kumar ShridharAlessandro StolfoMrinmaya Sachan
Department of Co",2212.00193,Distilling Reasoning Capabilities into Smaller Language Models,"Step-by-step reasoning approaches like chain of thought (CoT) have proved to
be very effective in inducing reasoning capabilities in large language models.
However, the success of the CoT approach is fundamentally tied to the model
size, and billion parameter-scale models are often needed to get CoT to work.
In this paper, we propose a knowledge distillation approach that leverages the
step-by-step CoT reasoning capabilities of larger models and distills these
abilities into smaller models.
  In this work, we propose an alternative reasoning scheme, Socratic CoT, that
learns a decomposition of the original problem into a sequence of subproblems
and uses it to guide the intermediate reasoning steps. We use Socratic CoT to
train a combination of two small distilled models: a problem decomposer and a
subproblem solver. In practice, given a new problem, the two distilled models
work in sync to decompose and solve complex problems. On multiple reasoning
datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies
boosts the performance of smaller models over 70% compared to the baselines.
Finally, we investigate when Socratic CoT is an effective alternative to CoT,
demonstrating cases where a much smaller model (GPT-2 large) can outperform a
10X larger model (GPT-3 6B). Our code is available here:
https://github.com/kumar-shridhar/Distiiling-LM","Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.   In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM",http://arxiv.org/pdf/2212.00193,"['Kumar Shridhar', 'Alessandro Stolfo', 'Mrinmaya Sachan']",128,,,
"SFace: An Ecient Network for Face Detection
in Large Scale Variations
Jianfeng Wang12, Ye Yuan1y, Boxun Liy, Gang Yuyand Sun J",1804.06559,SFace: An Efficient Network for Face Detection in Large Scale Variations,"Face detection serves as a fundamental research topic for many applications
like face recognition. Impressive progress has been made especially with the
recent development of convolutional neural networks. However, the issue of
large scale variations, which widely exists in high resolution images/videos,
has not been well addressed in the literature. In this paper, we present a
novel algorithm called SFace, which efficiently integrates the anchor-based
method and anchor-free method to address the scale issues. A new dataset called
4K-Face is also introduced to evaluate the performance of face detection with
extreme large scale variations. The SFace architecture shows promising results
on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per
second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset,
which outperforms the state-of-art algorithms by almost one order of magnitude
in speed while achieves comparative performance.","Face detection serves as a fundamental research topic for many applications like face recognition. Impressive progress has been made especially with the recent development of convolutional neural networks. However, the issue of large scale variations, which widely exists in high resolution images/videos, has not been well addressed in the literature. In this paper, we present a novel algorithm called SFace, which efficiently integrates the anchor-based method and anchor-free method to address the scale issues. A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations. The SFace architecture shows promising results on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.",http://arxiv.org/pdf/1804.06559,"['Jianfeng Wang', 'Ye Yuan', 'Boxun Li', 'Gang Yu', 'Sun Jian']",128,,,
"DIALO GPT : Large-Scale Generative Pre-training
for Conversational Response Generation
Yizhe Zhang Siqi Sun Michel Galley Yen-Ch",1911.00536,DialoGPT: Large-Scale Generative Pre-training for Conversational Response Generation,"We present a large, tunable neural conversational response generation model,
DialoGPT (dialogue generative pre-trained transformer). Trained on 147M
conversation-like exchanges extracted from Reddit comment chains over a period
spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch
transformer to attain a performance close to human both in terms of automatic
and human evaluation in single-turn dialogue settings. We show that
conversational systems that leverage DialoGPT generate more relevant,
contentful and context-consistent responses than strong baseline systems. The
pre-trained model and training pipeline are publicly released to facilitate
research into neural response generation and the development of more
intelligent open-domain dialogue systems.","We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",http://arxiv.org/pdf/1911.00536,"['Yizhe Zhang', 'Siqi Sun', 'Michel Galley', 'Yen-Chun Chen', 'Chris Brockett', 'Xiang Gao', 'Jianfeng Gao', 'Jingjing Liu', 'Bill Dolan']",128,,,
"CrowS-Pairs: A Challenge Dataset for Measuring Social Biases
in Masked Language Models
Nikita NangiaClara VaniaRasika Bhalerao",2010.00133,CrowS-Pairs: A Challenge Dataset for Measuring Social Biases in Masked Language Models,"Pretrained language models, especially masked language models (MLMs) have
seen success across many NLP tasks. However, there is ample evidence that they
use the cultural biases that are undoubtedly present in the corpora they are
trained on, implicitly creating harm with biased representations. To measure
some forms of social bias in language models against protected demographic
groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark
(CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing
with nine types of bias, like race, religion, and age. In CrowS-Pairs a model
is presented with two sentences: one that is more stereotyping and another that
is less stereotyping. The data focuses on stereotypes about historically
disadvantaged groups and contrasts them with advantaged groups. We find that
all three of the widely-used MLMs we evaluate substantially favor sentences
that express stereotypes in every category in CrowS-Pairs. As work on building
less biased models advances, this dataset can be used as a benchmark to
evaluate progress.","Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",http://arxiv.org/pdf/2010.00133,"['Nikita Nangia', 'Clara Vania', 'Rasika Bhalerao', 'Samuel R. Bowman']",128,,,
"OPT: Open Pre-trained Transformer Language Models
Susan Zhang,Stephen Roller,Naman Goyal,
Mikel Artetxe ,Moya Chen ,Shuohui ",2205.01068,OPT: Open Pre-trained Transformer Language Models,"Large language models, which are often trained for hundreds of thousands of
compute days, have shown remarkable capabilities for zero- and few-shot
learning. Given their computational cost, these models are difficult to
replicate without significant capital. For the few that are available through
APIs, no access is granted to the full model weights, making them difficult to
study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only
pre-trained transformers ranging from 125M to 175B parameters, which we aim to
fully and responsibly share with interested researchers. We show that OPT-175B
is comparable to GPT-3, while requiring only 1/7th the carbon footprint to
develop. We are also releasing our logbook detailing the infrastructure
challenges we faced, along with code for experimenting with all of the released
models.","Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",http://arxiv.org/pdf/2205.01068,"['Susan Zhang', 'Stephen Roller', 'Naman Goyal', 'Mikel Artetxe', 'Moya Chen', 'Shuohui Chen', 'Christopher Dewan', 'Mona Diab', 'Xian Li', 'Xi Victoria Lin', 'Todor Mihaylov', 'Myle Ott', 'Sam Shleifer', 'Kurt Shuster', 'Daniel Simig', 'Punit Singh Koura', 'Anjali Sridhar', 'Tianlu Wang', 'Luke Zettlemoyer']",128,,,
"Leveraging Passage Retrieval with Generative Models
for Open Domain Question Answering
Gautier Izacard1;2;3Edouard Grave1
1Faceb",2007.01282,Leveraging Passage Retrieval with Generative Models for Open Domain Question Answering,"Generative models for open domain question answering have proven to be
competitive, without resorting to external knowledge. While promising, this
approach requires to use models with billions of parameters, which are
expensive to train and query. In this paper, we investigate how much these
models can benefit from retrieving text passages, potentially containing
evidence. We obtain state-of-the-art results on the Natural Questions and
TriviaQA open benchmarks. Interestingly, we observe that the performance of
this method significantly improves when increasing the number of retrieved
passages. This is evidence that generative models are good at aggregating and
combining evidence from multiple passages.","Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.",http://arxiv.org/pdf/2007.01282,"['Gautier Izacard', 'Edouard Grave']",128,,,
"Preprint
RT-1: R OBOTICS TRANSFORMER
FOR REAL-WORLD CONTROL AT SCALE
1Anthony Brohan∗, Noah Brown∗, Justice Carbajal∗, Yevgen Ch",2212.06817,RT-1: Robotics Transformer for Real-World Control at Scale,"By transferring knowledge from large, diverse, task-agnostic datasets, modern
machine learning models can solve specific downstream tasks either zero-shot or
with small task-specific datasets to a high level of performance. While this
capability has been demonstrated in other fields such as computer vision,
natural language processing or speech recognition, it remains to be shown in
robotics, where the generalization capabilities of the models are particularly
critical due to the difficulty of collecting real-world robotic data. We argue
that one of the keys to the success of such general robotic models lies with
open-ended task-agnostic training, combined with high-capacity architectures
that can absorb all of the diverse, robotic data. In this paper, we present a
model class, dubbed Robotics Transformer, that exhibits promising scalable
model properties. We verify our conclusions in a study of different model
classes and their ability to generalize as a function of the data size, model
size, and data diversity based on a large-scale data collection on real robots
performing real-world tasks. The project's website and videos can be found at
robotics-transformer1.github.io","By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io",http://arxiv.org/pdf/2212.06817,"['Anthony Brohan', 'Noah Brown', 'Justice Carbajal', 'Yevgen Chebotar', 'Joseph Dabis', 'Chelsea Finn', 'Keerthana Gopalakrishnan', 'Karol Hausman', 'Alex Herzog', 'Jasmine Hsu', 'Julian Ibarz', 'Brian Ichter', 'Alex Irpan', 'Tomas Jackson', 'Sally Jesmonth', 'Nikhil J Joshi', 'Ryan Julian', 'Dmitry Kalashnikov', 'Yuheng Kuang', 'Isabel Leal', 'Kuang-Huei Lee', 'Sergey Levine', 'Yao Lu', 'Utsav Malla', 'Deeksha Manjunath', 'Igor Mordatch', 'Ofir Nachum', 'Carolina Parada', 'Jodilyn Peralta', 'Emily Perez', 'Karl Pertsch', 'Jornell Quiambao', 'Kanishka Rao', 'Michael Ryoo', 'Grecia Salazar', 'Pannag Sanketi', 'Kevin Sayed', 'Jaspiar Singh', 'Sumedh Sontakke', 'Austin Stone', 'Clayton Tan', 'Huong Tran', 'Vincent Vanhoucke', 'Steve Vega', 'Quan Vuong', 'Fei Xia', 'Ted Xiao', 'Peng Xu', 'Sichun Xu', 'Tianhe Yu', 'Brianna Zitkovich']",128,,,
"Learning Performance-Improving Code Edits
LEARNING PERFORMANCE -IMPROVING CODE EDITS
Aman Madaan1, Alexander Shypula2, Uri Alon1",2302.07867,Learning Performance-Improving Code Edits,"The waning of Moore's Law has shifted the focus of the tech industry towards
alternative methods for continued performance gains. While optimizing compilers
are a standard tool to help increase program efficiency, programmers continue
to shoulder much responsibility in crafting and refactoring code with better
performance characteristics. In this paper, we investigate the ability of large
language models (LLMs) to suggest functionally correct, performance improving
code edits. We hypothesize that language models can suggest such edits in ways
that would be impractical for static analysis alone. We investigate these
questions by curating a large-scale dataset of Performance-Improving Edits,
PIE. PIE contains trajectories of programs, where a programmer begins with an
initial, slower version and iteratively makes changes to improve the program's
performance. We use PIE to evaluate and improve the capacity of large language
models. Specifically, use examples from PIE to fine-tune multiple variants of
CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use
examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By
leveraging PIE, we find that both CODEX and CODEGEN can generate
performance-improving edits, with speedups of more than 2.5x for over 25% of
the programs, for C++ and Python, even after the C++ programs were compiled
using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an
open-sourced and 10x smaller model than CODEX, to match the performance of
CODEX on this challenging task. Overall, this work opens new doors for creating
systems and methods that can help programmers write efficient code.","The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",http://arxiv.org/pdf/2302.07867,"['Aman Madaan', 'Alexander Shypula', 'Uri Alon', 'Milad Hashemi', 'Parthasarathy Ranganathan', 'Yiming Yang', 'Graham Neubig', 'Amir Yazdanbakhsh']",128,,,
"Published as a conference paper at ICLR 2022
EXT5: T OWARDS EXTREME MULTI -TASK SCALING
FOR TRANSFER LEARNING
Vamsi Aribandi†, ",2111.10952,ExT5: Towards Extreme Multi-Task Scaling for Transfer Learning,"Despite the recent success of multi-task learning and transfer learning for
natural language processing (NLP), few works have systematically studied the
effect of scaling up the number of tasks during pre-training. Towards this
goal, this paper introduces ExMix (Extreme Mixture): a massive collection of
107 supervised NLP tasks across diverse domains and task-families. Using ExMix,
we study the effect of multi-task pre-training at the largest scale to date,
and analyze co-training transfer amongst common families of tasks. Through this
analysis, we show that manually curating an ideal set of tasks for multi-task
pre-training is not straightforward, and that multi-task scaling can vastly
improve models on its own. Finally, we propose ExT5: a model pre-trained using
a multi-task objective of self-supervised span denoising and supervised ExMix.
Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on
SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of
ExMix. ExT5 also significantly improves sample efficiency while pre-training.","Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.",http://arxiv.org/pdf/2111.10952,"['Vamsi Aribandi', 'Yi Tay', 'Tal Schuster', 'Jinfeng Rao', 'Huaixiu Steven Zheng', 'Sanket Vaibhav Mehta', 'Honglei Zhuang', 'Vinh Q. Tran', 'Dara Bahri', 'Jianmo Ni', 'Jai Gupta', 'Kai Hui', 'Sebastian Ruder', 'Donald Metzler']",128,,,
"Training Language Models with Language Feedback
Jérémy Scheurer1Jon Ander Campos1 2Jun Shern Chan1Angelica Chen1
Kyunghyun Cho1 ",2204.14146,Training Language Models with Language Feedback,"Pretrained language models often do not perform tasks in ways that are in
line with our preferences, e.g., generating offensive text or factually
incorrect summaries. Recent work approaches the above issue by learning from a
simple form of human evaluation: comparisons between pairs of model-generated
task outputs. Comparison feedback conveys limited information about human
preferences per human evaluation. Here, we propose to learn from natural
language feedback, which conveys more information per human evaluation. We
learn from language feedback on model outputs using a three-step learning
algorithm. First, we condition the language model on the initial output and
feedback to generate many refinements. Second, we choose the refinement with
the highest similarity to the feedback. Third, we finetune a language model to
maximize the likelihood of the chosen refinement given the input. In synthetic
experiments, we first evaluate whether language models accurately incorporate
feedback to produce refinements, finding that only large language models (175B
parameters) do so. Using only 100 samples of human-written feedback, our
learning algorithm finetunes a GPT-3 model to roughly human-level summarization
ability.","Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.",http://arxiv.org/pdf/2204.14146,"['Jérémy Scheurer', 'Jon Ander Campos', 'Jun Shern Chan', 'Angelica Chen', 'Kyunghyun Cho', 'Ethan Perez']",128,,,
"Habitat: A Platform for Embodied AI Research
Manolis Savva1;4*, Abhishek Kadian1*, Oleksandr Maksymets1*, Yili Zhao1,
Erik Wijma",1904.01201,Habitat: A Platform for Embodied AI Research,"We present Habitat, a platform for research in embodied artificial
intelligence (AI). Habitat enables training embodied agents (virtual robots) in
highly efficient photorealistic 3D simulation. Specifically, Habitat consists
of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with
configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is
fast -- when rendering a scene from Matterport3D, it achieves several thousand
frames per second (fps) running single-threaded, and can reach over 10,000 fps
multi-process on a single GPU. (ii) Habitat-API: a modular high-level library
for end-to-end development of embodied AI algorithms -- defining tasks (e.g.,
navigation, instruction following, question answering), configuring, training,
and benchmarking embodied agents.
  These large-scale engineering contributions enable us to answer scientific
questions requiring experiments that were till now impracticable or 'merely'
impractical. Specifically, in the context of point-goal navigation: (1) we
revisit the comparison between learning and SLAM approaches from two recent
works and find evidence for the opposite conclusion -- that learning
outperforms SLAM if scaled to an order of magnitude more experience than
previous investigations, and (2) we conduct the first cross-dataset
generalization experiments {train, test} x {Matterport3D, Gibson} for multiple
sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors
generalize across datasets. We hope that our open-source platform and these
findings will advance research in embodied AI.","We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.   These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",http://arxiv.org/pdf/1904.01201,"['Manolis Savva', 'Abhishek Kadian', 'Oleksandr Maksymets', 'Yili Zhao', 'Erik Wijmans', 'Bhavana Jain', 'Julian Straub', 'Jia Liu', 'Vladlen Koltun', 'Jitendra Malik', 'Devi Parikh', 'Dhruv Batra']",128,,,
"QT-Opt: Scalable Deep Reinforcement Learning
for Vision-Based Robotic Manipulation
Dmitry Kalashnikov1, Alex Irpan1, Peter Pasto",1806.10293,QT-Opt: Scalable Deep Reinforcement Learning for Vision-Based Robotic Manipulation,"In this paper, we study the problem of learning vision-based dynamic
manipulation skills using a scalable reinforcement learning approach. We study
this problem in the context of grasping, a longstanding challenge in robotic
manipulation. In contrast to static learning behaviors that choose a grasp
point and then execute the desired grasp, our method enables closed-loop
vision-based control, whereby the robot continuously updates its grasp strategy
based on the most recent observations to optimize long-horizon grasp success.
To that end, we introduce QT-Opt, a scalable self-supervised vision-based
reinforcement learning framework that can leverage over 580k real-world grasp
attempts to train a deep neural network Q-function with over 1.2M parameters to
perform closed-loop, real-world grasping that generalizes to 96% grasp success
on unseen objects. Aside from attaining a very high success rate, our method
exhibits behaviors that are quite distinct from more standard grasping systems:
using only RGB vision-based perception from an over-the-shoulder camera, our
method automatically learns regrasping strategies, probes objects to find the
most effective grasps, learns to reposition objects and perform other
non-prehensile pre-grasp manipulations, and responds dynamically to
disturbances and perturbations.","In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",http://arxiv.org/pdf/1806.10293,"['Dmitry Kalashnikov', 'Alex Irpan', 'Peter Pastor', 'Julian Ibarz', 'Alexander Herzog', 'Eric Jang', 'Deirdre Quillen', 'Ethan Holly', 'Mrinal Kalakrishnan', 'Vincent Vanhoucke', 'Sergey Levine']",128,,,
"arXiv:2004.07213v2  [cs.CY]  20 Apr 2020Toward Trustworthy AI Development:
Mechanisms for Supporting Veriﬁable Claims∗
Miles Bru",2004.07213,Toward Trustworthy AI Development: Mechanisms for Supporting Verifiable Claims,"With the recent wave of progress in artificial intelligence (AI) has come a
growing awareness of the large-scale impacts of AI systems, and recognition
that existing regulations and norms in industry and academia are insufficient
to ensure responsible AI development. In order for AI developers to earn trust
from system users, customers, civil society, governments, and other
stakeholders that they are building AI responsibly, they will need to make
verifiable claims to which they can be held accountable. Those outside of a
given organization also need effective means of scrutinizing such claims. This
report suggests various steps that different stakeholders can take to improve
the verifiability of claims made about AI systems and their associated
development processes, with a focus on providing evidence about the safety,
security, fairness, and privacy protection of AI systems. We analyze ten
mechanisms for this purpose--spanning institutions, software, and hardware--and
make recommendations aimed at implementing, exploring, or improving those
mechanisms.","With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",http://arxiv.org/pdf/2004.07213,"['Miles Brundage', 'Shahar Avin', 'Jasmine Wang', 'Haydn Belfield', 'Gretchen Krueger', 'Gillian Hadfield', 'Heidy Khlaaf', 'Jingying Yang', 'Helen Toner', 'Ruth Fong', 'Tegan Maharaj', 'Pang Wei Koh', 'Sara Hooker', 'Jade Leung', 'Andrew Trask', 'Emma Bluemke', 'Jonathan Lebensold', ""Cullen O'Keefe"", 'Mark Koren', 'Théo Ryffel', 'JB Rubinovitz', 'Tamay Besiroglu', 'Federica Carugati', 'Jack Clark', 'Peter Eckersley', 'Sarah de Haas', 'Maritza Johnson', 'Ben Laurie', 'Alex Ingerman', 'Igor Krawczuk', 'Amanda Askell', 'Rosario Cammarota', 'Andrew Lohn', 'David Krueger', 'Charlotte Stix', 'Peter Henderson', 'Logan Graham', 'Carina Prunkl', 'Bianca Martin', 'Elizabeth Seger', 'Noa Zilberman', 'Seán Ó hÉigeartaigh', 'Frens Kroeger', 'Girish Sastry', 'Rebecca Kagan', 'Adrian Weller', 'Brian Tse', 'Elizabeth Barnes', 'Allan Dafoe', 'Paul Scharre', 'Ariel Herbert-Voss', 'Martijn Rasser', 'Shagun Sodhani', 'Carrick Flynn', 'Thomas Krendl Gilbert', 'Lisa Dyer', 'Saif Khan', 'Yoshua Bengio', 'Markus Anderljung']",128,,,
"Journal of Machine Learning Research 21 (2020) 1-44 Submitted 4/20; Revised 10/20; Published 11/20
Towards the Systematic Report",2002.05651,Towards the Systematic Reporting of the Energy and Carbon Footprints of Machine Learning,"Accurate reporting of energy and carbon usage is essential for understanding
the potential climate impacts of machine learning research. We introduce a
framework that makes this easier by providing a simple interface for tracking
realtime energy consumption and carbon emissions, as well as generating
standardized online appendices. Utilizing this framework, we create a
leaderboard for energy efficient reinforcement learning algorithms to
incentivize responsible research in this area as an example for other areas of
machine learning. Finally, based on case studies using our framework, we
propose strategies for mitigation of carbon emissions and reduction of energy
consumption. By making accounting easier, we hope to further the sustainable
development of machine learning experiments and spur more research into energy
efficient algorithms.","Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",http://arxiv.org/pdf/2002.05651,"['Peter Henderson', 'Jieru Hu', 'Joshua Romoff', 'Emma Brunskill', 'Dan Jurafsky', 'Joelle Pineau']",128,,,
"LIMA: Less Is More for Alignment
Chunting Zhou<Pengfei Liu<Puxin XuSrini IyerJiao Sun
Yuning MaoXuezhe MaAvia EfratPing ",2305.11206,LIMA: Less Is More for Alignment,"Large language models are trained in two stages: (1) unsupervised pretraining
from raw text, to learn general-purpose representations, and (2) large scale
instruction tuning and reinforcement learning, to better align to end tasks and
user preferences. We measure the relative importance of these two stages by
training LIMA, a 65B parameter LLaMa language model fine-tuned with the
standard supervised loss on only 1,000 carefully curated prompts and responses,
without any reinforcement learning or human preference modeling. LIMA
demonstrates remarkably strong performance, learning to follow specific
response formats from only a handful of examples in the training data,
including complex queries that range from planning trip itineraries to
speculating about alternate history. Moreover, the model tends to generalize
well to unseen tasks that did not appear in the training data. In a controlled
human study, responses from LIMA are either equivalent or strictly preferred to
GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard
and 65% versus DaVinci003, which was trained with human feedback. Taken
together, these results strongly suggest that almost all knowledge in large
language models is learned during pretraining, and only limited instruction
tuning data is necessary to teach models to produce high quality output.","Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",http://arxiv.org/pdf/2305.11206,"['Chunting Zhou', 'Pengfei Liu', 'Puxin Xu', 'Srini Iyer', 'Jiao Sun', 'Yuning Mao', 'Xuezhe Ma', 'Avia Efrat', 'Ping Yu', 'Lili Yu', 'Susan Zhang', 'Gargi Ghosh', 'Mike Lewis', 'Luke Zettlemoyer', 'Omer Levy']",128,,,
"SUPER -NATURAL INSTRUCTIONS :
Generalization via Declarative Instructions on 1600+ NLP Tasks
}Yizhong Wang2}Swaroop Mishra3|Pega",2204.07705,Super-NaturalInstructions: Generalization via Declarative Instructions on 1600+ NLP Tasks,"How well can NLP models generalize to a variety of unseen tasks when provided
with task instructions? To address this question, we first introduce
Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their
expert-written instructions. Our collection covers 76 distinct task types,
including but not limited to classification, extraction, infilling, sequence
tagging, text rewriting, and text composition. This large and diverse
collection of tasks enables rigorous benchmarking of cross-task generalization
under instructions -- training models to follow instructions on a subset of
tasks and evaluating them on the remaining unseen ones. Furthermore, we build
Tk-Instruct, a transformer model trained to follow a variety of in-context
instructions (plain language task definitions or k-shot examples). Our
experiments show that Tk-Instruct outperforms existing instruction-following
models such as InstructGPT by over 9% on our benchmark despite being an order
of magnitude smaller. We further analyze generalization as a function of
various scaling parameters, such as the number of observed tasks, the number of
instances per task, and model sizes. We hope our dataset and model facilitate
future progress towards more general-purpose NLP models.","How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",http://arxiv.org/pdf/2204.07705,"['Yizhong Wang', 'Swaroop Mishra', 'Pegah Alipoormolabashi', 'Yeganeh Kordi', 'Amirreza Mirzaei', 'Anjana Arunkumar', 'Arjun Ashok', 'Arut Selvan Dhanasekaran', 'Atharva Naik', 'David Stap', 'Eshaan Pathak', 'Giannis Karamanolakis', 'Haizhi Gary Lai', 'Ishan Purohit', 'Ishani Mondal', 'Jacob Anderson', 'Kirby Kuznia', 'Krima Doshi', 'Maitreya Patel', 'Kuntal Kumar Pal', 'Mehrad Moradshahi', 'Mihir Parmar', 'Mirali Purohit', 'Neeraj Varshney', 'Phani Rohitha Kaza', 'Pulkit Verma', 'Ravsehaj Singh Puri', 'Rushang Karia', 'Shailaja Keyur Sampat', 'Savan Doshi', 'Siddhartha Mishra', 'Sujan Reddy', 'Sumanta Patro', 'Tanay Dixit', 'Xudong Shen', 'Chitta Baral', 'Yejin Choi', 'Noah A. Smith', 'Hannaneh Hajishirzi', 'Daniel Khashabi']",128,,,
"Training a Helpful and Harmless Assistant with
Reinforcement Learning from Human Feedback
Yuntao Bai, Andy Jones, Kamal Ndousse",2204.05862,Training a Helpful and Harmless Assistant with Reinforcement Learning from Human Feedback,"We apply preference modeling and reinforcement learning from human feedback
(RLHF) to finetune language models to act as helpful and harmless assistants.
We find this alignment training improves performance on almost all NLP
evaluations, and is fully compatible with training for specialized skills such
as python coding and summarization. We explore an iterated online mode of
training, where preference models and RL policies are updated on a weekly
cadence with fresh human feedback data, efficiently improving our datasets and
models. Finally, we investigate the robustness of RLHF training, and identify a
roughly linear relation between the RL reward and the square root of the KL
divergence between the policy and its initialization. Alongside our main
results, we perform peripheral analyses on calibration, competing objectives,
and the use of OOD detection, compare our models with human writers, and
provide samples from our models using prompts appearing in recent related work.","We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",http://arxiv.org/pdf/2204.05862,"['Yuntao Bai', 'Andy Jones', 'Kamal Ndousse', 'Amanda Askell', 'Anna Chen', 'Nova DasSarma', 'Dawn Drain', 'Stanislav Fort', 'Deep Ganguli', 'Tom Henighan', 'Nicholas Joseph', 'Saurav Kadavath', 'Jackson Kernion', 'Tom Conerly', 'Sheer El-Showk', 'Nelson Elhage', 'Zac Hatfield-Dodds', 'Danny Hernandez', 'Tristan Hume', 'Scott Johnston', 'Shauna Kravec', 'Liane Lovitt', 'Neel Nanda', 'Catherine Olsson', 'Dario Amodei', 'Tom Brown', 'Jack Clark', 'Sam McCandlish', 'Chris Olah', 'Ben Mann', 'Jared Kaplan']",128,,,
"Uniﬁed Language Model Pre-training for
Natural Language Understanding and Generation
Li DongNan YangWenhui WangFuru WeiyXiao",1905.03197,Unified Language Model Pre-training for Natural Language Understanding and Generation,"This paper presents a new Unified pre-trained Language Model (UniLM) that can
be fine-tuned for both natural language understanding and generation tasks. The
model is pre-trained using three types of language modeling tasks:
unidirectional, bidirectional, and sequence-to-sequence prediction. The unified
modeling is achieved by employing a shared Transformer network and utilizing
specific self-attention masks to control what context the prediction conditions
on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0
and CoQA question answering tasks. Moreover, UniLM achieves new
state-of-the-art results on five natural language generation datasets,
including improving the CNN/DailyMail abstractive summarization ROUGE-L to
40.51 (2.04 absolute improvement), the Gigaword abstractive summarization
ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question
answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question
generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7
document-grounded dialog response generation NIST-4 to 2.67 (human performance
is 2.65). The code and pre-trained models are available at
https://github.com/microsoft/unilm.","This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.",http://arxiv.org/pdf/1905.03197,"['Li Dong', 'Nan Yang', 'Wenhui Wang', 'Furu Wei', 'Xiaodong Liu', 'Yu Wang', 'Jianfeng Gao', 'Ming Zhou', 'Hsiao-Wuen Hon']",128,,,
"All-in-One Image-Grounded Conversational Agents
Da Ju,Kurt Shuster ,Y-Lan Boureau and Jason Weston
Facebook AI Research
fdaju, ",1912.12394,All-in-One Image-Grounded Conversational Agents,"As single-task accuracy on individual language and image tasks has improved
substantially in the last few years, the long-term goal of a generally skilled
agent that can both see and talk becomes more feasible to explore. In this
work, we focus on leveraging individual language and image tasks, along with
resources that incorporate both vision and language towards that objective. We
design an architecture that combines state-of-the-art Transformer and ResNeXt
modules fed into a novel attentive multimodal module to produce a combined
model trained on many tasks. We provide a thorough analysis of the components
of the model, and transfer performance when training on one, some, or all of
the tasks. Our final models provide a single system that obtains good results
on all vision and language tasks considered, and improves the state-of-the-art
in image-grounded conversational applications.","As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore. In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective. We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks. We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks. Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.",http://arxiv.org/pdf/1912.12394,"['Da Ju', 'Kurt Shuster', 'Y-Lan Boureau', 'Jason Weston']",128,,,
"Atlas: Few-shot Learning with
Retrieval Augmented Language Models
Gautier Izacard∗ ∗,♦,♣,♥gizacard@fb.com
Patrick Lewis∗,♦plewis",2208.03299,Atlas: Few-shot Learning with Retrieval Augmented Language Models,"Large language models have shown impressive few-shot results on a wide range
of tasks. However, when knowledge is key for such results, as is the case for
tasks such as question answering and fact checking, massive parameter counts to
store knowledge seem to be needed. Retrieval augmented models are known to
excel at knowledge intensive tasks without the need for as many parameters, but
it is unclear whether they work in few-shot settings. In this work we present
Atlas, a carefully designed and pre-trained retrieval augmented language model
able to learn knowledge intensive tasks with very few training examples. We
perform evaluations on a wide range of tasks, including MMLU, KILT and
NaturalQuestions, and study the impact of the content of the document index,
showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy
on Natural Questions using only 64 examples, outperforming a 540B parameters
model by 3% despite having 50x fewer parameters.","Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",http://arxiv.org/pdf/2208.03299,"['Gautier Izacard', 'Patrick Lewis', 'Maria Lomeli', 'Lucas Hosseini', 'Fabio Petroni', 'Timo Schick', 'Jane Dwivedi-Yu', 'Armand Joulin', 'Sebastian Riedel', 'Edouard Grave']",128,,,
"arXiv:1910.10486v3  [cs.CL]  31 Oct 2020Does Gender Matter?
Towards Fairness in Dialogue Systems
Haochen Liu1, Jamell Dacon1, We",1910.10486,Does Gender Matter? Towards Fairness in Dialogue Systems,"Recently there are increasing concerns about the fairness of Artificial
Intelligence (AI) in real-world applications such as computer vision and
recommendations. For example, recognition algorithms in computer vision are
unfair to black people such as poorly detecting their faces and inappropriately
identifying them as ""gorillas"". As one crucial application of AI, dialogue
systems have been extensively applied in our society. They are usually built
with real human conversational data; thus they could inherit some fairness
issues which are held in the real world. However, the fairness of dialogue
systems has not been well investigated. In this paper, we perform a pioneering
study about the fairness issues in dialogue systems. In particular, we
construct a benchmark dataset and propose quantitative measures to understand
fairness in dialogue models. Our studies demonstrate that popular dialogue
models show significant prejudice towards different genders and races. Besides,
to mitigate the bias in dialogue systems, we propose two simple but effective
debiasing methods. Experiments show that our methods can reduce the bias in
dialogue systems significantly. The dataset and the implementation are released
to foster fairness research in dialogue systems.","Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as gorillas. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",http://arxiv.org/pdf/1910.10486,"['Haochen Liu', 'Jamell Dacon', 'Wenqi Fan', 'Hui Liu', 'Zitao Liu', 'Jiliang Tang']",128,,,
"Self-critiquing models for assisting human evaluators
William SaundersCatherine YehJeff Wu
Steven Bills Long Ouyang Jonathan ",2206.05802,Self-critiquing models for assisting human evaluators,"We fine-tune large language models to write natural language critiques
(natural language critical comments) using behavioral cloning. On a topic-based
summarization task, critiques written by our models help humans find flaws in
summaries that they would have otherwise missed. Our models help find naturally
occurring flaws in both model and human written summaries, and intentional
flaws in summaries written by humans to be deliberately misleading. We study
scaling properties of critiquing with both topic-based summarization and
synthetic tasks. Larger models write more helpful critiques, and on most tasks,
are better at self-critiquing, despite having harder-to-critique outputs.
Larger models can also integrate their own self-critiques as feedback, refining
their own summaries into better ones. Finally, we motivate and introduce a
framework for comparing critiquing ability to generation and discrimination
ability. Our measurements suggest that even large models may still have
relevant knowledge they cannot or do not articulate as critiques. These results
are a proof of concept for using AI-assisted human feedback to scale the
supervision of machine learning systems to tasks that are difficult for humans
to evaluate directly. We release our training datasets, as well as samples from
our critique assistance experiments.","We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.",http://arxiv.org/pdf/2206.05802,"['William Saunders', 'Catherine Yeh', 'Jeff Wu', 'Steven Bills', 'Long Ouyang', 'Jonathan Ward', 'Jan Leike']",128,,,
"ReCoCoRD: Bridging the Gap between Human
and Machine Commonsense Reading Comprehension
Sheng Zhangy, Xiaodong Liuz, Jingjing Li",1810.12885,ReCoRD: Bridging the Gap between Human and Machine Commonsense Reading Comprehension,"We present a large-scale dataset, ReCoRD, for machine reading comprehension
requiring commonsense reasoning. Experiments on this dataset demonstrate that
the performance of state-of-the-art MRC systems fall far behind human
performance. ReCoRD represents a challenge for future research to bridge the
gap between human and machine commonsense reading comprehension. ReCoRD is
available at http://nlp.jhu.edu/record.","We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.",http://arxiv.org/pdf/1810.12885,"['Sheng Zhang', 'Xiaodong Liu', 'Jingjing Liu', 'Jianfeng Gao', 'Kevin Duh', 'Benjamin Van Durme']",128,,,
"PEER: A Collaborative Language Model
Timo Schick}Jane Dwivedi-Yu}Zhengbao Jiang};~Fabio Petroni}
Patrick Lewis}Gautier Izacard};",2208.11663,PEER: A Collaborative Language Model,"Textual content is often the output of a collaborative writing process: We
start with an initial draft, ask for suggestions, and repeatedly make changes.
Agnostic of this process, today's language models are trained to generate only
the final result. As a consequence, they lack several abilities crucial for
collaborative writing: They are unable to update existing texts, difficult to
control and incapable of verbally planning or explaining their actions. To
address these shortcomings, we introduce PEER, a collaborative language model
that is trained to imitate the entire writing process itself: PEER can write
drafts, add suggestions, propose edits and provide explanations for its
actions. Crucially, we train multiple instances of PEER able to infill various
parts of the writing process, enabling the use of self-training techniques for
increasing the quality, amount and diversity of training data. This unlocks
PEER's full potential by making it applicable in domains for which no edit
histories are available and improving its ability to follow instructions, to
write useful comments, and to explain its actions. We show that PEER achieves
strong performance across various domains and editing tasks.","Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.",http://arxiv.org/pdf/2208.11663,"['Timo Schick', 'Jane Dwivedi-Yu', 'Zhengbao Jiang', 'Fabio Petroni', 'Patrick Lewis', 'Gautier Izacard', 'Qingfei You', 'Christoforos Nalmpantis', 'Edouard Grave', 'Sebastian Riedel']",128,,,
"Megatron-LM: Training Multi-Billion Parameter Language Models Using
Model Parallelism
Mohammad Shoeybi1 2Mostofa Patwary1 2Raul ",1909.08053,Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism,"Recent work in language modeling demonstrates that training large transformer
models advances the state of the art in Natural Language Processing
applications. However, very large models can be quite difficult to train due to
memory constraints. In this work, we present our techniques for training very
large transformer models and implement a simple, efficient intra-layer model
parallel approach that enables training transformer models with billions of
parameters. Our approach does not require a new compiler or library changes, is
orthogonal and complimentary to pipeline model parallelism, and can be fully
implemented with the insertion of a few communication operations in native
PyTorch. We illustrate this approach by converging transformer based models up
to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the
entire application with 76% scaling efficiency when compared to a strong single
GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To
demonstrate that large language models can further advance the state of the art
(SOTA), we train an 8.3 billion parameter transformer language model similar to
GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful
attention to the placement of layer normalization in BERT-like models is
critical to achieving increased performance as the model size grows. Using the
GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA
perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%)
datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9%
compared to SOTA accuracy of 89.4%).","Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",http://arxiv.org/pdf/1909.08053,"['Mohammad Shoeybi', 'Mostofa Patwary', 'Raul Puri', 'Patrick LeGresley', 'Jared Casper', 'Bryan Catanzaro']",128,,,
"Published as a conference paper at ICLR 2016
HIGH-DIMENSIONAL CONTINUOUS CONTROL USING
GENERALIZED ADVANTAGE ESTIMATION
John Sch",1506.02438,High-Dimensional Continuous Control Using Generalized Advantage Estimation,"Policy gradient methods are an appealing approach in reinforcement learning
because they directly optimize the cumulative reward and can straightforwardly
be used with nonlinear function approximators such as neural networks. The two
main challenges are the large number of samples typically required, and the
difficulty of obtaining stable and steady improvement despite the
nonstationarity of the incoming data. We address the first challenge by using
value functions to substantially reduce the variance of policy gradient
estimates at the cost of some bias, with an exponentially-weighted estimator of
the advantage function that is analogous to TD(lambda). We address the second
challenge by using trust region optimization procedure for both the policy and
the value function, which are represented by neural networks.
  Our approach yields strong empirical results on highly challenging 3D
locomotion tasks, learning running gaits for bipedal and quadrupedal simulated
robots, and learning a policy for getting the biped to stand up from starting
out lying on the ground. In contrast to a body of prior work that uses
hand-crafted policy representations, our neural network policies map directly
from raw kinematics to joint torques. Our algorithm is fully model-free, and
the amount of simulated experience required for the learning tasks on 3D bipeds
corresponds to 1-2 weeks of real time.","Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",http://arxiv.org/pdf/1506.02438,"['John Schulman', 'Philipp Moritz', 'Sergey Levine', 'Michael Jordan', 'Pieter Abbeel']",128,,,
"Gender Bias in Contextualized Word Embeddings
Jieyu ZhaoxTianlu WangyMark Yatskarz
Ryan Cotterell@Vicente OrdonezyKai-Wei Changx",1904.0331,Gender Bias in Contextualized Word Embeddings,"In this paper, we quantify, analyze and mitigate gender bias exhibited in
ELMo's contextualized word vectors. First, we conduct several intrinsic
analyses and find that (1) training data for ELMo contains significantly more
male than female entities, (2) the trained ELMo embeddings systematically
encode gender information and (3) ELMo unequally encodes gender information
about male and female entities. Then, we show that a state-of-the-art
coreference system that depends on ELMo inherits its bias and demonstrates
significant bias on the WinoBias probing corpus. Finally, we explore two
methods to mitigate such gender bias and show that the bias demonstrated on
WinoBias can be eliminated.","In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",http://arxiv.org/pdf/1904.03310,"['Jieyu Zhao', 'Tianlu Wang', 'Mark Yatskar', 'Ryan Cotterell', 'Vicente Ordonez', 'Kai-Wei Chang']",128,,,
"Mirages: On Anthropomorphism in Dialogue Systems
Gavin Abercrombie
Heriot-Watt University
g.abercrombie@hw.ac.uk
Tanvi Dinkar
He",2305.098,Mirages: On Anthropomorphism in Dialogue Systems,"Automated dialogue or conversational systems are anthropomorphised by
developers and personified by users. While a degree of anthropomorphism is
inevitable, conscious and unconscious design choices can guide users to
personify them to varying degrees. Encouraging users to relate to automated
systems as if they were human can lead to transparency and trust issues, and
high risk scenarios caused by over-reliance on their outputs. As a result,
natural language processing researchers have begun to investigate factors that
induce personification and develop resources to mitigate such effects. However,
these efforts are fragmented, and many aspects of anthropomorphism have yet to
be considered. In this paper, we discuss the linguistic factors that contribute
to the anthropomorphism of dialogue systems and the harms that can arise,
arguing that it can reinforce stereotypes of gender roles and notions of
acceptable language. We recommend that future efforts towards developing
dialogue systems take particular care in their design, development, release,
and description; and attend to the many linguistic cues that can elicit
personification by users.","Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",http://arxiv.org/pdf/2305.09800,"['Gavin Abercrombie', 'Amanda Cercas Curry', 'Tanvi Dinkar', 'Zeerak Talat']",128,,,
"Wav2Letter: an End-to-End ConvNet-based Speech
Recognition System
Ronan Collobert
Facebook AI Research, Menlo Park
locronan@fb.c",1609.03193,Wav2Letter: an End-to-End ConvNet-based Speech Recognition System,"This paper presents a simple end-to-end model for speech recognition,
combining a convolutional network based acoustic model and a graph decoding. It
is trained to output letters, with transcribed speech, without the need for
force alignment of phonemes. We introduce an automatic segmentation criterion
for training from sequence annotation without alignment that is on par with CTC
while being simpler. We show competitive results in word error rate on the
Librispeech corpus with MFCC features, and promising results from raw waveform.","This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.",http://arxiv.org/pdf/1609.03193,"['Ronan Collobert', 'Christian Puhrsch', 'Gabriel Synnaeve']",128,,,
"Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation
Emily Dinan, Angela Fany, Adina Williams, Jack Urbanek,",1911.03842,Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation,"Models often easily learn biases present in the training data, and their
predictions directly reflect this bias. We analyze gender bias in dialogue
data, and examine how this bias is actually amplified in subsequent generative
chit-chat dialogue models. We measure gender bias in six existing dialogue
datasets, and focus on the most biased one, the multi-player text-based fantasy
adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The
LIGHT dataset is highly imbalanced with respect to gender, containing
predominantly male characters, likely because it is entirely collected by
crowdworkers and reflects common biases that exist in fantasy or medieval
settings. We consider three techniques to mitigate gender bias: counterfactual
data augmentation, targeted data collection, and bias controlled training. We
show that our proposed techniques mitigate gender bias in LIGHT by balancing
the genderedness of generated dialogue utterances and are particularly
effective in combination. We quantify performance using various evaluation
methods---such as quantity of gendered words, a dialogue safety classifier, and
human studies---all of which show that our models generate less gendered, but
equally engaging chit-chat responses.","Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.",http://arxiv.org/pdf/1911.03842,"['Emily Dinan', 'Angela Fan', 'Adina Williams', 'Jack Urbanek', 'Douwe Kiela', 'Jason Weston']",128,,,
"ACUTE -EVAL : Improved dialogue evaluation with optimized questions and
multi-turn comparisons
Margaret Li
Facebook AI Research
",1909.03087,ACUTE-EVAL: Improved Dialogue Evaluation with Optimized Questions and Multi-turn Comparisons,"While dialogue remains an important end-goal of natural language research,
the difficulty of evaluation is an oft-quoted reason why it remains troublesome
to make real progress towards its solution. Evaluation difficulties are
actually two-fold: not only do automatic metrics not correlate well with human
judgments, but also human judgments themselves are in fact difficult to
measure. The two most used human judgment tests, single-turn pairwise
evaluation and multi-turn Likert scores, both have serious flaws as we discuss
in this work.
  We instead provide a novel procedure involving comparing two full dialogues,
where a human judge is asked to pay attention to only one speaker within each,
and make a pairwise judgment. The questions themselves are optimized to
maximize the robustness of judgments across different annotators, resulting in
better tests. We also show how these tests work in self-play model chat setups,
resulting in faster, cheaper tests. We hope these tests become the de facto
standard, and will release open-source code to that end.","While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work.   We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.",http://arxiv.org/pdf/1909.03087,"['Margaret Li', 'Jason Weston', 'Stephen Roller']",128,,,
"Deep Learning Based Text Classification: A Comprehensive Review
Shervin Minaee, Snapchat Inc
Nal Kalchbrenner, Google Brain, Ams",2004.03705,Deep Learning Based Text Classification: A Comprehensive Review,"Deep learning based models have surpassed classical machine learning based
approaches in various text classification tasks, including sentiment analysis,
news categorization, question answering, and natural language inference. In
this paper, we provide a comprehensive review of more than 150 deep learning
based models for text classification developed in recent years, and discuss
their technical contributions, similarities, and strengths. We also provide a
summary of more than 40 popular datasets widely used for text classification.
Finally, we provide a quantitative analysis of the performance of different
deep learning models on popular benchmarks, and discuss future research
directions.","Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.",http://arxiv.org/pdf/2004.03705,"['Shervin Minaee', 'Nal Kalchbrenner', 'Erik Cambria', 'Narjes Nikzad', 'Meysam Chenaghlu', 'Jianfeng Gao']",128,,,
"OpenAssistant Conversations - Democratizing Large
Language Model Alignment
Andreas Köpf
andreas.koepf@provisio.comYannic Kilche",2304.07327,OpenAssistant Conversations -- Democratizing Large Language Model Alignment,"Aligning large language models (LLMs) with human preferences has proven to
drastically improve usability and has driven rapid adoption as demonstrated by
ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and
reinforcement learning from human feedback (RLHF) greatly reduce the required
skill and domain knowledge to effectively harness the capabilities of LLMs,
increasing their accessibility and utility across various domains. However,
state-of-the-art alignment techniques like RLHF rely on high-quality human
feedback data, which is expensive to create and often remains proprietary. In
an effort to democratize research on large-scale alignment, we release
OpenAssistant Conversations, a human-generated, human-annotated assistant-style
conversation corpus consisting of 161,443 messages distributed across 66,497
conversation trees, in 35 different languages, annotated with 461,292 quality
ratings. The corpus is a product of a worldwide crowd-sourcing effort involving
over 13,500 volunteers. To demonstrate the OpenAssistant Conversations
dataset's effectiveness, we present OpenAssistant, the first fully open-source
large-scale instruction-tuned model to be trained on human data. A preference
study revealed that OpenAssistant replies are comparably preferred to
GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7%
respectively. We release our code and data under fully permissive licenses.","Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7% respectively. We release our code and data under fully permissive licenses.",http://arxiv.org/pdf/2304.07327,"['Andreas Köpf', 'Yannic Kilcher', 'Dimitri von Rütte', 'Sotiris Anagnostidis', 'Zhi-Rui Tam', 'Keith Stevens', 'Abdullah Barhoum', 'Nguyen Minh Duc', 'Oliver Stanley', 'Richárd Nagyfi', 'Shahul ES', 'Sameer Suri', 'David Glushkov', 'Arnav Dantuluri', 'Andrew Maguire', 'Christoph Schuhmann', 'Huu Nguyen', 'Alexander Mattick']",128,,,
"Evaluating the Social Impact of Generative AI Systems
in Systems and Society
Irene Solaiman∗
Hugging FaceZeerak Talat∗
Independe",2306.05949,Evaluating the Social Impact of Generative AI Systems in Systems and Society,"Generative AI systems across modalities, ranging from text, image, audio, and
video, have broad social impacts, but there exists no official standard for
means of evaluating those impacts and which impacts should be evaluated. We
move toward a standard approach in evaluating a generative AI system for any
modality, in two overarching categories: what is able to be evaluated in a base
system that has no predetermined application and what is able to be evaluated
in society. We describe specific social impact categories and how to approach
and conduct evaluations in the base technical system, then in people and
society. Our framework for a base system defines seven categories of social
impact: bias, stereotypes, and representational harms; cultural values and
sensitive content; disparate performance; privacy and data protection;
financial costs; environmental costs; and data and content moderation labor
costs. Suggested methods for evaluation apply to all modalities and analyses of
the limitations of existing evaluations serve as a starting point for necessary
investment in future evaluations. We offer five overarching categories for what
is able to be evaluated in society, each with their own subcategories:
trustworthiness and autonomy; inequality, marginalization, and violence;
concentration of authority; labor and creativity; and ecosystem and
environment. Each subcategory includes recommendations for mitigating harm. We
are concurrently crafting an evaluation repository for the AI research
community to contribute existing evaluations along the given categories. This
version will be updated following a CRAFT session at ACM FAccT 2023.","Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.",http://arxiv.org/pdf/2306.05949,"['Irene Solaiman', 'Zeerak Talat', 'William Agnew', 'Lama Ahmad', 'Dylan Baker', 'Su Lin Blodgett', 'Hal Daumé III', 'Jesse Dodge', 'Ellie Evans', 'Sara Hooker', 'Yacine Jernite', 'Alexandra Sasha Luccioni', 'Alberto Lusoli', 'Margaret Mitchell', 'Jessica Newman', 'Marie-Therese Png', 'Andrew Strait', 'Apostol Vassilev']",128,,,
"1
Training Production Language Models
without Memorizing User Data
Swaroop Ramaswamy*, Om Thakkar*, Rajiv Mathews, Galen Andrew,",2009.10031,Training Production Language Models without Memorizing User Data,"This paper presents the first consumer-scale next-word prediction (NWP) model
trained with Federated Learning (FL) while leveraging the Differentially
Private Federated Averaging (DP-FedAvg) technique. There has been prior work on
building practical FL infrastructure, including work demonstrating the
feasibility of training language models on mobile devices using such
infrastructure. It has also been shown (in simulations on a public corpus) that
it is possible to train NWP models with user-level differential privacy using
the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models
with DP-FedAvg in a real-world production environment on a heterogeneous fleet
of mobile phones requires addressing numerous challenges. For instance, the
coordinating central server has to keep track of the devices available at the
start of each round and sample devices uniformly at random from them, while
ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL
work of which we are aware, for the first time we demonstrate the deployment of
a differentially private mechanism for the training of a production neural
network in FL, as well as the instrumentation of the production training
infrastructure to perform an end-to-end empirical measurement of unintended
memorization.","This paper presents the first consumer-scale next-word prediction (NWP) model trained with Federated Learning (FL) while leveraging the Differentially Private Federated Averaging (DP-FedAvg) technique. There has been prior work on building practical FL infrastructure, including work demonstrating the feasibility of training language models on mobile devices using such infrastructure. It has also been shown (in simulations on a public corpus) that it is possible to train NWP models with user-level differential privacy using the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models with DP-FedAvg in a real-world production environment on a heterogeneous fleet of mobile phones requires addressing numerous challenges. For instance, the coordinating central server has to keep track of the devices available at the start of each round and sample devices uniformly at random from them, while ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL work of which we are aware, for the first time we demonstrate the deployment of a differentially private mechanism for the training of a production neural network in FL, as well as the instrumentation of the production training infrastructure to perform an end-to-end empirical measurement of unintended memorization.",http://arxiv.org/pdf/2009.10031,"['Swaroop Ramaswamy', 'Om Thakkar', 'Rajiv Mathews', 'Galen Andrew', 'H. Brendan McMahan', 'Françoise Beaufays']",128,,,
"Published as a conference paper at ICLR 2023
LARGE LANGUAGE MODELS ARE HUMAN -LEVEL
PROMPT ENGINEERS
Yongchao Zhou1;2;, Andrei ",2211.0191,Large Language Models Are Human-Level Prompt Engineers,"By conditioning on natural language instructions, large language models
(LLMs) have displayed impressive capabilities as general-purpose computers.
However, task performance depends significantly on the quality of the prompt
used to steer the model, and most effective prompts have been handcrafted by
humans. Inspired by classical program synthesis and the human approach to
prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic
instruction generation and selection. In our method, we treat the instruction
as the ""program,"" optimized by searching over a pool of instruction candidates
proposed by an LLM in order to maximize a chosen score function. To evaluate
the quality of the selected instruction, we evaluate the zero-shot performance
of another LLM following the selected instruction. Experiments on 24 NLP tasks
show that our automatically generated instructions outperform the prior LLM
baseline by a large margin and achieve better or comparable performance to the
instructions generated by human annotators on 19/24 tasks. We conduct extensive
qualitative and quantitative analyses to explore the performance of APE. We
show that APE-engineered prompts can be applied to steer models toward
truthfulness and/or informativeness, as well as to improve few-shot learning
performance by simply prepending them to standard in-context learning prompts.
Please check out our webpage at
https://sites.google.com/view/automatic-prompt-engineer.","By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the program, optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",http://arxiv.org/pdf/2211.01910,"['Yongchao Zhou', 'Andrei Ioan Muresanu', 'Ziwen Han', 'Keiran Paster', 'Silviu Pitis', 'Harris Chan', 'Jimmy Ba']",128,,,
"Large Language Models Struggle to Learn Long-Tail Knowledge
Nikhil Kandpal1Haikang Deng1Adam Roberts2Eric Wallace3Colin Raffel1
",2211.08411,Large Language Models Struggle to Learn Long-Tail Knowledge,"The Internet contains a wealth of knowledge -- from the birthdays of
historical figures to tutorials on how to code -- all of which may be learned
by language models. However, while certain pieces of information are ubiquitous
on the web, others appear extremely rarely. In this paper, we study the
relationship between the knowledge memorized by large language models and the
information in pre-training datasets scraped from the web. In particular, we
show that a language model's ability to answer a fact-based question relates to
how many documents associated with that question were seen during pre-training.
We identify these relevant documents by entity linking pre-training datasets
and counting documents that contain the same entities as a given
question-answer pair. Our results demonstrate strong correlational and causal
relationships between accuracy and relevant document count for numerous
question answering datasets (e.g., TriviaQA), pre-training corpora (e.g.,
ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models
are better at learning long-tail knowledge, we estimate that today's models
must be scaled by many orders of magnitude to reach competitive QA performance
on questions with little support in the pre-training data. Finally, we show
that retrieval-augmentation can reduce the dependence on relevant pre-training
information, presenting a promising approach for capturing the long-tail.","The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.",http://arxiv.org/pdf/2211.08411,"['Nikhil Kandpal', 'Haikang Deng', 'Adam Roberts', 'Eric Wallace', 'Colin Raffel']",128,,,
"Training Veriers to Solve Math Word Problems
Karl Cobbe∗Vineet Kosaraju∗Mohammad Bavarian Mark Chen
Heewoo Jun  Lukasz Kaiser M",2110.14168,Training Verifiers to Solve Math Word Problems,"State-of-the-art language models can match human performance on many tasks,
but they still struggle to robustly perform multi-step mathematical reasoning.
To diagnose the failures of current models and support research, we introduce
GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math
word problems. We find that even the largest transformer models fail to achieve
high test performance, despite the conceptual simplicity of this problem
distribution. To increase performance, we propose training verifiers to judge
the correctness of model completions. At test time, we generate many candidate
solutions and select the one ranked highest by the verifier. We demonstrate
that verification significantly improves performance on GSM8K, and we provide
strong empirical evidence that verification scales more effectively with
increased data than a finetuning baseline.","State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",http://arxiv.org/pdf/2110.14168,"['Karl Cobbe', 'Vineet Kosaraju', 'Mohammad Bavarian', 'Mark Chen', 'Heewoo Jun', 'Lukasz Kaiser', 'Matthias Plappert', 'Jerry Tworek', 'Jacob Hilton', 'Reiichiro Nakano', 'Christopher Hesse', 'John Schulman']",128,,,
"Published as a conference paper at ICLR 2017
SGDR: S TOCHASTIC GRADIENT DESCENT WITH
WARM RESTARTS
Ilya Loshchilov & Frank Hutte",1608.03983,SGDR: Stochastic Gradient Descent with Warm Restarts,"Restart techniques are common in gradient-free optimization to deal with
multimodal functions. Partial warm restarts are also gaining popularity in
gradient-based optimization to improve the rate of convergence in accelerated
gradient schemes to deal with ill-conditioned functions. In this paper, we
propose a simple warm restart technique for stochastic gradient descent to
improve its anytime performance when training deep neural networks. We
empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where
we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively.
We also demonstrate its advantages on a dataset of EEG recordings and on a
downsampled version of the ImageNet dataset. Our source code is available at
https://github.com/loshchil/SGDR","Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",http://arxiv.org/pdf/1608.03983,"['Ilya Loshchilov', 'Frank Hutter']",128,,,
"The False Promise of Imitating Proprietary LLMs
Arnav Gudibande∗
UC Berkeley
arnavg@berkeley.eduEric Wallace∗
UC Berkeley
ericwa",2305.15717,The False Promise of Imitating Proprietary LLMs,"An emerging method to cheaply improve a weaker language model is to finetune
it on outputs from a stronger model, such as a proprietary system like ChatGPT
(e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply
imitate the proprietary model's capabilities using a weaker open-source model.
In this work, we critically analyze this approach. We first finetune a series
of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data
sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the
models using crowd raters and canonical NLP benchmarks. Initially, we were
surprised by the output quality of our imitation models -- they appear far
better at following instructions, and crowd workers rate their outputs as
competitive with ChatGPT. However, when conducting more targeted automatic
evaluations, we find that imitation models close little to none of the gap from
the base LM to ChatGPT on tasks that are not heavily supported in the imitation
data. We show that these performance discrepancies may slip past human raters
because imitation models are adept at mimicking ChatGPT's style but not its
factuality. Overall, we conclude that model imitation is a false promise: there
exists a substantial capabilities gap between open and closed LMs that, with
current methods, can only be bridged using an unwieldy amount of imitation data
or by using more capable base LMs. In turn, we argue that the highest leverage
action for improving open-source models is to tackle the difficult challenge of
developing better base LMs, rather than taking the shortcut of imitating
proprietary systems.","An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",http://arxiv.org/pdf/2305.15717,"['Arnav Gudibande', 'Eric Wallace', 'Charlie Snell', 'Xinyang Geng', 'Hao Liu', 'Pieter Abbeel', 'Sergey Levine', 'Dawn Song']",128,,,
"arXiv:2202.12040v4  [cs.LG]  18 Sep 2023Self-Training: A Survey
Massih-Reza Amini1, Vasilii Feofanov2, Loic Pauletto3,
Li` es Ha",2202.1204,Self-Training: A Survey,"Semi-supervised algorithms aim to learn prediction functions from a small set
of labeled observations and a large set of unlabeled observations. Because this
framework is relevant in many applications, they have received a lot of
interest in both academia and industry. Among the existing techniques,
self-training methods have undoubtedly attracted greater attention in recent
years. These models are designed to find the decision boundary on low density
regions without making additional assumptions about the data distribution, and
use the unsigned output score of a learned classifier, or its margin, as an
indicator of confidence. The working principle of self-training algorithms is
to learn a classifier iteratively by assigning pseudo-labels to the set of
unlabeled training samples with a margin greater than a certain threshold. The
pseudo-labeled examples are then used to enrich the labeled training data and
to train a new classifier in conjunction with the labeled training set. In this
paper, we present self-training methods for binary and multi-class
classification; as well as their variants and two related approaches, namely
consistency-based approaches and transductive learning. We examine the impact
of significant self-training features on various methods, using different
general and image classification benchmarks, and we discuss our ideas for
future research in self-training. To the best of our knowledge, this is the
first thorough and complete survey on this subject.","Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multi-class classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.",http://arxiv.org/pdf/2202.12040,"['Massih-Reza Amini', 'Vasilii Feofanov', 'Loic Pauletto', 'Lies Hadjadj', 'Emilie Devijver', 'Yury Maximov']",128,,,
"CAN MACHINES LEARN MORALITY ?
THE
:COMMONSENSE MORAL MACHINES FOR
ETHICAL JUDGMENTS ON EVERYDAY SITUATIONS
Liwei Jiang∫∑Jena D. ",2110.07574,Can Machines Learn Morality? The Delphi Experiment,"As AI systems become increasingly powerful and pervasive, there are growing
concerns about machines' morality or a lack thereof. Yet, teaching morality to
machines is a formidable task, as morality remains among the most intensely
debated questions in humanity, let alone for AI. Existing AI systems deployed
to millions of users, however, are already making decisions loaded with moral
implications, which poses a seemingly impossible challenge: teaching machines
moral sense, while humanity continues to grapple with it.
  To explore this challenge, we introduce Delphi, an experimental framework
based on deep neural networks trained directly to reason about descriptive
ethical judgments, e.g., ""helping a friend"" is generally good, while ""helping a
friend spread fake news"" is not. Empirical results shed novel insights on the
promises and limits of machine ethics; Delphi demonstrates strong
generalization capabilities in the face of novel ethical situations, while
off-the-shelf neural network models exhibit markedly poor judgment including
unjust biases, confirming the need for explicitly teaching machines moral
sense.
  Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and
inconsistencies. Despite that, we demonstrate positive use cases of imperfect
Delphi, including using it as a component model within other imperfect AI
systems. Importantly, we interpret the operationalization of Delphi in light of
prominent ethical theories, which leads us to important future research
questions.","As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., helping a friend is generally good, while helping a friend spread fake news is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.",http://arxiv.org/pdf/2110.07574,"['Liwei Jiang', 'Jena D. Hwang', 'Chandra Bhagavatula', 'Ronan Le Bras', 'Jenny Liang', 'Jesse Dodge', 'Keisuke Sakaguchi', 'Maxwell Forbes', 'Jon Borchardt', 'Saadia Gabriel', 'Yulia Tsvetkov', 'Oren Etzioni', 'Maarten Sap', 'Regina Rini', 'Yejin Choi']",128,,,
"Equality of Opportunity in Supervised Learning
Moritz Hardt Eric Price Nathan Srebro
February 18, 2022
Abstract
We propose a cri",1610.02413,Equality of Opportunity in Supervised Learning,"We propose a criterion for discrimination against a specified sensitive
attribute in supervised learning, where the goal is to predict some target
based on available features. Assuming data about the predictor, target, and
membership in the protected group are available, we show how to optimally
adjust any learned predictor so as to remove discrimination according to our
definition. Our framework also improves incentives by shifting the cost of poor
classification from disadvantaged groups to the decision maker, who can respond
by improving the classification accuracy.
  In line with other studies, our notion is oblivious: it depends only on the
joint statistics of the predictor, the target and the protected attribute, but
not on interpretation of individualfeatures. We study the inherent limits of
defining and identifying biases based on such oblivious measures, outlining
what can and cannot be inferred from different oblivious tests.
  We illustrate our notion using a case study of FICO credit scores.","We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.   In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.   We illustrate our notion using a case study of FICO credit scores.",http://arxiv.org/pdf/1610.02413,"['Moritz Hardt', 'Eric Price', 'Nathan Srebro']",128,,,
"Published as a conference paper at ICLR 2020
LARGE BATCH OPTIMIZATION FOR DEEP LEARNING :
TRAINING BERT IN76MINUTES
Yang You2, J",1904.00962,Large Batch Optimization for Deep Learning: Training BERT in 76 minutes,"Training large deep neural networks on massive datasets is computationally
very challenging. There has been recent surge in interest in using large batch
stochastic optimization methods to tackle this issue. The most prominent
algorithm in this line of research is LARS, which by employing layerwise
adaptive learning rates trains ResNet on ImageNet in a few minutes. However,
LARS performs poorly for attention models like BERT, indicating that its
performance gains are not consistent across tasks. In this paper, we first
study a principled layerwise adaptation strategy to accelerate training of deep
neural networks using large mini-batches. Using this strategy, we develop a new
layerwise adaptive large batch optimization technique called LAMB; we then
provide convergence analysis of LAMB as well as LARS, showing convergence to a
stationary point in general nonconvex settings. Our empirical results
demonstrate the superior performance of LAMB across various tasks such as BERT
and ResNet-50 training with very little hyperparameter tuning. In particular,
for BERT training, our optimizer enables use of very large batch sizes of 32868
without any degradation of performance. By increasing the batch size to the
memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to
just 76 minutes (Table 1). The LAMB implementation is available at
https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py","Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py",http://arxiv.org/pdf/1904.00962,"['Yang You', 'Jing Li', 'Sashank Reddi', 'Jonathan Hseu', 'Sanjiv Kumar', 'Srinadh Bhojanapalli', 'Xiaodan Song', 'James Demmel', 'Kurt Keutzer', 'Cho-Jui Hsieh']",128,,,
"ANTICIPATING SAFETY ISSUES IN
E2E C ONVERSATIONAL AI:
FRAMEWORK AND TOOLING
Emily Dinan1, Gavin Abercrombie2, A. Stevie Bergman3",2107.03451,Anticipating Safety Issues in E2E Conversational AI: Framework and Tooling,"Over the last several years, end-to-end neural conversational agents have
vastly improved in their ability to carry a chit-chat conversation with humans.
However, these models are often trained on large datasets from the internet,
and as a result, may learn undesirable behaviors from this data, such as toxic
or otherwise harmful language. Researchers must thus wrestle with the issue of
how and when to release these models. In this paper, we survey the problem
landscape for safety for end-to-end conversational AI and discuss recent and
related work. We highlight tensions between values, potential positive impact
and potential harms, and provide a framework for making decisions about whether
and how to release these models, following the tenets of value-sensitive
design. We additionally provide a suite of tools to enable researchers to make
better-informed decisions about training and releasing end-to-end
conversational AI models.","Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.",http://arxiv.org/pdf/2107.03451,"['Emily Dinan', 'Gavin Abercrombie', 'A. Stevie Bergman', 'Shannon Spruit', 'Dirk Hovy', 'Y-Lan Boureau', 'Verena Rieser']",128,,,
"arXiv:1607.01759v3  [cs.CL]  9 Aug 2016BagofTricks forEfﬁcient TextClassiﬁcation
Armand Joulin Edouard Grave PiotrBojanowski Tom",1607.01759,Bag of Tricks for Efficient Text Classification,"This paper explores a simple and efficient baseline for text classification.
Our experiments show that our fast text classifier fastText is often on par
with deep learning classifiers in terms of accuracy, and many orders of
magnitude faster for training and evaluation. We can train fastText on more
than one billion words in less than ten minutes using a standard multicore~CPU,
and classify half a million sentences among~312K classes in less than a minute.","This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",http://arxiv.org/pdf/1607.01759,"['Armand Joulin', 'Edouard Grave', 'Piotr Bojanowski', 'Tomas Mikolov']",128,,,
"ImageNet32x32, ImageNet16x16 and ImageNet64x64
A D OWNSAMPLED VARIANT OF IMAGE NET AS AN ALTERNATIVE TO THE CIFAR DATASETS
Patry",1707.08819,A Downsampled Variant of ImageNet as an Alternative to the CIFAR datasets,"The original ImageNet dataset is a popular large-scale benchmark for training
Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm
design, architecture search, and hyperparameter tuning) on the original dataset
might be prohibitive, we propose to consider a downsampled version of ImageNet.
In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet,
our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and
ImageNet16$\times$16) contains exactly the same number of classes and images as
ImageNet, with the only difference that the images are downsampled to
32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the
variants, respectively). Experiments on these downsampled variants are
dramatically faster than on the original ImageNet and the characteristics of
the downsampled datasets with respect to optimal hyperparameters appear to
remain similar. The proposed datasets and scripts to reproduce our results are
available at http://image-net.org/download-images and
https://github.com/PatrykChrabaszcz/Imagenet32_Scripts","The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and ImageNet16$\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32_Scripts",http://arxiv.org/pdf/1707.08819,"['Patryk Chrabaszcz', 'Ilya Loshchilov', 'Frank Hutter']",128,,,
"arXiv:2302.07842v1  [cs.CL]  15 Feb 2023Augmented Language Models: a Survey
Grégoire Mialon∗gmialon@meta.com
Roberto Dessì∗†rdes",2302.07842,Augmented Language Models: a Survey,"This survey reviews works in which language models (LMs) are augmented with
reasoning skills and the ability to use tools. The former is defined as
decomposing a potentially complex task into simpler subtasks while the latter
consists in calling external modules such as a code interpreter. LMs can
leverage these augmentations separately or in combination via heuristics, or
learn to do so from demonstrations. While adhering to a standard missing tokens
prediction objective, such augmented LMs can use various, possibly
non-parametric external modules to expand their context processing ability,
thus departing from the pure language modeling paradigm. We therefore refer to
them as Augmented Language Models (ALMs). The missing token objective allows
ALMs to learn to reason, use tools, and even act, while still performing
standard natural language tasks and even outperforming most regular LMs on
several benchmarks. In this work, after reviewing current advance in ALMs, we
conclude that this new research direction has the potential to address common
limitations of traditional LMs such as interpretability, consistency, and
scalability issues.","This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",http://arxiv.org/pdf/2302.07842,"['Grégoire Mialon', 'Roberto Dessì', 'Maria Lomeli', 'Christoforos Nalmpantis', 'Ram Pasunuru', 'Roberta Raileanu', 'Baptiste Rozière', 'Timo Schick', 'Jane Dwivedi-Yu', 'Asli Celikyilmaz', 'Edouard Grave', 'Yann LeCun', 'Thomas Scialom']",128,,,
"mGPT: Few-Shot Learners Go Multilingual
Oleh Shliazhko
SberDevices, Sber
Vladislav Mikhailov
SberDevices, Sber
HSE UniversityAl",2204.0758,mGPT: Few-Shot Learners Go Multilingual,"Recent studies report that autoregressive language models can successfully
solve many NLP tasks via zero- and few-shot learning paradigms, which opens up
new possibilities for using the pre-trained language models. This paper
introduces two autoregressive GPT-like models with 1.3 billion and 13 billion
parameters trained on 60 languages from 25 language families using Wikipedia
and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using
GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron
frameworks allow us to parallelize the training and inference steps
effectively. The resulting models show performance on par with the recently
released XGLM models by Facebook, covering more languages and enhancing NLP
possibilities for low resource languages of CIS countries and Russian small
nations. We detail the motivation for the choices of the architecture design,
thoroughly describe the data preparation pipeline, and train five small
versions of the model to choose the most optimal multilingual tokenization
strategy. We measure the model perplexity in all covered languages and evaluate
it on the wide spectre of multilingual tasks, including classification,
generative, sequence labeling and knowledge probing. The models were evaluated
with the zero-shot and few-shot methods. Furthermore, we compared the
classification tasks with the state-of-the-art multilingual model XGLM. source
code and the mGPT XL model are publicly released.","Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.",http://arxiv.org/pdf/2204.07580,"['Oleh Shliazhko', 'Alena Fenogenova', 'Maria Tikhonova', 'Vladislav Mikhailov', 'Anastasia Kozlova', 'Tatiana Shavrina']",128,,,
"Exploring the Role of Task Transferability in Large-Scale Multi-Task
Learning
Vishakh Padmakumar1Leonard Lausen2Miguel Balleste",2204.11117,Exploring the Role of Task Transferability in Large-Scale Multi-Task Learning,"Recent work has found that multi-task training with a large number of diverse
tasks can uniformly improve downstream performance on unseen target tasks. In
contrast, literature on task transferability has established that the choice of
intermediate tasks can heavily affect downstream task performance. In this
work, we aim to disentangle the effect of scale and relatedness of tasks in
multi-task representation learning. We find that, on average, increasing the
scale of multi-task learning, in terms of the number of tasks, indeed results
in better learned representations than smaller multi-task setups. However, if
the target tasks are known ahead of time, then training on a smaller set of
related tasks is competitive to the large-scale multi-task training at a
reduced computational cost.","Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.",http://arxiv.org/pdf/2204.11117,"['Vishakh Padmakumar', 'Leonard Lausen', 'Miguel Ballesteros', 'Sheng Zha', 'He He', 'George Karypis']",128,,,
"Don’t Say What You Don’t Know: Improving the Consistency of
Abstractive Summarization by Constraining Beam Search
Daniel KingZ",2203.08436,Don't Say What You Don't Know: Improving the Consistency of Abstractive Summarization by Constraining Beam Search,"Abstractive summarization systems today produce fluent and relevant output,
but often ""hallucinate"" statements not supported by the source text. We analyze
the connection between hallucinations and training data, and find evidence that
models hallucinate because they train on target summaries that are unsupported
by the source. Based on our findings, we present PINOCCHIO, a new decoding
method that improves the consistency of a transformer-based abstractive
summarizer by constraining beam search to avoid hallucinations. Given the model
states and outputs at a given step, PINOCCHIO detects likely model
hallucinations based on various measures of attribution to the source text.
PINOCCHIO backtracks to find more consistent output, and can opt to produce no
summary at all when no consistent generation can be found. In experiments, we
find that PINOCCHIO improves the consistency of generation (in terms of F1) by
an average of~67% on two abstractive summarization datasets.","Abstractive summarization systems today produce fluent and relevant output, but often hallucinate statements not supported by the source text. We analyze the connection between hallucinations and training data, and find evidence that models hallucinate because they train on target summaries that are unsupported by the source. Based on our findings, we present PINOCCHIO, a new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations. Given the model states and outputs at a given step, PINOCCHIO detects likely model hallucinations based on various measures of attribution to the source text. PINOCCHIO backtracks to find more consistent output, and can opt to produce no summary at all when no consistent generation can be found. In experiments, we find that PINOCCHIO improves the consistency of generation (in terms of F1) by an average of~67% on two abstractive summarization datasets.",http://arxiv.org/pdf/2203.08436,"['Daniel King', 'Zejiang Shen', 'Nishant Subramani', 'Daniel S. Weld', 'Iz Beltagy', 'Doug Downey']",128,,,
"Annotators with Attitudes:
How Annotator Beliefs And Identities Bias Toxic Language Detection
Maarten Sap~Swabha SwayamdiptaLa",2111.07997,Annotators with Attitudes: How Annotator Beliefs And Identities Bias Toxic Language Detection,"The perceived toxicity of language can vary based on someone's identity and
beliefs, but this variation is often ignored when collecting toxic language
datasets, resulting in dataset and model biases. We seek to understand the who,
why, and what behind biases in toxicity annotations. In two online studies with
demographically and politically diverse participants, we investigate the effect
of annotator identities (who) and beliefs (why), drawing from social psychology
research about hate speech, free speech, racist beliefs, political leaning, and
more. We disentangle what is annotated as toxic by considering posts with three
characteristics: anti-Black language, African American English (AAE) dialect,
and vulgarity. Our results show strong associations between annotator identity
and beliefs and their ratings of toxicity. Notably, more conservative
annotators and those who scored highly on our scale for racist beliefs were
less likely to rate anti-Black language as toxic, but more likely to rate AAE
as toxic. We additionally present a case study illustrating how a popular
toxicity detection system's ratings inherently reflect only specific beliefs
and perspectives. Our findings call for contextualizing toxicity labels in
social variables, which raises immense implications for toxic language
annotation and detection.","The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",http://arxiv.org/pdf/2111.07997,"['Maarten Sap', 'Swabha Swayamdipta', 'Laura Vianna', 'Xuhui Zhou', 'Yejin Choi', 'Noah A. Smith']",128,,,
"arXiv:2006.07235v2  [cs.CL]  30 Sep 2020SemEval-2020 Task 12: Multilingual Offensive Language Ide ntiﬁcation
in Social Media (Of",2006.07235,SemEval-2020 Task 12: Multilingual Offensive Language Identification in Social Media (OffensEval 2020),"We present the results and main findings of SemEval-2020 Task 12 on
Multilingual Offensive Language Identification in Social Media (OffensEval
2020). The task involves three subtasks corresponding to the hierarchical
taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The
task featured five languages: English, Arabic, Danish, Greek, and Turkish for
Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020
was one of the most popular tasks at SemEval-2020 attracting a large number of
participants across all subtasks and also across all languages. A total of 528
teams signed up to participate in the task, 145 teams submitted systems during
the evaluation period, and 70 submitted system description papers.","We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The task involves three subtasks corresponding to the hierarchical taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The task featured five languages: English, Arabic, Danish, Greek, and Turkish for Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020 was one of the most popular tasks at SemEval-2020 attracting a large number of participants across all subtasks and also across all languages. A total of 528 teams signed up to participate in the task, 145 teams submitted systems during the evaluation period, and 70 submitted system description papers.",http://arxiv.org/pdf/2006.07235,"['Marcos Zampieri', 'Preslav Nakov', 'Sara Rosenthal', 'Pepa Atanasova', 'Georgi Karadzhov', 'Hamdy Mubarak', 'Leon Derczynski', 'Zeses Pitenis', 'Çağrı Çöltekin']",128,,,
"MathQA: Towards Interpretable Math Word Problem Solving
with Operation-Based Formalisms
Aida Amini1, Saadia Gabriel1, Shanchuan ",1905.13319,MathQA: Towards Interpretable Math Word Problem Solving with Operation-Based Formalisms,"We introduce a large-scale dataset of math word problems and an interpretable
neural math problem solver that learns to map problems to operation programs.
Due to annotation challenges, current datasets in this domain have been either
relatively small in scale or did not offer precise operational annotations over
diverse problem types. We introduce a new representation language to model
precise operation programs corresponding to each math problem that aim to
improve both the performance and the interpretability of the learned models.
Using this representation language, our new dataset, MathQA, significantly
enhances the AQuA dataset with fully-specified operational programs. We
additionally introduce a neural sequence-to-program model enhanced with
automatic problem categorization. Our experiments show improvements over
competitive baselines in our MathQA as well as the AQuA dataset. The results
are still significantly lower than human performance indicating that the
dataset poses new challenges for future research. Our dataset is available at:
https://math-qa.github.io/math-QA/","We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/",http://arxiv.org/pdf/1905.13319,"['Aida Amini', 'Saadia Gabriel', 'Peter Lin', 'Rik Koncel-Kedziorski', 'Yejin Choi', 'Hannaneh Hajishirzi']",128,,,
"Automatic Detection of Machine Generated Text: A Critical Survey
Ganesh Jawahar, Muhammad Abdul-Mageed, Laks V .S. Lakshmanan
Un",2011.01314,Automatic Detection of Machine Generated Text: A Critical Survey,"Text generative models (TGMs) excel in producing text that matches the style
of human language reasonably well. Such TGMs can be misused by adversaries,
e.g., by automatically generating fake news and fake product reviews that can
look authentic and fool humans. Detectors that can distinguish text generated
by TGM from human written text play a vital role in mitigating such misuse of
TGMs. Recently, there has been a flurry of works from both natural language
processing (NLP) and machine learning (ML) communities to build accurate
detectors for English. Despite the importance of this problem, there is
currently no work that surveys this fast-growing literature and introduces
newcomers to important research challenges. In this work, we fill this void by
providing a critical survey and review of this literature to facilitate a
comprehensive understanding of this problem. We conduct an in-depth error
analysis of the state-of-the-art detector and discuss research directions to
guide future work in this exciting area.","Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",http://arxiv.org/pdf/2011.01314,"['Ganesh Jawahar', 'Muhammad Abdul-Mageed', 'Laks V. S. Lakshmanan']",128,,,
"Know What You Don’t Know: Unanswerable Questions for SQuAD
Pranav RajpurkarRobin JiaPercy Liang
Computer Science Department, S",1806.03822,Know What You Don't Know: Unanswerable Questions for SQuAD,"Extractive reading comprehension systems can often locate the correct answer
to a question in a context document, but they also tend to make unreliable
guesses on questions for which the correct answer is not stated in the context.
Existing datasets either focus exclusively on answerable questions, or use
automatically generated unanswerable questions that are easy to identify. To
address these weaknesses, we present SQuAD 2.0, the latest version of the
Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD
data with over 50,000 unanswerable questions written adversarially by
crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0,
systems must not only answer questions when possible, but also determine when
no answer is supported by the paragraph and abstain from answering. SQuAD 2.0
is a challenging natural language understanding task for existing models: a
strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on
SQuAD 2.0.","Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",http://arxiv.org/pdf/1806.03822,"['Pranav Rajpurkar', 'Robin Jia', 'Percy Liang']",128,,,
"Do Prompt-Based Models Really Understand
the Meaning of Their Prompts?
Albert Webson1,2and Ellie Pavlick1
{albert_webson, ellie_",2109.01247,Do Prompt-Based Models Really Understand the Meaning of their Prompts?,"Recently, a boom of papers has shown extraordinary progress in zero-shot and
few-shot learning with various prompt-based models. It is commonly argued that
prompts help models to learn faster in the same way that humans learn faster
when provided with task instructions expressed in natural language. In this
study, we experiment with over 30 prompt templates manually written for natural
language inference (NLI). We find that models learn just as fast with many
prompts that are intentionally irrelevant or even pathologically misleading as
they do with instructively ""good"" prompts. Further, such patterns hold even for
models as large as 175 billion parameters (Brown et al., 2020) as well as the
recently proposed instruction-tuned models which are trained on hundreds of
prompts (Sanh et al., 2022). That is, instruction-tuned models often produce
good predictions with irrelevant and misleading prompts even at zero shots. In
sum, notwithstanding prompt-based models' impressive improvement, we find
evidence of serious limitations that question the degree to which such
improvement is derived from models understanding task instructions in ways
analogous to humans' use of task instructions.","Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively good prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.",http://arxiv.org/pdf/2109.01247,"['Albert Webson', 'Ellie Pavlick']",128,,,
"SCALING LAWS FOR GENERATIVE MIXED -MODAL
LANGUAGE MODELS
Armen Aghajanyany, Lili Yuy, Alexis Conneauy, Wei-Ning Hsuy
Karen Ham",2301.03728,Scaling Laws for Generative Mixed-Modal Language Models,"Generative language models define distributions over sequences of tokens that
can represent essentially any combination of data modalities (e.g., any
permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens
for language or code, and so on). To better understand the scaling properties
of such mixed-modal models, we conducted over 250 experiments using seven
different modalities and model sizes ranging from 8 million to 30 billion,
trained on 5-100 billion tokens. We report new mixed-modal scaling laws that
unify the contributions of individual modalities and the interactions between
them. Specifically, we explicitly model the optimal synergy and competition due
to data and model size as an additive term to previous uni-modal scaling laws.
We also find four empirical phenomena observed during the training, such as
emergent coordinate-ascent style training that naturally alternates between
modalities, guidelines for selecting critical hyper-parameters, and connections
between mixed-modal competition and training stability. Finally, we test our
scaling law by training a 30B speech-text model, which significantly
outperforms the corresponding unimodal models. Overall, our research provides
valuable insights into the design and training of mixed-modal generative
models, an important new class of unified models that have unique
distributional properties.","Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.",http://arxiv.org/pdf/2301.03728,"['Armen Aghajanyan', 'Lili Yu', 'Alexis Conneau', 'Wei-Ning Hsu', 'Karen Hambardzumyan', 'Susan Zhang', 'Stephen Roller', 'Naman Goyal', 'Omer Levy', 'Luke Zettlemoyer']",128,,,
"Towards Understanding and Mitigating Social Biases in Language Models
Paul Pu Liang1Chiyu Wu1Louis-Philippe Morency1Ruslan Salak",2106.13219,Towards Understanding and Mitigating Social Biases in Language Models,"As machine learning methods are deployed in real-world settings such as
healthcare, legal systems, and social science, it is crucial to recognize how
they shape social biases and stereotypes in these sensitive decision-making
processes. Among such real-world deployments are large-scale pretrained
language models (LMs) that can be potentially dangerous in manifesting
undesirable representational biases - harmful biases resulting from
stereotyping that propagate negative generalizations involving gender, race,
religion, and other social constructs. As a step towards improving the fairness
of LMs, we carefully define several sources of representational biases before
proposing new benchmarks and metrics to measure them. With these tools, we
propose steps towards mitigating social biases during text generation. Our
empirical results and human evaluation demonstrate effectiveness in mitigating
bias while retaining crucial contextual information for high-fidelity text
generation, thereby pushing forward the performance-fairness Pareto frontier.","As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",http://arxiv.org/pdf/2106.13219,"['Paul Pu Liang', 'Chiyu Wu', 'Louis-Philippe Morency', 'Ruslan Salakhutdinov']",128,,,
"LLaMA: Open and Efﬁcient Foundation Language Models
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet
Marie-Anne",2302.13971,LLaMA: Open and Efficient Foundation Language Models,"We introduce LLaMA, a collection of foundation language models ranging from
7B to 65B parameters. We train our models on trillions of tokens, and show that
it is possible to train state-of-the-art models using publicly available
datasets exclusively, without resorting to proprietary and inaccessible
datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks,
and LLaMA-65B is competitive with the best models, Chinchilla-70B and
PaLM-540B. We release all our models to the research community.","We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",http://arxiv.org/pdf/2302.13971,"['Hugo Touvron', 'Thibaut Lavril', 'Gautier Izacard', 'Xavier Martinet', 'Marie-Anne Lachaux', 'Timothée Lacroix', 'Baptiste Rozière', 'Naman Goyal', 'Eric Hambro', 'Faisal Azhar', 'Aurelien Rodriguez', 'Armand Joulin', 'Edouard Grave', 'Guillaume Lample']",128,,,
"Enhancing the Transformer With Explicit Relational
Encoding for Math Problem Solving
Imanol Schlag1, Paul Smolensky2;3, Roland ",1910.06611,Enhancing the Transformer with Explicit Relational Encoding for Math Problem Solving,"We incorporate Tensor-Product Representations within the Transformer in order
to better support the explicit representation of relation structure. Our
Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the
recently-introduced Mathematics Dataset containing 56 categories of free-form
math word-problems. The essential component of the model is a novel attention
mechanism, called TP-Attention, which explicitly encodes the relations between
each Transformer cell and the other cells from which values have been retrieved
by attention. TP-Attention goes beyond linear combination of retrieved values,
strengthening representation-building and resolving ambiguities introduced by
multiple layers of standard attention. The TP-Transformer's attention maps give
better insights into how it is capable of solving the Mathematics Dataset's
challenging problems. Pretrained models and code will be made available after
publication.","We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.",http://arxiv.org/pdf/1910.06611,"['Imanol Schlag', 'Paul Smolensky', 'Roland Fernandez', 'Nebojsa Jojic', 'Jürgen Schmidhuber', 'Jianfeng Gao']",128,,,
"Continual Lifelong Learning with Neural Networks:
A Review
German I. Parisi1, Ronald Kemker2, Jose L. Part3, Christopher Kanan2,",1802.07569,Continual Lifelong Learning with Neural Networks: A Review,"Humans and animals have the ability to continually acquire, fine-tune, and
transfer knowledge and skills throughout their lifespan. This ability, referred
to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms
that together contribute to the development and specialization of our
sensorimotor skills as well as to long-term memory consolidation and retrieval.
Consequently, lifelong learning capabilities are crucial for autonomous agents
interacting in the real world and processing continuous streams of information.
However, lifelong learning remains a long-standing challenge for machine
learning and neural network models since the continual acquisition of
incrementally available information from non-stationary data distributions
generally leads to catastrophic forgetting or interference. This limitation
represents a major drawback for state-of-the-art deep neural network models
that typically learn representations from stationary batches of training data,
thus without accounting for situations in which information becomes
incrementally available over time. In this review, we critically summarize the
main challenges linked to lifelong learning for artificial learning systems and
compare existing neural network approaches that alleviate, to different
extents, catastrophic forgetting. We discuss well-established and emerging
research motivated by lifelong learning factors in biological systems such as
structural plasticity, memory replay, curriculum and transfer learning,
intrinsic motivation, and multisensory integration.","Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",http://arxiv.org/pdf/1802.07569,"['German I. Parisi', 'Ronald Kemker', 'Jose L. Part', 'Christopher Kanan', 'Stefan Wermter']",128,,,
"Detecting Emergent Intersectional Biases:
Contextualized Word Embeddings Contain a Distribution of Human-like Biases
Wei Guo1and",2006.03955,Detecting Emergent Intersectional Biases: Contextualized Word Embeddings Contain a Distribution of Human-like Biases,"With the starting point that implicit human biases are reflected in the
statistical regularities of language, it is possible to measure biases in
English static word embeddings. State-of-the-art neural language models
generate dynamic word embeddings dependent on the context in which the word
appears. Current methods measure pre-defined social and intersectional biases
that appear in particular contexts defined by sentence templates. Dispensing
with templates, we introduce the Contextualized Embedding Association Test
(CEAT), that can summarize the magnitude of overall bias in neural language
models by incorporating a random-effects model. Experiments on social and
intersectional biases show that CEAT finds evidence of all tested biases and
provides comprehensive information on the variance of effect magnitudes of the
same bias in different contexts. All the models trained on English corpora that
we study contain biased representations.
  Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and
Emergent Intersectional Bias Detection (EIBD), to automatically identify the
intersectional biases and emergent intersectional biases from static word
embeddings in addition to measuring them in contextualized word embeddings. We
present the first algorithmic bias detection findings on how intersectional
group members are strongly associated with unique emergent biases that do not
overlap with the biases of their constituent minority identities. IBD and EIBD
achieve high accuracy when detecting the intersectional and emergent biases of
African American females and Mexican American females. Our results indicate
that biases at the intersection of race and gender associated with members of
multiple minority groups, such as African American females and Mexican American
females, have the highest magnitude across all neural language models.","With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations.   Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",http://arxiv.org/pdf/2006.03955,"['Wei Guo', 'Aylin Caliskan']",128,,,
"Teaching Small Language Models to Reason
Lucie Charlotte Magister∗
University of Cambridge
lcm67@cam.ac.ukJonathan Mallinson
Goo",2212.0841,Teaching Small Language Models to Reason,"Chain of thought prompting successfully improves the reasoning capabilities
of large language models, achieving state of the art results on a range of
datasets. However, these reasoning capabilities only appear to emerge in models
with a size of over 100 billion parameters. In this paper, we explore the
transfer of such reasoning capabilities to models with less than 100 billion
parameters via knowledge distillation. Specifically, we finetune a student
model on the chain of thought outputs generated by a larger teacher model. Our
experiments show that the proposed method improves task performance across
arithmetic, commonsense and symbolic reasoning datasets. For example, the
accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on
PaLM-540B generated chains of thought.","Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.",http://arxiv.org/pdf/2212.08410,"['Lucie Charlotte Magister', 'Jonathan Mallinson', 'Jakub Adamek', 'Eric Malmi', 'Aliaksei Severyn']",128,,,
"COMMONSENSE QA: A Question Answering Challenge Targeting
Commonsense Knowledge
Alon Talmor;1;2Jonathan Herzig;1Nicholas Louri",1811.00937,CommonsenseQA: A Question Answering Challenge Targeting Commonsense Knowledge,"When answering a question, people often draw upon their rich world knowledge
in addition to the particular context. Recent work has focused primarily on
answering questions given some relevant document or context, and required very
little general background. To investigate question answering with prior
knowledge, we present CommonsenseQA: a challenging new dataset for commonsense
question answering. To capture common sense beyond associations, we extract
from ConceptNet (Speer et al., 2017) multiple target concepts that have the
same semantic relation to a single source concept. Crowd-workers are asked to
author multiple-choice questions that mention the source concept and
discriminate in turn between each of the target concepts. This encourages
workers to create questions with complex semantics that often require prior
knowledge. We create 12,247 questions through this procedure and demonstrate
the difficulty of our task with a large number of strong baselines. Our best
baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy,
well below human performance, which is 89%.","When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",http://arxiv.org/pdf/1811.00937,"['Alon Talmor', 'Jonathan Herzig', 'Nicholas Lourie', 'Jonathan Berant']",128,,,
