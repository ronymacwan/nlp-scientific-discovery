summary,keywords_using_BERT,keywords_using_togetherai,keywords_human_annotated
"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.",transfer learning techniques | pre-trained models | knowledge transfer methods | distilbert compression | model optimization | language model applications | on-device computing | bert integration,transfer learning | pre-trained models | knowledge distillation | distilbert | model compression | language modeling | on-device computation | BERT,transfer learning | pre-trained models | distilbert | knowledge distillation | language modeling | on-device computation
"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement.",reading comprehension models | text interpretation | answering questions | multi-hop reasoning tasks | evidence aggregation | data creation for qa | query-answer matching | information retrieval systems,reading comprehension | text understanding | question-answering | multi-hop reasoning | evidence synthesis | dataset generation | query-answer mapping | information retrieval,reading comprehension | multi-hop inference | text understanding | query-answer pairs | evidence integration | dataset creation
"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.",policy gradient methods | reinforcement learning strategies | ppo optimization | sample efficiency improvement | stochastic gradient methods | robotic task control | policy optimization algorithms | navigation tasks for robots,policy gradient | reinforcement learning | PPO | sample efficiency | stochastic optimization | robotic control | policy optimization | robot navigation,policy gradient | reinforcement learning | proximal policy optimization | sample complexity | stochastic gradient | robotic locomotion
"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.",automatic text editing | neural machine translation models | human-assisted edits | post-editing systems | nmt applications | corpus creation for mt | machine translation improvements | translation model refinement,automatic post-editing | neural machine translation | human edits | NMT | post-editing models | corpus creation | machine translation | translation models,automatic post-editing | neural machine translation | human post-edits | ape models | nmt | corpus compilation
"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).",human-human dialogue | multi-issue negotiation | end-to-end negotiation models | dialogue system technologies | natural language reasoning | negotiation strategy development | conversational ai for negotiations,human-human negotiation | multi-issue bargaining | end-to-end models | dialogue systems | natural language dialogue | reasoning abilities | negotiation strategies | conversational AI,human-human negotiation | end-to-end models | dialogue rollouts | multi-issue bargaining | natural language dialogue | reasoning skills
"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.",books-to-movies transformation | neural sentence embeddings | video-text fusion | context-aware models | movie adaptations from books | visual reasoning in content | multimodal embeddings | visual content analysis techniques,books to movies | neural sentence embedding | video-text alignment | context-aware models | movie adaptation | visual reasoning | multimodal embedding | visual content analysis,books to movies | neural sentence embedding | video-text embedding | context-aware cnn | movie/book alignment | visual content explanation
"Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.",multimodal contrastive learning | clip model | backdoor vulnerability attacks | poisoning in models | adversarial threats | uncurated datasets | image classification tasks | model robustness improvements,multimodal contrastive learning | CLIP | backdoor attacks | poisoning attacks | adversarial attacks | uncurated data | image classification | model robustness,multimodal contrastive learning | clip | backdoor attacks | poisoning attacks | uncurated datasets | image misclassification
"In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.",hate speech detection | african american vernacular english | bias reduction techniques | adversarial training methods | false positive reduction | toxicity detection models | discrimination bias correction | offensive language identification,hate speech | african american english | bias mitigation | adversarial training | false positives | toxicity detection | discrimination bias | offensive language,hate speech | african american english | bias mitigation | adversarial training | false positive rate | toxicity detection
"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models out-perform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks",open-domain question answering | model generalization techniques | test set evaluation methods | question generation processes | benchmark dataset creation | performance metric development | cross-domain question answering,open-domain QA | model generalization | test set evaluation | question generation | benchmark datasets | performance metrics | question answering systems | cross-domain evaluation,open-domain qa | model generalization | test set analysis | question formulation | benchmark datasets | performance evaluation
"Increasing interest in privacy-preserving machine learning has led to new and evolved approaches for generating private synthetic data from undisclosed real data. However, mechanisms of privacy preservation can significantly reduce the utility of synthetic data, which in turn impacts downstream tasks such as learning predictive models or inference. We propose several re-weighting strategies using privatised likelihood ratios that not only mitigate statistical bias of downstream estimators but also have general applicability to differentially private generative models. Through large-scale empirical evaluation, we show that private importance weighting provides simple and effective privacy-compliant augmentation for general applications of synthetic data.",privacy-preserving machine learning | synthetic datasets | data privacy | likelihood metrics | differential privacy techniques | generative adversarial networks | anonymizing data | privacy safeguards,privacy-preserving ML | synthetic data | privacy protection | likelihood ratios | differential privacy | generative adversarial models | data anonymization | privacy models,privacy-preserving machine learning | synthetic data | privacy preservation | likelihood ratios | differential privacy | generative models
"Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.",hate speech detection | annotation reliability | social media monitoring | harmful content identification | user feedback integration | content moderation systems | dataset biases | definition consistency,hate speech | annotation consistency | social media platforms | offensive content | user feedback | content moderation | bias in datasets | definition alignment,hate speech | annotation reliability | social media | offensive content | user survey | definition alignment
"The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.",large-scale language models | text classification | log-likelihood metrics | detectgpt tool | model bias detection | fake news recognition | misinformation filtering | content validation,large language models | text detection | log likelihood | detectGPT | model bias | fake news identification | misinformation detection | content authenticity,large language models | text detection | log probability | detectgpt | model discrimination | fake news detection
"The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.",natural language understanding | few-shot tasks | reasoning with chain of thought | counterfactual analysis | large models | task-specific adaptations | prompt optimization | model fine-tuning,natural language processing | few-shot learning | chain of thought reasoning | counterfactual reasoning | large language models | task-specific models | prompt engineering | model adaptation,natural language processing | few-shot learning | chain of thought | counterfactual prompting | large language models | task understanding
"Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.",variational inference techniques | partitioned Bayesian methods | federated machine learning | lifelong learning | Bayesian networks for prediction | Gaussian process models | probabilistic approaches | model uncertainty estimation,variational inference | partitioned variational methods | federated learning | continual learning | Bayesian networks | Gaussian processes | probabilistic models | model uncertainty,variational inference | partitioned variational inference | federated learning | continual learning | Bayesian neural networks | Gaussian process models
"Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.",cross-lingual pretraining methods | unsupervised machine translation | multilingual NLP models | XLM techniques | supervised translation tasks | state-of-the-art methods | cross-lingual capabilities | multilingual model development,cross-lingual pretraining | generative pretraining | XLM | unsupervised machine translation | supervised translation | multilingual models | state-of-the-art NLP | cross-lingual learning,cross-lingual pretraining | generative pretraining | XLM | unsupervised machine translation | supervised machine translation | state-of-the-art results
"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.",lamda models | transformer-based architectures | conversational agents | model scalability techniques | factual grounding in AI | safety measures in AI | external knowledge integration | dialogue-based systems,Lamda | transformer models | dialogue systems | model scalability | factual grounding | safety in AI | external knowledge retrieval | conversational AI,lamda | transformer-based models | dialog applications | model scaling | safety and factual grounding | external knowledge sources
"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.",external knowledge usage | chain-of-thought prompting methods | retrieval-based approaches | post-processing for reasoning | commonsense reasoning tasks | temporal inference | knowledge-driven reasoning | logical deduction,external knowledge | chain-of-thought prompting | retrieval methods | post-processing techniques | commonsense reasoning | temporal reasoning | knowledge-based reasoning | logical inference,external knowledge | chain-of-thought prompting | retrieval-based methods | post-processing approach | commonsense reasoning | temporal reasoning
"Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT",large models for ranking | information retrieval systems | gpt-4 models | zero-shot reasoning | fine-tuned models | retrieval-augmented systems | question answering frameworks | information extraction techniques,large language models | relevance ranking | information retrieval | GPT-4 | zero-shot models | fine-tuning | retrieval-based models | question answering,large language models | relevance ranking | information retrieval | GPT-4 | zero-shot learning | fine-tuning
"In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.",yes/no queries | comprehension tasks | transfer learning applications | entailment inference | bert models | multinli dataset | text categorization | sentence relationships,yes/no questions | reading comprehension | transfer learning | entailment inference | BERT | multiNLI | text classification | sentence entailment,yes/no questions | reading comprehension | transfer learning | entailment inference | BERT | multiNLI
"In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent's abilities in text games -- challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules.",interactive agents | symbolic reasoning tasks | multi-step problem solving | behavior imitation | text-based gaming | action prediction | game-playing agents | logical tasks,interactive agents | symbolic reasoning | multi-step reasoning | behavior cloning | text-based games | action generation | game agents | reasoning tasks,interactive agents | symbolic modules | multi-step reasoning | behavior cloning | text games | action injection
"Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (Birds cannot [MASK]) and non-negated (Birds can [MASK]) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add misprimes to cloze questions (Talk? Birds can [MASK]). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.",factual knowledge retrieval | pre-trained models | probing challenges | handling negation | mispriming effects in NLP | cloze-type tasks | knowledge extraction | language interpretation,factual knowledge | pretrained language models | probing tasks | negation handling | mispriming effects | cloze questions | knowledge probing | language understanding,factual knowledge | pretrained language models | probing tasks | negation | mispriming | cloze questions
"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).",chain-of-thought prompting | self-consistency verification | large models | reasoning processes | greedy decoding techniques | arithmetic logic | reasoning strategies | inference generation,chain-of-thought prompting | self-consistency | large language models | reasoning tasks | greedy decoding | arithmetic reasoning | reasoning strategies | model inference,chain-of-thought prompting | self-consistency | large language models | reasoning tasks | greedy decoding | arithmetic reasoning
"We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?   To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.",reinforcement learning systems | language models | human-in-the-loop feedback | rl4lms tasks | grue benchmark | policy optimization techniques | huggingface platforms | preference-based learning,reinforcement learning | language models | human feedback | RL4LMs | GRUE benchmark | policy optimization | HuggingFace | preference learning,reinforcement learning | language models | human preferences | RL4LMs | GRUE benchmark | policy optimization | NLPO | HuggingFace
"Neural network training relies on our ability to find good minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple filter normalization method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.",neural network training | loss function adjustments | hyperparameter optimization | generalization across tasks | optimization techniques | loss landscape exploration | model structures | normalization filters,neural networks | loss function | training parameters | generalization | optimization algorithms | loss landscape | model architecture | filter normalization,neural networks | loss function | training parameters | generalization | optimization | loss landscape | architecture design | filter normalization | visualization methods
"The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present CONDAQA, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage -- paraphrasing the negated statement, changing the scope of the negation, and reversing the negation -- resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our consistency metric, well below human performance which is 81%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future NLP methods that work on negated language.",negation handling | reading comprehension challenges | natural language interpretation | dataset creation | question-answer generation | logical reasoning tasks | paraphrasing techniques | scope of negation,negation | reading comprehension | natural language understanding | dataset generation | question-answer pairs | logical reasoning | paraphrasing | negation scope,negation | reading comprehension | natural language understanding | dataset | question-answer pairs | reasoning | paraphrasing | scope of negation | negated statements | consistency metric | unifiedqa | nlp models | few-shot learning
"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.",chain-of-thought reasoning | large models for reasoning | advanced prompting strategies | arithmetic problem solving | commonsense deduction | symbolic logic tasks | math challenge solving,chain of thought | large language models | reasoning tasks | prompting techniques | arithmetic reasoning | commonsense reasoning | symbolic reasoning | math problem solving,chain of thought | large language models | reasoning | prompting | arithmetic reasoning | commonsense reasoning | symbolic reasoning | GPT-3 | GSM8K benchmark | math word problems | model performance | intermediate reasoning steps | prompting strategy
"We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.",androidenv simulation | reinforcement learning agents | rl agents in android | touchscreen-based interfaces | simulated environments | open-source agent platforms | agent evaluation metrics,androidenv | reinforcement learning | RL agents | android ecosystem | touchscreen interface | simulation environments | open-source platforms | agent evaluation,androidenv | reinforcement learning | rl agents | android ecosystem | touchscreen interface | simulation | open-source platform | app interaction | research tools | rl agent evaluation | empirical evaluation | task performance | real device deployment
"Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing the model to learn incorrect inputs to serve their malicious needs.   In this paper, we automatically generate fake CTI text descriptions using transformers. We show that given an initial prompt sentence, a public language model like GPT-2 with fine-tuning, can generate plausible CTI text with the ability of corrupting cyber-defense systems. We utilize the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The poisoning attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cybersecurity professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI as true.",cybersecurity systems | threat intelligence | cyber threat analysis | knowledge representations | data contamination | public intelligence sources | open-source data | fabricated threat data generation,cyber-defense systems | cyber threat intelligence | ctI | knowledge graphs | data poisoning | open-source intelligence | osint | fake ctI generation,cyber-defense systems | cyber threat intelligence | ctI | knowledge graphs | data poisoning | open-source intelligence | osint | fake ctI generation | transformers | gpt-2 | fine-tuning | cybersecurity knowledge graph | ckg | poisoning attack | reasoning outputs | representation poisoning | ai-based cyber defense systems | human evaluation | threat hunters
"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",large neural models | arithmetic problem-solving | symbolic reasoning processes | few-shot instruction techniques | program-enhanced models | pal frameworks | python execution engine | symbolic neural networks,large language models | arithmetic reasoning | symbolic reasoning | few-shot prompting | program-aided models | PAL | Python interpreter | neural-symbolic models,large language models | lLms | arithmetic reasoning | symbolic reasoning | few-shot prompting | chain-of-thought | program-aided language models | pal | python interpreter | neural-symbolic synergy | decomposition | reasoning tasks | big-bench hard | code generation | gsm8k benchmark | codex | paLm-540b | few-shot accuracy
"Large instruction-tuned language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.",instruction-optimized models | task-following capabilities | model fine-tuning | pre-trained neural networks | gpt-3 architecture | task instruction generation | domain-specific training | model alignment | human-in-the-loop evaluation | self-guided instruction | task tuning | zero-shot task transfer,large instruction-tuned models | instruction-following tasks | fine-tuning | pretrained language models | GPT-3 | instruction generation | task-specific training | alignment | human evaluation | self-instruct | instruction tuning | zero-shot generalization,large instruction-tuned language models | finetuning | instruction-following | self-instruct | pretrained language models | gpt-3 | instruction generation | super-naturalinstructions | instructgpt-001 | zero-shot generalization | synthetic dataset | alignment | task-specific tuning | annotation-free method | human evaluation | instruction tuning
"Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works' taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different mitigation strategies' motivations, their limitations, and open problems for future research.",language systems | equity | safety protocols | harm reduction | model safety concerns | risk identification | societal damage | harm mitigation strategies | language research challenges | ethical issues | fairness measures,language models | fairness | safety | harm mitigation | model risks | risk detection | societal harms | safety measures | language model research | open problems | model fairness | mitigation strategies,language models | societal harms | language generation risks | fairness | safety | harm mitigation | risk detection | practical guide | language model research | mitigation strategies | open problems | model fairness | societal impact | language model safety
"We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of ~40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes ~15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use. Accepted at ACL 2021.",hate speech identification | human-in-the-loop systems | robust detection models | model adaptation | dynamic dataset generation | data alteration | annotation protocols | real-time hate speech detection | data creation | detailed labels | hate check systems | detection model accuracy,hate speech detection | human-in-the-loop models | robust models | model training | dynamic data creation | data perturbations | annotation guidelines | online hate detection | dataset generation | fine-grained labels | HateCheck | model performance,hate detection | human-in-the-loop | dataset generation | model training | robust models | hate speech | data perturbations | fine-grained labels | dynamic data creation | annotation guidelines | HateCheck | test sets | model performance | online hate detection
"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.",longformer model | transformer architectures | attention networks | extended sequences | sequence handling | language processing | pretrained architectures | task-specific applications | roberta variations | document summarization | large document processing | led approach,Longformer | transformer models | attention mechanisms | long sequences | sequence length | language modeling | pretrained models | downstream tasks | RoBERTa | arXiv summarization | document processing | LED,Longformer | transformer models | attention mechanism | long sequences | sequence length | language modeling | text8 | enwik8 | RoBERTa | pretrained models | downstream tasks | WikiHop | TriviaQA | LED | generative tasks | arXiv summarization | document processing
"Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.",large neural models | lifelong learning | knowledge preservation | forgetting avoidance | parameter enlargement | knowledge update mechanisms | world knowledge integration | continuous learning processes | knowledge degradation | benchmarking tasks | learning acquisition,large language models | continual learning | knowledge retention | catastrophic forgetting | parameter expansion | knowledge update | world knowledge | continual knowledge learning | knowledge forgetting | benchmark tasks | knowledge acquisition,Large Language Models (LMs) | Knowledge retention | Continual Learning (CL) | Continual Knowledge Learning (CKL) | Catastrophic forgetting | Knowledge update | Benchmark | Parameter expansion | World knowledge | Knowledge acquisition | Evaluation scripts | Baselines | Knowledge forgetting
"How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as polite refusal score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user's perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.",conversational systems | verbal abuse recognition | abusive behavior response | user response strategies | polite rejection | demographic variables | data-driven approaches | rule-based models | commercial AI systems | abuse severity evaluation | response assessment,conversational agents | verbal abuse detection | abuse response strategies | user perception | polite refusal | demographic factors | data-driven models | rule-based systems | commercial systems | abuse severity | abuse response evaluation,Conversational agents | Verbal abuse | Response strategies | Polite refusal | Demographic factors | User perception | Data-driven models | Rule-based systems | Commercial systems | Abuse severity | Abuse response evaluation
"We take a step towards addressing the under-representation of the African continent in NLP research by creating the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages, bringing together a variety of stakeholders. We detail characteristics of the languages to help researchers understand the challenges that these languages pose for NER. We analyze our datasets and conduct an extensive empirical evaluation of state-of-the-art methods across both supervised and transfer learning settings. We release the data, code, and models in order to inspire future research on African NLP.",african dialects | named entity identification | natural language processing for african languages | supervised learning techniques | dataset generation | transferability of models | high-quality corpus creation | ner dataset publication | african language research | empirical testing | linguistic hurdles,African languages | Named Entity Recognition (NER) | NLP for African languages | supervised learning | data set creation | transfer learning | high-quality datasets | NER dataset release | African NLP research | empirical evaluation | language challenges,African languages | NLP | Named Entity Recognition (NER) | Data set creation | High-quality dataset | Empirical evaluation | Supervised learning | Transfer learning | Language challenges | African NLP research | Stakeholders | NER dataset release
"In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.",deep learning networks | batch sizes | gradient noise level | data utilization | parallel processing | large-scale batches | domain-specific networks | reinforcement strategies | adaptive batch adjustment | efficient training | time vs. compute trade-offs,deep learning models | batch size | gradient noise scale | data efficiency | parallelism | large batch sizes | domain-specific models | reinforcement learning | adaptive batch size | training efficiency | time-efficiency vs. compute-efficiency,Deep learning models | Batch size | Data efficiency | Gradient noise scale | Large batch sizes | Domain-specific limits | Parallelism | Supervised learning | Reinforcement learning | Adaptive batch size | Time-efficiency vs. compute-efficiency | Training theory
"General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.",general-purpose models | multilingual features | gpt architectures | t5 network | cross-lingual tasks | non-english predictions | few-shot examples | contextual learning | language model assessment | multilingual nlp challenges | benchmark analysis,general-purpose language models | multilingual capabilities | GPT model | T5 model | cross-lingual prediction | non-English languages | few-shot learning | in-context learning | language model evaluation | multilingual NLP tasks | benchmark comparison,General-purpose language models | Multilingual capabilities | GPT model | T5 model | Cross-lingual prediction | Few-shot learning | Non-English languages | In-context learning | Language model evaluation | Multilingual NLP tasks | Benchmark comparison
"Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train open-book QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.",large models | fact validation | hallucination issues | open-ended qa | human feedback reinforcement | evidence support | answer-based citations | gophercite approach | model reliability | evaluation metrics,large language models | fact-checking | hallucination | open-book question answering | reinforcement learning from human preferences | supporting evidence | evidence-based answers | citation-based QA | GopherCite | Trustworthiness | Human evaluation,Large language models | Fact-checking | Hallucination | Reinforcement learning from human preferences (RLHP) | Open-book question answering (QA) | Supporting evidence | GopherCite | Evidence-based answers | NaturalQuestions dataset | ELI5 dataset | Trustworthiness | Citation-based QA | Human evaluation
"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \url{https://github.com/EleutherAI/pythia}.",pythia models | network scaling | model training dynamics | memorization behavior | few-shot effectiveness | open-access systems | model development | training data sources | llm research focus | benchmark results,Pythia | large language models | training dynamics | model scaling | memorization | few-shot performance | open access | model evolution | training data | LLM research | benchmark performance,Pythia | Large Language Models | Training dynamics | Model scaling | Memorization | Gender bias | Few-shot performance | Open access | 154 checkpoints | Training data | LLM research | Model evolution | Benchmark | URL: https://github.com/EleutherAI/pythia
"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.",abstractive summarization | novel fiction | human input | recursive processing | gpt-3 usage | reward-based learning | behavior cloning models | book content summaries | narrativeqa dataset | zero-shot question answering | booksum performance,abstractive summarization | fiction novels | human feedback | recursive decomposition | GPT-3 | reward modeling | behavioral cloning | book summaries | NarrativeQA | zero-shot QA | BookSum dataset | state-of-the-art performance,Abstractive summarization | Fiction novels | Human feedback | Recursive task decomposition | GPT-3 | Behavioral cloning | Reward modeling | Book summaries | NarrativeQA | Zero-shot QA | BookSum dataset | Model performance | State-of-the-art
"The intriguing phenomenon of adversarial examples has attracted significant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. With the focus on UAP against deep classifiers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. We aim to extend this work as a dynamic survey that will regularly update its content to follow new works regarding UAP or universal attack in a wide range of domains, such as image, audio, video, text, etc. Relevant updates will be discussed at: https://bit.ly/2SbQlLG. We welcome authors of future works in this field to contact us for including your new finding.","adversarial attacks | deep classification models | universal perturbations | defense strategies | uap research | multi-domain attacks | adversarial perturbations in text, image, audio | machine learning defense techniques | adversarial example challenges","adversarial examples | deep classifiers | universal adversarial perturbations | attack & defense | UAP challenges | multi-domain attacks | image, audio, video, text adversarial examples | UAP in machine learning | research updates",Adversarial examples | Universal adversarial perturbations | Deep classifiers | Attack & defense | UAP challenges | Dynamic survey | Multi-domain | Image | Audio | Video | Text | UAP in ML | Research updates
"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.",mixture-of-experts models | efficient scaling techniques | autoregressive networks | dense architectures | computational efficiency | language processing models | zero-shot priming methods | model fine-tuning | scaling mechanisms | compute-equivalent approaches,mixture of experts | efficient scaling | autoregressive MoE models | dense models | compute efficiency | language modeling | zero-shot priming | fine-tuning | model scaling | compute-equivalent models | performance gap | public models,Mixture of Experts | Efficient scaling | Autoregressive MoE models | Dense models | Compute efficiency | Language modeling | Zero-shot priming | Fine-tuning | Model scaling | Compute-equivalent | Performance gap | Public models
"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.",asynchronous updates | deep reinforcement learning techniques | actor-critic systems | atari games domain | motor control tasks | maze navigation | cpu vs gpu performance | reinforcement algorithms,asynchronous gradient descent | deep reinforcement learning | actor-critic methods | Atari domain | continuous motor control | 3D maze navigation | multi-core CPU vs GPU | reinforcement learning algorithms,Asynchronous gradient descent | Deep reinforcement learning | Actor-critic | Atari domain | Continuous motor control | 3D maze navigation | Multi-core CPU | GPU comparison | Reinforcement learning algorithms
"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.",truthful models | question answering systems | false response generation | gpt-3 architecture | gpt-neo models | t5 network | model fine-tuning | truthfulness in models | human vs model response | misconceptions handling,truthful language models | question answering | false answers | GPT-3 | GPT-Neo | T5 | fine-tuning | human vs. model truthfulness | model performance | misconceptions | imitation learning,Truthful language models | Benchmark | Misconceptions | Question answering | False answers | Model performance | Human vs. model truthfulness | GPT-3 | GPT-Neo | GPT-2 | T5 | Fine-tuning | Imitation learning
"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.",bloom model | open-source llm | multi-task training | responsible ai | model public release | roots corpus | competitive performance levels | democratizing language models | 176b parameters | wide range of benchmarks | code sharing,BLOOM | open-access LLM | multitask fine-tuning | responsible AI | model release | ROOTS corpus | competitive performance | language model democratization | 176B parameters | wide benchmarks | code release,BLOOM | Open-access LLM | 176B parameters | ROOTS corpus | Multitask finetuning | Competitive performance | Wide benchmarks | Responsible AI | Model release | Code release | Language model democratization
"Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.",machine translation systems | gender bias in translation | mitigating bias in machine translation | translation technologies | framework design | theoretical perspectives | bias evaluation | future research areas,machine translation | gender bias in MT | bias mitigation in translation | translation technology | framework | theoretical insights | bias assessment | future research directions,Machine translation | Gender bias | Bias in MT | Framework | Theoretical insights | Mitigation strategies | Future research directions | Bias assessment | Translation technology
"The probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the inner layers of the neural network, using external classifiers and statistical analysis. Pre-trained transformer-based language models are widely used both for natural language understanding (NLU) and natural language generation (NLG) tasks making them most commonly used for downstream applications. However, little analysis was carried out, whether the models were pre-trained enough or contained knowledge correlated with linguistic theory. We are presenting the chronological probing study of transformer English models such as MultiBERT and T5. We sequentially compare the information about the language learned by the models in the process of training on corpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy. We also introduce the open-source framework for chronological probing research, compatible with other transformer-based models. https://github.com/EkaterinaVoloshina/chronological_probing",probing approaches | linguistic features | transformer networks | pre-trained networks | multibert model | t5 architecture | neural network depth | morphology analysis | syntactic structures | discourse features | open-source tools | chronological analysis,probing methodology | linguistic phenomena | transformer models | pre-trained models | MultiBERT | T5 | neural network layers | morphology | syntax | discourse | open-source framework | chronological probing,Probing methodology | Linguistic phenomena | Neural network layers | Pre-trained models | Transformer models | MultiBERT | T5 | Morphology | Syntax | Discourse | Chronological probing | Linguistic theory | Open-source framework
"The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLMs' ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.",generative models | data enhancement | context creation | model fine-tuning | question-answering datasets | synthetic information | few-shot efficiency | zero-shot capability | classification challenges | abstractive question answering | common sense reasoning | high-level problem solving,generative language models | data augmentation | context generation | fine-tuning | QA datasets | synthetic data | few-shot performance | zero-shot performance | classification tasks | abstractive QA | common-sense QA | high-level reasoning,Generative language models | Data augmentation | CONDA | Context generation | QA datasets | Fine-tuning | Synthetic data | Few-shot performance | Zero-shot performance | Classification tasks | High-level reasoning | Abstractive QA | Common-sense QA
"Dialogue systems in the form of chatbots and personal assistants are being increasingly integrated into people's lives. Modern dialogue systems may consider adopting anthropomorphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the first large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and genders. We define persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we find that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment. We also analyze how personas can result in different amounts of harm towards specific demographics.",persona-related biases | conversational agents | chatbots interactions | social status bias | racial bias influence | gender-related bias | anthropomorphic chatbot personas | harmful replies | bias reduction | blender framework | dialogpt framework | unitpersonabias model,persona biases | dialogue systems | chatbots | social class | racial biases | gender biases | anthropomorphic personas | harmful responses | bias mitigation | Blender | DialoGPT | open-source framework | UnitPersonaBias,Persona biases | Dialogue systems | Chatbots | Anthropomorphic personas | Social class | Sexual orientation | Racial biases | Gender biases | Blender | DialoGPT | Harmful responses | Bias mitigation | Open-source framework | UnitPersonaBias
"We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.",milabot project | deep reinforcement learning models | chatbot systems | amazon alexa prize challenge | casual conversation | speech and text processing | natural language generation | retrieval-based systems | sequence-to-sequence network models | ab testing | user feedback | performance assessment,MILABOT | deep reinforcement learning | chatbot | Amazon Alexa Prize | small talk | speech and text | natural language generation | retrieval models | sequence-to-sequence models | A/B testing | user interactions | performance evaluation,MILABOT | Deep reinforcement learning | Chatbot | Amazon Alexa Prize | Small talk | Speech and text | Natural language generation | Retrieval models | Sequence-to-sequence models | A/B testing | User interactions | Performance evaluation
"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.",alphacode model | programming code generation | competitive coding | algorithm comprehension | transformer-based models | problem-solving techniques | codeforces competitions | model generation | reasoning skills | programming contests | performance evaluation,AlphaCode | code generation | competitive programming | algorithm understanding | transformer-based architectures | problem-solving | Codeforces | model sampling | reasoning capabilities | programming competitions | evaluation,AlphaCode | Code generation | Competitive programming | Problem-solving | Algorithm understanding | Language models | Transformer-based architectures | Codeforces | Model sampling | Evaluation | Reasoning capabilities | Top ranking | Programming competitions
"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.",model dimensions | transformer networks | compute-optimized training | chinchilla model | gopher model | model scaling techniques | training data size | downstream applications | mmlu benchmarking | fine-tuning approaches | cutting-edge accuracy | model effectiveness,model size | transformer models | compute-optimal training | Chinchilla | Gopher | model scaling | training tokens | downstream tasks | MMLU benchmark | fine-tuning | state-of-the-art accuracy | model performance,Model size | Transformer language models | Compute-optimal training | Chinchilla | Gopher | Scaling models | Model performance | Training tokens | Downstream tasks | Fine-tuning | MMLU benchmark | State-of-the-art accuracy
"Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.",safety in conversational models | human-in-the-loop frameworks | generative networks | offensive behavior control | toxicity reduction | open-domain dialogues | model assessment | safety-focused training | usability metrics | model enhancement | safety strategies,safety in dialogue models | human-and-model-in-the-loop | generative models | offensive behavior | toxicity mitigation | open-domain dialogue | model evaluation | safety distillation | usability metrics | model improvements | safety techniques,Safety in dialogue models | Human-and-model-in-the-loop | Generative models | Offensive behavior | Toxicity mitigation | Open-domain dialogue | Model evaluation | Distillation of safety | Usability metrics | Safety techniques | Model improvements
"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.",logical reasoning tasks | multi-step deductions | inference problems | selection-inference model | causal reasoning methods | system performance | 5-shot generalization | 7b parameter model | trustworthy inference systems | reasoning analysis | si framework,logical reasoning | multi-step reasoning | inference tasks | Selection-Inference framework | causal reasoning | model performance | 5-shot generalization | 7B parameter LLM | trustworthy systems | reasoning trace | SI framework,Logical reasoning | Language models | Multi-step reasoning | Selection-Inference framework | Inference tasks | 5-shot generalization | Causal reasoning | Model performance | 7B parameter LLM | Trustworthy systems | Reasoning trace | SI framework
"Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.",instruction tuning | dialogue models | zero-shot | InstructDial | task generalization,instruction tuning | dialogue systems | zero-shot performance | InstructDial | dialogue tasks | text-to-text format | cross-task generalization | meta-tasks | few-shot performance | benchmark | intent detection,Instruction tuning | Dialogue systems | Zero-shot performance | InstructDial | Dialogue tasks | Text-to-text format | Cross-task generalization | Meta-tasks | Few-shot performance | Benchmark | Dialogue evaluation | Intent detection
"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.",racial bias | NLP development | marginalized voices | historical racism | inclusion in research,race in NLP | racial bias | NLP model development | racial hierarchies | historical racism | single-dimensional variables | marginalized voices | racial justice | inclusion in NLP research | research gaps | ACL anthology,Race in NLP | Racial bias | NLP model development | Racial hierarchies | Research gaps | Single-dimensional variables | Historical racism | Marginalized voices | Racial justice | Inclusion in NLP research | ACL anthology
"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.",neural networks | symbolic math | integration | differential equations | symbolic problem-solving,neural networks | symbolic mathematics | symbolic integration | differential equations | mathematical tasks | sequence-to-sequence models | computer algebra systems | Matlab | Mathematica | symbolic problem-solving | training datasets,Neural networks | Symbolic mathematics | Symbolic integration | Differential equations | Mathematical tasks | Sequence-to-sequence models | Computer Algebra Systems | Matlab | Mathematica | Training datasets | Symbolic problem-solving
"Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.",instruction-following | fine-tuning | GPT-4 | data generation | reward model,instruction-following data | fine-tuning LLMs | GPT-4 | instruction-tuned models | zero-shot performance | data generation | English and Chinese datasets | feedback collection | reward model training | LLaMA models | public data release,Instruction-following data | Finetuning LLMs | GPT-4 | Instruction-tuned models | Zero-shot performance | Data generation | English and Chinese datasets | Feedback collection | Reward model training | LLaMA models | Public data release | Codebase
"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.",C4 Dataset | Common Crawl | machine text | minority bias | data filtering,C4 Dataset | Common Crawl | machine-generated text | minority biases | data filtering | dataset documentation | web-scale datasets,C4 Dataset | Common Crawl | machine-generated text | minority biases | data filtering | dataset documentation | web-scale datasets
"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.",least-to-most prompting | chain-of-thought | symbolic reasoning | math tasks | compositional generalization,least-to-most prompting | chain-of-thought | symbolic manipulation | reasoning tasks | math reasoning | compositional generalization | task generalization | GPT-3,least-to-most prompting | chain-of-thought | reasoning tasks | symbolic manipulation | compositional generalization | math reasoning | GPT-3 | task generalization
"Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .",Binder | neural-symbolic | programming language | WikiTableQuestions | program generation,Binder | neural-symbolic framework | programming languages | language model API | zero-training | WikiTableQuestions | TabFact | program generation,Binder | neural-symbolic framework | programming languages | language model API | zero-training | WikiTableQuestions | TabFact | program generation
"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.   We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.",instruction tuning | FLAN model | zero-shot | natural language | task generalization,instruction tuning | zero-shot learning | FLAN model | fine-tuning | natural language instructions | GPT-3 | task generalization,instruction tuning | zero-shot learning | FLAN model | fine-tuning | natural language instructions | GPT-3 | task generalization
"We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.",language bias | word embeddings | semantic analysis | gender bias | ethnic bias,language bias | Reddit | word embeddings | semantic analysis | gender bias | religion bias | ethnic bias | bias discovery,language bias | Reddit | word embeddings | semantic analysis | gender bias | religion bias | ethnic bias | bias discovery
"In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52\% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.",humor generation | wordplay | context humor | semantic ambiguity | cognitive linguistics,humor generation | wordplay | context-based humor | GPT-3 | ambiguity in words | semantic analysis | cognitive linguistics,pun generation | GPT-3 | humor theory | word ambiguity | context words | one-shot learning | semantic analysis
"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the NASNet search space) which enables transferability. In our experiments, we search for the best convolutional layer (or cell) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named NASNet architecture. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.",convolutional networks | CIFAR-10 | image classification | visual recognition | deep learning,convolutional neural networks | CIFAR-10 | image classification | transfer learning | architecture search | visual recognition | neural network optimization | deep learning models,NASNet | neural architecture search | convolutional layers | CIFAR-10 | ImageNet | ScheduledDropPath | generalization | transfer learning
"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.",conversational agents | goal-oriented dialogue | task-driven interaction | NLP for conversation | user intent modeling,conversational agents | goal-driven dialogue | reinforcement learning | task-based interaction | dialogue systems | NLP for dialogue | user intent modeling,goal-oriented dialogue | chit-chat | reinforcement learning | imitation learning | text-based fantasy | multi-player interaction | dialogue models
"Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.",hate speech detection | bias identification | offensive language analysis | fairness in models | social media classification,hate speech recognition | machine learning for bias detection | toxic text analysis | gender bias detection | language model fairness | social media classification | racial bias in text,hate speech detection | GPT-3 | zero-shot learning | racist text | sexist text | language model | bias classification | text analysis
"In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.   In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance.   We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.",AI reliability | adversarial vulnerabilities | safety protocols | model dependability | defensive measures in AI,robustness in AI | adversarial attacks | AI safety protocols | model reliability | language generation security | defense mechanisms in AI | AI robustness in high-risk environments,adversarial training | AI safety | high-stakes settings | reliability | language generation | safe AI | robustness | text filtering
"Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.",crowdsourced labeling | text categorization tasks | evaluation metrics | zero-shot learning | intercoder consistency,crowdsourced annotation | text classification tasks | performance evaluation | GPT-based models | zero-shot learning in classification | intercoder reliability | annotation efficiency,ChatGPT | crowd-workers | annotation tasks | performance | zero-shot accuracy | cost-effective | text classification | intercoder agreement | tweets
"Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of AI for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.",AI in coding | program translation | machine learning in programming | CodeNet dataset | algorithmic problem solving,AI in software engineering | code translation | machine learning for code | reinforcement learning in software development | CodeNet dataset | deep learning for programming tasks | algorithmic problem-solving,software development | AI for Code | CodeNet dataset | deep learning | machine learning | code classification | code translation | code performance | reinforcement learning | AI techniques | software engineering
"Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power.Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.",ethics in AI | societal impact | AI fairness | algorithm accountability | generalization in machine models,machine learning ethics | societal impacts of AI | research values | AI fairness | generalization in machine learning | algorithmic accountability | efficiency in tech companies,machine learning values | institutional practices | research papers | societal impact | machine learning values | performance | generalization | efficiency | novelty | tech companies | elite universities
"The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .",reinforcement learning | policy improvement | complex environments | agent training | reward-driven learning,behavioral reinforcement learning | policy optimization | complex environments | agent training | motor control tasks | reward-based learning | deep reinforcement models,reinforcement learning | behavior learning | reward function | diverse environments | locomotion | policy gradient | complex behavior | agents | simulated bodies | environmental contexts
"In a world increasingly dominated by AI applications, an understudied aspect is the carbon and social footprint of these power-hungry algorithms that require copious computation and a trove of data for training and prediction. While profitable in the short-term, these practices are unsustainable and socially extractive from both a data-use and energy-use perspective. This work proposes an ESG-inspired framework combining socio-technical measures to build eco-socially responsible AI systems. The framework has four pillars: compute-efficient machine learning, federated learning, data sovereignty, and a LEEDesque certificate.   Compute-efficient machine learning is the use of compressed network architectures that show marginal decreases in accuracy. Federated learning augments the first pillar's impact through the use of techniques that distribute computational loads across idle capacity on devices. This is paired with the third pillar of data sovereignty to ensure the privacy of user data via techniques like use-based privacy and differential privacy. The final pillar ties all these factors together and certifies products and services in a standardized manner on their environmental and social impacts, allowing consumers to align their purchase with their values.",AI sustainability | energy-efficient algorithms | federated learning | eco-friendly AI | carbon reduction strategies,AI sustainability | energy-efficient models | federated learning | eco-friendly machine learning | carbon footprint reduction | privacy-preserving AI | environmental impacts of AI,AI carbon footprint | ESG framework | eco-social responsibility | machine learning | federated learning | data sovereignty | compute-efficient AI | differential privacy | environmental impact
"Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe",parallel processing | transformer scaling | model optimization | distributed training systems | multi-GPU setups,parallel computing | transformer models scaling | model training optimization | pipeline parallelism | distributed learning systems | AWS for deep learning | multi-GPU training,model parallelism | transformer models | TeraPipe | pipeline parallelism | model scaling | training optimization | GPT-3 | efficiency | AWS cluster | deep learning
"As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.",summarization models | text summarization | human feedback | reinforcement learning | ROUGE evaluation,summarization models | machine learning for summarization | human feedback in summarization | fine-tuning | ROUGE score | TL;DR dataset | CNN/DM | human preferences | reinforcement learning for summarization,summary quality | machine learning | human preferences | reinforcement learning | summarization models | ROUGE | TL;DR dataset | CNN/DM | human feedback | fine-tuning
"Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. On top of our DC accelerator optimized neural architecture search space, we further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from previous compound scaling results. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than EfficientNet (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100.",Neural architecture search | model optimization | CNN models | accelerator chips | latency reduction | model scaling | EfficientNet | TPUv3 | GPU optimization,Neural Architecture Search | model efficiency | CNN networks | DC accelerators | latency optimization | model scaling | EfficientNet-X | TPUv3 | GPUv100 | compound scaling | accuracy,Neural Architecture Search | DC accelerators | CNN models | efficiency | latency | compound scaling | EfficientNet-X | TPUv3 | GPUv100 | accuracy | model scaling
"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.",BLOOMZ model | multilingual language models | multi-task fine-tuning | zero-shot capability | BLOOM | mT5 | language generalization | multilingual tasks,BLOOMZ model | multilingual language modeling | multitask fine-tuning | zero-shot learning | BLOOM | mT5 | state-of-the-art language generalization | multilingual tasks | mT0,multilingual models | multitask finetuning | BLOOMZ | mT0 | zero-shot learning | language generalization | BLOOM | mT5 | multilingual tasks | state-of-the-art
"This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to \emph{distill} knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\% (1.5\% absolute improvement\footnote{ Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and pre-trained models will be made publicly available at https://github.com/namisan/mt-dnn.",Knowledge distillation | multi-task learning | MT-DNN | GLUE tasks | model ensemble | text representation | distillation method | deep neural networks,knowledge distillation | multi-task learning in NLP | MT-DNN | GLUE benchmark | model ensemble methods | text representations | distillation technique | deep neural networks | multi-task performance,knowledge distillation | multi-task learning | MT-DNN | model ensemble | GLUE benchmark | text representations | distillation method | model performance | deep neural networks | multi-task tasks
"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.",Reasoning in NLP | cognitive abilities | critical thinking tasks | problem-solving skills | decision-making models | language models | reasoning evaluation | benchmarks for logic,reasoning in NLP | human intelligence | critical thinking tasks | problem solving | decision-making abilities | large language models | natural language reasoning | evaluation techniques | benchmarks for reasoning,reasoning | human intelligence | problem solving | decision making | critical thinking | large language models | natural language processing | reasoning abilities | evaluation | techniques | benchmarks | future directions
"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.",Low-resource languages | multilingual datasets | web-mined corpora | language quality checks | rare language codes | mislabeling risks | data evaluation techniques,low-resource languages | multilingual corpora | web-mined language datasets | language quality audit | nonstandard language codes | mislabeling risks | evaluation techniques for corpora | manual data curation,multilingual corpora | language datasets | low-resource languages | web-mined text | manual audit | language quality | corpora issues | mislabeling | nonstandard language codes | evaluation techniques | risks
"We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.",Dataset growth in ML | NLP dataset efficiency | vision datasets | high-quality vs poor data | data availability | future dataset trends | compute budgets,dataset growth in machine learning | NLP dataset efficiency | computer vision datasets | high-quality vs low-quality data | data availability | future dataset predictions | compute budgets for data | data efficiency in learning,dataset growth | machine learning | natural language processing | computer vision | dataset sizes | data efficiency | high-quality data | low-quality data | future predictions | data availability | compute budgets
"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.",Transformer-XL architecture | segment-level recurrence | dependency learning in models | positional encoding | long-term dependencies | language model performance | text generation speed,Transformer-XL architecture | segment-level recurrence in language models | neural architecture for dependency learning | positional encoding | long-term dependency modeling | language modeling performance | text generation speed | pretrained models | perplexity/bpc,Transformer-XL | neural architecture | dependency learning | segment-level recurrence | positional encoding | language modeling | long-term dependency | temporal coherence | performance | speed | bpc/perplexity | text generation | pretrained models
"Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.",ALERT benchmark | reasoning tasks in models | abductive reasoning | analogical reasoning | model generalization | robustness in reasoning | textual entailment tasks,ALERT benchmark for reasoning tasks | reasoning abilities in pre-trained models | abductive reasoning | analogical reasoning | generalization in model fine-tuning | robustness in reasoning models | textual entailment tasks | empirical model analysis | prompt overfitting,ALERT benchmark | reasoning abilities | pre-trained models | fine-tuned models | reasoning tasks | textual entailment | abductive reasoning | analogical reasoning | generalization | robustness | prompt overfitting | empirical analysis
"We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.",Transformer networks | weight programming | universal models | attention mechanisms | iterative computations | transformer loops | backpropagation | program execution,transformer networks | programming with weights | universal computation models | attention mechanism | iterative algorithms | transformer loops for computation | backpropagation | in-context learning | program execution,transformer networks | universal computers | programming with weights | iterative algorithms | attention mechanism | computation blocks | emulation | program execution | transformer loops | backpropagation | in-context learning
"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in.   In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.",Gender bias reduction | word embeddings | debiasing methods | contextual embeddings | NLP debiasing | bias mitigation | representation learning | gender bias,gender bias reduction | word embeddings | debiasing techniques | contextualized embeddings | NLP applications | bias mitigation strategies | bias analysis | representation learning | gender bias in NLP,gender bias | word embeddings | contextualized embeddings | gender bias reduction | NLP applications | bias mitigation | debiasing | representation | bias analysis
"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.",Transfer learning techniques | ULMFiT model | text classification | fine-tuning models | pretrained networks | error reduction strategies | task adaptation,transfer learning techniques | Universal Language Model Fine-tuning | ULMFiT model | text classification tasks | fine-tuning state-of-the-art models | pretrained models | error reduction strategies | task adaptation in NLP,transfer learning | Universal Language Model Fine-tuning | ULMFiT | text classification | fine-tuning techniques | state-of-the-art performance | error reduction | pretrained models | task adaptation
"Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.",Neural dialogue systems | RL for dialogue | black-box models | manipulation defense | model inputs | manipulation prevention | inappropriate responses,neural dialogue systems | reinforcement learning for dialogue | black-box models | manipulation of target outputs | model inputs | defense strategies against manipulation | inappropriate responses in dialogue,neural dialogue systems | manipulation | black-box models | target outputs | reinforcement learning | model inputs | inappropriate responses | defense strategies | dialogue manipulation
"Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.",Continual learning | self-supervised models | zero-shot abilities | task retention | model adaptation | pre-training strategies | continual learning studies,continual learning | self-supervision in language models | zero-shot performance in models | Continual-T0 (CT0) model | task retention and adaptation | pre-training methods | empirical investigation in continual learning | instruction following,continual learning | language models | zero-shot performance | self-supervision | pre-training | Continual-T0 (CT0) | diverse tasks | compositionality | instruction following | model adaptation | task retention | empirical investigation
"We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.",GPT-NeoX-20B | autoregressive models | 20 billion parameters | Pile dataset | few-shot reasoning | GPT-3 | FairSeq architecture | knowledge tasks,GPT-NeoX-20B model | autoregressive language models | 20 billion parameters | Pile dataset | few-shot reasoning performance | open-source models | GPT-3 | FairSeq architecture | knowledge-based tasks | model weights | language understanding,GPT-NeoX-20B | autoregressive model | 20 billion parameters | Pile dataset | open-source | few-shot reasoning | evaluation | performance | GPT-3 | FairSeq models | architecture | training | language understanding | mathematics | knowledge-based tasks | model weights
"Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.",Carbon footprint of ML | energy consumption | environmental cost of NLP | model training impact | computational resources | emission reporting | energy-efficient inference,carbon footprint of machine learning | energy consumption in model training | environmental impact of NLP | BLOOM model | computational resources for training | 176 billion parameters | carbon emissions reporting | inference energy operations,carbon footprint | machine learning | environmental impact | computational resources | energy consumption | BLOOM model | 176 billion parameters | carbon emissions | training | life cycle | inference | energy-based operations | API endpoint | emissions reporting
"We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.",Retrieval system | multi-task tuning | BERRI dataset | task-aware retrieval | document retrieval methods | zero-shot performance | domain adaptation,retrieval system for multi-task instruction tuning | BERRI dataset | TART retrieval tasks | task-aware retrieval performance | document retrieval methods | zero-shot benchmarks | domain adaptation in retrieval | BEIR | LOTTE | X^2-Retrieval | user intent,retrieval system | multi-task instruction tuning | BERRI dataset | TART | task-aware retrieval | instructions | document retrieval | zero-shot benchmarks | BEIR | LOTTE | X^2-Retrieval | domain adaptation | user intent | retrieval performance
"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.",Unsupervised learning | deep convolutional GANs | image data generation | adversarial training | generator-discriminator networks | hierarchical learning | image task generation,unsupervised learning in CNNs | deep convolutional generative adversarial networks (DCGANs) | image datasets generation | adversarial pair learning | generator and discriminator networks | hierarchical representation learning | unsupervised tasks in image generation,unsupervised learning | CNNs | deep convolutional generative adversarial networks | DCGANs | image datasets | adversarial pair | generator | discriminator | hierarchy of representations | general image representations | unsupervised tasks
"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.",Generalization error | neural network size | scaling models | width and depth | network theory | vision tasks | data/model scaling,generalization error in neural networks | model size and dataset size | scaling of neural networks | width and depth of models | functional form of models | theory of neural networks | vision and language tasks | model/data scale relationships,generalization error | neural networks | model size | dataset size | functional form | scaling | width | depth | model/data scales | theory of neural networks | vision tasks | language tasks | model predictions
"Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.",Knowledge generation | commonsense QA | Reinforced Knowledge Introspector | Rainier model | GPT-3 reasoning | RL for QA | commonsense benchmarks,reasoning in knowledge generation | commonsense question answering | Reinforced Knowledge Introspector | knowledge generation performance | Rainier model | GPT-3 reasoning abilities | reinforcement learning for question answering | commonsense benchmarks,reasoning | knowledge | commonsense question answering | Rainier | Reinforced Knowledge Introspector | knowledge generation | GPT-3 | reinforcement learning | question answering performance | knowledge quality | commonsense benchmarks
"Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\rightarrow$ 85.4%) higher than the state-of-the-art.",Math word problems | sequence-to-sequence generation | ranking tasks | mathematical expression generation | multi-task math framework | model performance on Math23k,math word problems (MWP) | sequence-to-sequence models for generation | ranking task in math problems | mathematical expression generation | multi-task framework for math problems | mistakes correction in MWP models | model performance on Math23k dataset,math word problems | MWP | sequence-to-sequence models | generation task | mathematical expressions | ranking task | Generate & Rank | multi-task framework | generative language model | mistakes correction | model performance | Math23k dataset
"This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.",Multilingual idiomaticity detection | text similarity | sentence embeddings | binary classification tasks | multilingual data annotation | evaluation metrics | idiomatic expression detection,multilingual idiomaticity detection | semantic text similarity | sentence embedding for idiomatic expressions | binary classification tasks | datasets for idiomatic expressions | annotation procedure for multilingual tasks | evaluation metrics for idiomatic detection | team submissions for evaluation,multilingual idiomaticity detection | sentence embedding | idiomatic expressions | binary classification | semantic text similarity | datasets | annotation procedure | evaluation metrics | participant systems | team submissions | English | Portuguese | Galician
"Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Python-based natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its transformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robustness analysis results are available publicly on the NL-Augmenter repository (https://github.com/GEM-benchmark/NL-Augmenter).",Data augmentation | NLP robustness | NL-Augmenter framework | task augmentation | diversity in NLP tasks | model robustness analysis | data transformations,data augmentation techniques | NLP robustness evaluation | NL-Augmenter framework | data transformations for NLP | natural language tasks augmentation | data filters for robustness | model diversity in NLP tasks | robustness analysis | public repository for augmentation,data augmentation | NLP models | robustness evaluation | NL-Augmenter | data transformations | data filters | natural language tasks | augmentation framework | model diversity | robustness analysis | public repository
"Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.",Decoder transformers | semantic search | sentence embeddings | model scaling | fine-tuning for search | SGPT model | BEIR benchmark | embedding improvement | large models | code release,decoder transformers | semantic search models | sentence embeddings for semantic search | model scaling techniques | fine-tuning for semantic search | SGPT model | BEIR benchmark | sentence embedding improvement | large language models | 5.8 billion parameters | code release for models,decoder transformers | semantic search | sentence embeddings | model scaling | SGPT | prompting | fine-tuning | BEIR benchmark | sentence embedding improvement | 5.8 billion parameters | large language models | code release | semantic search models
"Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.",Language generation biases | fairness decoding methods | user interaction | societal impact | inclusivity in AI | fairness in models | bias reduction | marginalized groups | AI challenges | fairness research,language generation and societal biases | decoding methods for fairness | user interaction in language generation | societal impact of models | inclusivity in model techniques | fairness in AI | bias reduction in societal applications | marginalized populations and AI | open challenges in fairness | research directions for societal impact,language generation | societal biases | model techniques | user interaction | decoding methods | fairness | inclusivity | bias reduction | societal impact | marginalized populations | open challenges | research directions
"We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.   We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.",Compositional reasoning | multi-hop questions | reasoning sub-problems | chain-of-thought | compositionality gap | GPT-3 for reasoning | self-ask method | reasoning improvement | search engine integration,compositional reasoning in language models | multi-hop questions | sub-problems in reasoning | chain of thought method | compositionality gap in language tasks | single-hop performance improvement | GPT-3 for reasoning tasks | self-ask method in reasoning | reasoning improvement techniques | search engine integration for reasoning,compositional reasoning | language models | multi-hop questions | compositionality gap | sub-problems | single-hop performance | GPT-3 | self-ask method | chain of thought | reasoning improvement | search engine integration
"Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.",Environmental impact | carbon emissions | energy consumption | mobile equipment footprint | hardware manufacturing | sustainability in performance | energy-efficient systems | carbon reduction strategies,environmental impact of computing systems | carbon emissions in data centers | mobile equipment and energy consumption | hardware manufacturing carbon footprint | sustainability in performance and efficiency | future directions for environmental sustainability | energy consumption in data centers | carbon footprint reduction strategies,environmental impact | carbon emissions | computing systems | mobile equipment | data-centers | hardware manufacturing | energy consumption | carbon footprint | performance and efficiency | sustainability | future directions
"We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a probe network and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.",Visual classification | task embeddings | Fisher matrix | task similarity prediction | meta-learning in tasks | pre-trained features | semantic relations in classification,visual classification with vectorial representations | task embeddings and Fisher information matrix | task similarity prediction | meta-learning in task performance | pre-trained feature extractor for classification | semantic relations in visual classification | task performance prediction models,visual classification | vectorial representations | task embeddings | Fisher information matrix | task similarity | semantic relations | meta-learning | pre-trained feature extractor | task performance prediction
"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.",Meena chatbot | open-domain conversations | multi-turn dialogues | 2.6B parameters | perplexity minimization | SSA metric | chatbot evaluation | filtering mechanism | decoding optimization,Meena chatbot for multi-turn conversations | open-domain conversation in chatbots | 2.6B parameters for chatbot models | perplexity minimization techniques | SSA metric for chatbot evaluation | human-like conversation models | performance evaluation in chatbot systems | filtering mechanism for chatbots | decoding optimization for chatbots,Meena | multi-turn chatbot | open-domain conversation | 2.6B parameters | perplexity minimization | SSA metric | human-like conversation | chatbot evaluation | filtering mechanism | decoding optimization | performance evaluation
"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.",Constitutional AI | reinforcement learning | AI behavior control | harmless assistants | supervised feedback | chain-of-thought | preference model | human oversight | AI transparency,Constitutional AI for self-improvement | reinforcement learning in AI behavior control | harmless AI assistant models | supervised learning for AI feedback | chain-of-thought reasoning in AI | preference model for behavior control | human oversight in AI systems | transparency in AI behavior,Constitutional AI | self-improvement | reinforcement learning | harmless AI assistant | supervised learning | AI feedback | chain-of-thought | human oversight | preference model | AI behavior control | transparency
"Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \url{https://huggingface.co/docs/transformers/model_doc/roformer}.",Position encoding | Rotary Position Embedding | sequence length | relative position dependency | RoFormer model | self-attention | long text classification | transformer performance,position encoding techniques in transformer models | Rotary Position Embedding (RoPE) for sequence length | relative position dependency in transformers | RoFormer model | self-attention in transformer architecture | long text classification improvements | transformer architecture performance | sequence length flexibility in transformers,position encoding | transformer architecture | Rotary Position Embedding (RoPE) | relative position dependency | sequence length flexibility | RoFormer | long text classification | self-attention | model performance | transformer improvements
"GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as N/A. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.",Few-shot learning | prompt instability | model prediction accuracy | contextual calibration | model bias | accuracy improvement | training examples | prompt formats,few-shot learning in GPT-3 | prompt instability in few-shot models | accuracy variation in model predictions | contextual calibration in few-shot learning | model bias in few-shot tasks | task accuracy improvement strategies | training example order in prompt-based learning | prompt formats and model performance,few-shot learning | GPT-3 | prompt instability | accuracy variation | calibration | contextual calibration | model bias | task accuracy improvement | prompt formats | training example order | model performance
"We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data $(x, y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we have access to the conditionals $p(x \mid y)$ and $p(y \mid x)$ in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation.",KERMIT generative modeling | sequence pairs | insertion-based approach | machine translation | autoregressive decoding | zero-shot question answering | KERMIT performance | sequence modeling | representation learning,KERMIT generative modeling for sequence pairs | insertion-based approach in generative models | joint distribution in machine translation | marginals in generative models | autoregressive decoding techniques | zero-shot question answering with KERMIT | performance of KERMIT in sequence modeling | representation learning in KERMIT,KERMIT | generative modeling | sequence pairs | insertion-based approach | joint distribution | marginals | machine translation | representation learning | zero-shot question answering | autoregressive decoding | performance
"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).",DeBERTa model | BERT | RoBERTa | disentangled attention | content vectors | position vectors | mask decoder | adversarial training | SuperGLUE performance | model fine-tuning,DeBERTa model and BERT | RoBERTa and disentangled attention | content and position vectors in transformer models | mask decoder in fine-tuning models | virtual adversarial training for NLP tasks | model fine-tuning for DeBERTa | performance improvement on SuperGLUE leaderboard | BERT and DeBERTa comparison in performance,DeBERTa | BERT | RoBERTa | disentangled attention | content and position vectors | mask decoder | virtual adversarial training | model fine-tuning | NLP tasks | performance improvement | SuperGLUE leaderboard
"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.",Factual consistency | summarization models | weakly-supervised techniques | rule-based transformations | span extraction | model-based methods | natural inference | summarization consistency,factual consistency in summarization models | weakly-supervised summarization techniques | rule-based transformations for summarization | span extraction for summarization tasks | model-based summarization methods | natural language inference for summarization consistency,factual consistency | summarization | weakly-supervised | model-based | rule-based transformations | span extraction | natural language inference
"Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans in the wild remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.",Reinforcement learning | crowd-sourced data | real-time systems | conversational bots | dynamic actions | Google Assistant | open-ended dialogue systems,reinforcement learning | crowd-sourced data | real-time systems | conversational bots | dynamic action space | Google Assistant | open-ended dialogue systems,reinforcement learning | conversational bots | open-ended dialogue | dynamic action space | real-time system | Google Assistant | crowd-sourced data
"Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.",In-context learning | large models | GPT-3 | latent document concepts | LSTMs | Transformers | synthetic datasets | GINC models,in-context learning | large language models | GPT-3 | latent document concepts | LSTMs | Transformers | synthetic datasets | GINC models,in-context learning | large language models | GPT-3 | latent document-level concepts | Transformers | LSTMs | synthetic dataset | GINC
"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the Colossal Clean Crawled Corpus and achieve a 4x speedup over the T5-XXL model.",Mixture of Experts | sparse models | Switch Transformer | pre-training speed | multilingual models | computational cost | training stability | T5 models,Mixture of Experts models | sparse models | Switch Transformer | pre-training speed | multilingual models | computational cost | training stability | T5 models,Mixture of Experts | Switch Transformer | sparse models | computational cost | training stability | multilingual models | pre-training speed | T5
"Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.",Holistic evaluation | model transparency | fairness metrics | robustness | toxicity detection | multi-metric assessment | accuracy and bias,Holistic Evaluation of Language Models | model transparency | fairness evaluation | robustness metrics | toxicity detection | multi-metric model assessment | accuracy and bias,Holistic Evaluation of Language Models | transparency | model evaluation | accuracy | fairness | robustness | toxicity | bias | multi-metric approach
"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.",Transformer models | reading comprehension | language modeling | AI safety | fact-checking | Gopher model | bias detection,Transformer-based models | reading comprehension | language modelling | AI safety | fact-checking | Gopher model | bias and toxicity,Transformer-based models | language modelling | reading comprehension | fact-checking | bias | toxicity | AI safety | model scale | Gopher
"The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.",Pruning techniques | quantization | neural networks | algorithmic bias detection | fairness | human-in-the-loop | model compression,pruning techniques | quantization for neural networks | algorithmic bias detection | fairness in deep learning | human-in-the-loop auditing | model compression | identified exemplars,pruning | quantization | Compression Identified Exemplars | algorithmic bias | fairness | deep neural networks | human-in-the-loop auditing
"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.",Multi-agent learning | task distribution | 3D world tasks | agent improvement | cooperation | zero-shot generalization | tool use,multi-agent learning systems | task distribution | 3D world tasks | agent improvement | cooperation in agents | zero-shot generalization | tool use in agent-based learning,multi-agent learning | 3D world tasks | generalization | agent improvement | task distribution | zero-shot generalization | cooperation | tool use
"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.",Program synthesis | CODEGEN | multi-turn programming | Python generation | JAXFORMER | open-source solutions | HumanEval,program synthesis with CODEGEN | multi-turn programming models | Python code generation | JAXFORMER framework | training library for code generation | open-source solutions | HumanEval,program synthesis | CODEGEN | multi-turn programming | JAXFORMER | Python code generation | HumanEval | training library | open-source
"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of green tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.",Watermarking models | algorithmic detection | model outputs | green tokens | security | OPT model robustness | output watermarking,watermarking language models | algorithmic detection of model outputs | green tokens | security in NLP models | OPT model robustness | model output watermarking,watermarking | language models | model output | algorithmic detection | green tokens | security | robustness | OPT model
"Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.",sequence generation | iterative processes | neural model improvements | multi-step refinement,edit-based models for sequence generation | iterative refinement techniques | multi-step edits in neural models | sequence generation models | neural model enhancements,edit-based models | iterative refinement | sequence generation | multi-step edits | neural models
"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.",text classification | memory-efficient architectures | quantization methods | model precision,compact architectures for text classification | memory efficiency in models | product quantization techniques | text classification accuracy | model efficiency,compact architectures | text classification | memory efficiency | product quantization | accuracy
"Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.",algebraic problems | mathematical operations | answer explanations | dataset generation,algebraic word problems | arithmetic operations | answer rationales | dataset | program synthesis,algebraic word problems | arithmetic operations | program synthesis | answer rationales | dataset
"Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks' learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.",intracranial EEG | CNN-based models | brain activity response | decoding performance,intracranial EEG data | CNN models | deep learning | error-related brain response | decoding accuracy,intracranial EEG | deep learning | CNNs | error-related brain response | decoding accuracy
"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like Let's think step by step to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the Let's think step by step prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot",reasoning steps | Auto-CoT method | large models | thought generation,chain-of-thought reasoning | large language models | Auto-CoT | reasoning steps | demonstration generation,chain-of-thought | large language models | reasoning steps | Auto-CoT | demonstration generation
"Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.",HTML comprehension | T5-based models | semantic categorization | web navigation,HTML understanding | T5 models | semantic classification | autonomous web navigation | large language models,HTML understanding | large language models | T5 models | semantic classification | autonomous web navigation
"Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.",StructBERT | transformer pre-training | language patterns | task-specific models,StructBERT model | BERT pre-training | language structures | downstream tasks | transformer models,StructBERT | BERT | pre-training | language structures | downstream tasks
"We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.",NewsQA dataset | answer pairs | comprehension tasks | F1 metric,NewsQA dataset | question-answer pairs | machine comprehension | reasoning abilities | F1 score,NewsQA | machine comprehension | question-answer pairs | reasoning | F1 score
"UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.",UMAP technique | dimensional reduction | manifold mapping | data visualization,UMAP algorithm | manifold learning | dimension reduction | Riemannian geometry | data visualization,UMAP | dimension reduction | manifold learning | Riemannian geometry | visualization
"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .",latent models | autoencoders | image creation | attention mechanisms,latent diffusion models | denoising autoencoders | image generation | cross-attention layers | computational efficiency,latent diffusion models | denoising autoencoders | image generation | computational efficiency | cross-attention layers
"We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.",MS MARCO | reading tasks | answer retrieval | Bing context,MS MARCO dataset | machine reading comprehension | question answering | passage retrieval | benchmarking | Bing context,ms marco | machine reading comprehension | dataset | question answering | benchmarking | bing | context | passage retrieval
"Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).",flashattention optimization | tiling techniques | memory management | self-attention mechanism | GPU speedup,flashattention optimization | IO-aware transformers | memory optimization | self-attention mechanism | GPU speedup | tiling techniques,flashattention | io-aware | transformer | speedup | memory optimization | self-attention | gpu | tiling
"There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.",transformer scaling | performance scaling | architecture design | pretraining models | inductive bias,transformer scaling properties | model architecture | inductive bias | pretraining | performance scaling | transfer learning | diverse models,transformer scaling | model architecture | inductive bias | pretraining | transfer learning | scaling properties | diverse models | performance
"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.",foundation models | multimodal capabilities | fine-tuning methods | pretrained encoders | general-purpose interface,foundation models | language models | general-purpose interface | multimodal capabilities | pretrained encoders | fine-tuning | in-context learning,foundation models | language models | general-purpose interface | multimodal | fine-tuning | pretrained encoders | vision | in-context learning
"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.",AI ethics | human values | deceptive language | behavioral challenges | AI alignment,AI alignment | human values | behavioral challenges | deceptive language | language agents | manipulative actions | misspecification | AI ethics,ai alignment | human values | behavioral issues | language agents | deceptive language | misspecification | manipulative | ai ethics
"The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.",algorithmic fairness | equity in AI | machine learning bias | fairness research | theoretical discussions,algorithmic fairness | bias in machine learning | equity in AI | theoretical research | fairness in AI | workshop discussions | research directions,algorithmic fairness | machine learning | bias | research directions | fairness | workshop | theoretical work | equity
"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.   In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM",Socratic CoT | problem decomposition | model performance | knowledge distillation | CoT techniques,Socratic CoT | knowledge distillation | reasoning through models | small-scale models | problem decomposition | subproblem solvers | CoT methods | model performance improvement,socratic cot | knowledge distillation | reasoning | small models | problem decomposition | subproblem solver | model performance | cot
"Face detection serves as a fundamental research topic for many applications like face recognition. Impressive progress has been made especially with the recent development of convolutional neural networks. However, the issue of large scale variations, which widely exists in high resolution images/videos, has not been well addressed in the literature. In this paper, we present a novel algorithm called SFace, which efficiently integrates the anchor-based method and anchor-free method to address the scale issues. A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations. The SFace architecture shows promising results on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.",face detection | CNN models | real-time accuracy | scale variations | 4k-face dataset,face detection | scale variations in faces | CNN models | real-time accuracy | 4k-face detection | wider face dataset,face detection | sface | 4k-face | scale variations | cnn | wider face | accuracy | real-time
"We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.",DialogPT | conversational generation | neural dialogue models | open-domain conversations | human evaluation,DialogPT | conversational response generation | dialogue systems | neural models for dialogue | transformer models | human evaluation | Reddit-based data | open-domain conversations,dialogpt | conversational response generation | neural models | dialogue systems | human evaluation | reddit | transformer | open-domain
"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.",pretrained language models | stereotype identification | cultural bias | demographic biases | bias evaluation,pretrained language models | social bias | cultural bias | stereotype identification | crowdsourcing for bias evaluation | demographic biases | MLM models,pretrained language models | cultural bias | social bias | crowds-pairs | stereotypes | bias evaluation | mlm | demographics
"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.",open-source transformers | large models | zero-shot learning | computational expense | carbon footprint,open-source pre-trained transformers | large language models | zero-shot learning | computational expense | transformer models | carbon footprint | model weights,open pre-trained transformers | large language models | zero-shot learning | computational cost | transformer models | carbon footprint | model weights | open source
"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.",generative models | open-domain question answering | triviaQA | evidence aggregation | retrieval-based methods,generative models for question answering | retrieval-based methods | open-domain question answering | triviaQA | performance evaluation | evidence aggregation methods,generative models | question answering | retrieval-based methods | open domain | natural questions | triviaqa | performance | evidence aggregation
"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io",robotics models | task scalability | performance metrics | large-scale data | real-world generalization,robotics transformer models | task-agnostic training | model scalability | generalization to real-world tasks | large-scale robotic data | performance metrics,robotics transformer | task-agnostic training | generalization | large-scale data | model scalability | real-world tasks | robotic data | performance
"The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",PIE performance edits | large language models | CODEGEN optimization | speedup in C++ | Python code,performance-improving edits (PIE) | code optimization | large language models | CODEGEN | CODEX | speedup in C++ and Python | PIE dataset,performance-improving edits | large language models | code optimization | PIE dataset | CODEGEN | CODEX | speedup | C++ Python
"Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.",multi-task learning | transfer learning | task scaling | ExMix | ExT5 improvement,multi-task learning | pre-training techniques | task scaling | ExMix | ExT5 | model improvement | transfer learning | sample efficiency,multi-task learning | pre-training | ExMix | task scaling | ExT5 | transfer learning | sample efficiency | model improvement
"Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.",language feedback | human evaluation | model refinement | GPT-3 summarization | fine-tuning,language feedback | human evaluation | model refinement | natural language feedback | GPT-3 | summarization tasks | fine-tuning | model output enhancement,language feedback | human evaluation | model refinement | natural language feedback | GPT-3 | summarization | finetuning | model output
"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.   These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.",habitat simulation | embodied AI | robot training | 3D navigation | SLAM dataset,habitat simulation | embodied AI training | 3D robot simulation | point-goal navigation | SLAM | dataset comparison | generalization in robot models,habitat | embodied AI | 3D simulation | robot training | point-goal navigation | SLAM | generalization | dataset comparison
"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.",dynamic manipulation | RL vision control | object grasping | QT-Opt model | real-world performance,dynamic manipulation in RL | vision-based control | object grasping | QT-Opt model | reinforcement learning | closed-loop control | real-world manipulation performance,dynamic manipulation | reinforcement learning | grasping | vision-based control | QT-Opt | closed-loop | object manipulation | real-world performance
"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.",responsible AI | fairness | AI regulation | system transparency | privacy protection,responsible AI | trust and accountability in AI | AI regulation | system transparency | fairness | privacy protection | AI safety,responsible AI | accountability | trust in AI | AI regulations | system transparency | fairness | privacy | safety
"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.",energy efficiency | sustainable AI | carbon emissions | reinforcement learning | consumption strategies,energy efficiency in AI | carbon emissions | sustainable machine learning | climate impact | reinforcement learning | energy consumption mitigation strategies,energy efficiency | carbon emissions | machine learning | reinforcement learning | sustainable AI | energy consumption | climate impact | mitigation strategies
"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.",LIMA model | unsupervised pretraining | GPT-4 | generalization | instruction tuning,LIMA model | unsupervised pretraining | reinforcement learning | generalization in AI | instruction tuning | GPT-4 | Bard | DaVinci003,LIMA | unsupervised pretraining | instruction tuning | reinforcement learning | generalization | GPT-4 | Bard | DaVinci003  
"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.",NLP generalization | Super-NaturalInstructions | cross-task evaluation | scaling parameters | Tk-Instruct,NLP models | generalization capabilities | Super-NaturalInstructions | Tk-Instruct | cross-task evaluation | scaling parameters for NLP models,NLP models | generalization | Super-NaturalInstructions | Tk-Instruct | cross-task evaluation | scaling parameters  
"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.",preference modeling | RLHF | human feedback | alignment training | OOD detection,preference modeling | RLHF | human feedback for alignment training | out-of-distribution (OOD) detection | Python coding | summarization tasks,preference modeling | RLHF | alignment training | human feedback | python coding | summarization | OOD detection  
"This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.",UniLM model | GLUE benchmark | question answering | SQuAD | ROUGE-L,UniLM model | language model | GLUE benchmark | question answering | SQuAD | CoQA | ROUGE-L | BLEU-4,UniLM | language model | GLUE benchmark | SQuAD | CoQA | ROUGE-L | BLEU-4 | question answering  
"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore. In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective. We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks. We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks. Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.",multimodal model | Transformer architecture | vision-language integration | ResNeXt | attentive modules,multimodal model | vision-language integration | Transformer architecture | ResNeXt | attentive modules | image-grounded conversation,multimodal model | vision and language | Transformer | ResNeXt | attentive module | image-grounded conversation  
"Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.",Atlas model | retrieval-augmented models | few-shot learning | MMLU | KILT benchmark,Atlas model | retrieval-augmented models | few-shot learning | knowledge-intensive tasks | Natural Questions dataset | MMLU | KILT benchmark,Atlas | retrieval augmented models | few-shot learning | knowledge-intensive tasks | Natural Questions | MMLU | KILT  
"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as gorillas. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.",Fairness in AI | debiasing methods | gender prejudice | racial prejudice | bias mitigation,Fairness in AI | dialogue systems | debiasing methods | bias mitigation | fairness benchmarks | gender and racial prejudice | addressing bias,Fairness in AI | dialogue systems | bias mitigation | fairness benchmark | gender and race prejudice | debiasing methods  
"We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.",Natural language critiques | behavioral cloning | model self-critique | self-critiquing AI | AI-assisted feedback,Natural language critiques | behavioral cloning in AI | model self-critique | summarization tasks | self-critiquing AI | AI-assisted feedback,Natural language critiques | behavioral cloning | model critiques | summarization | self-critiquing | AI-assisted human feedback  
"We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.",ReCoRD dataset | machine reading comprehension | commonsense reasoning | MRC systems | human evaluation,ReCoRD dataset | machine reading comprehension | commonsense reasoning | MRC systems | human performance evaluation,ReCoRD | machine reading comprehension | commonsense reasoning | MRC systems | human performance  
"Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.",PEER model | collaborative language models | self-training models | instruction-following systems | text infilling,PEER model | collaborative language models | writing process enhancement | self-training models | infilling text | instruction-following systems,PEER | collaborative language model | writing process | self-training | infilling | text editing | instruction following  
"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).",Training large transformers | intra-layer parallelism | GPU scaling | state-of-the-art results | BERT optimization,Training large transformer models | intra-layer model parallelism | GPU scaling | state-of-the-art results | model size optimization | GPT-2 | BERT | WikiText103 | LAMBADA | RACE dataset,Training large transformer models | intra-layer model parallelism | GPU scaling | SOTA results | model size | GPT-2 | BERT | WikiText103 | LAMBADA | RACE dataset  
"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.",Policy gradient methods | reinforcement learning techniques | model-free learning | trust region methods | neural policies,Policy gradient methods in RL | reinforcement learning techniques | value function optimization | trust region methods | 3D locomotion | bipedal and quadrupedal robots | neural network policies | model-free learning approaches,Policy gradient methods | reinforcement learning | value functions | trust region optimization | 3D locomotion | bipedal and quadrupedal robots | neural network policies | model-free learning  
"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.",Gender bias in NLP | ELMo embeddings | WinoBias dataset | bias mitigation strategies | coreference resolution,Gender bias in NLP | ELMo embeddings | contextualized word vectors | coreference resolution systems | WinoBias dataset | bias mitigation strategies | intrinsic analysis of bias,Gender bias | ELMo | contextualized word vectors | coreference system | WinoBias | bias mitigation | intrinsic analysis  
"Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.",Anthropomorphism in AI | dialogue systems | user personification | trust-building | design recommendations,Anthropomorphism in AI | dialogue systems | user personification | system transparency | trust-building | stereotype influences | design recommendations for AI,Anthropomorphism | dialogue systems | user personification | transparency | trust | stereotypes | linguistic factors | design recommendations  
"This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.",Speech recognition | convolutional neural networks | acoustic modeling | word error rates | MFCC feature extraction,Speech recognition systems | convolutional neural networks (CNN) | acoustic modeling | graph decoding techniques | automatic segmentation | word error rates | Librispeech corpus | MFCC feature extraction,Speech recognition | convolutional network | acoustic model | graph decoding | automatic segmentation | word error rate | Librispeech corpus | MFCC features  
"Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.",Gender bias in dialogue data | counterfactual data augmentation | targeted data collection | bias-controlled training | evaluation methods for bias,Gender bias in dialogue data | bias amplification in generative models | counterfactual data augmentation techniques | targeted data collection | bias-controlled training approaches | evaluation methods for bias reduction,Gender bias in dialogue data | bias amplification in generative models | mitigation techniques | counterfactual data augmentation | targeted data collection | bias controlled training | evaluation methods  
"While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work.   We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.",Dialogue evaluation challenges | pairwise comparisons | self-play model setups | novel evaluation methods | full dialogue assessments,Dialogue evaluation challenges | human judgment testing | pairwise comparisons in dialogue systems | novel evaluation methods | full dialogue assessments | self-play model chat setups | cost-effective testing strategies,Dialogue evaluation difficulties | human judgment tests | pairwise comparisons | novel evaluation procedure | full dialogues | self-play model chat setups | faster | cheaper tests  
"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.",Deep learning models | text classification | sentiment analysis | question answering | performance analysis,Deep learning models for text classification | sentiment analysis | question answering | model reviews | performance analysis | datasets for evaluation | future research directions,Deep learning models | text classification | sentiment analysis | question answering | review of models | datasets | performance analysis | future research directions  
"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7% respectively. We release our code and data under fully permissive licenses.",Aligning large language models | RLHF | human preference alignment | OpenAssistant Conversations | preference studies,Aligning large language models (LLMs) | human preference alignment | RLHF | OpenAssistant Conversations | human-annotated datasets | preference studies | comparison with GPT-3.5-turbo,Aligning large language models (LLMs) | human preferences | RLHF | OpenAssistant Conversations | human-annotated dataset | OpenAssistant | preference study | comparison with GPT-3.5-turbo  
"Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.",Generative AI systems | social impact framework | societal evaluations | privacy concerns | harm mitigation strategies,Generative AI systems | social impact framework | privacy concerns | environmental costs | data moderation techniques | societal evaluations | AI research contributions | harm mitigation strategies,Generative AI systems | social impact evaluation framework | bias | privacy | environmental costs | data moderation | societal evaluations | AI research contributions | harm mitigation  
"This paper presents the first consumer-scale next-word prediction (NWP) model trained with Federated Learning (FL) while leveraging the Differentially Private Federated Averaging (DP-FedAvg) technique. There has been prior work on building practical FL infrastructure, including work demonstrating the feasibility of training language models on mobile devices using such infrastructure. It has also been shown (in simulations on a public corpus) that it is possible to train NWP models with user-level differential privacy using the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models with DP-FedAvg in a real-world production environment on a heterogeneous fleet of mobile phones requires addressing numerous challenges. For instance, the coordinating central server has to keep track of the devices available at the start of each round and sample devices uniformly at random from them, while ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL work of which we are aware, for the first time we demonstrate the deployment of a differentially private mechanism for the training of a production neural network in FL, as well as the instrumentation of the production training infrastructure to perform an end-to-end empirical measurement of unintended memorization.",Differentially Private Federated Learning | next-word prediction | deployment challenges | empirical measurement | unintended memorization,Differentially Private Federated Learning (DP-FedAvg) | next-word prediction | real-world deployment | production-quality models | challenges in deployment | unintended memorization | empirical measurement of privacy risks,Differentially Private Federated Learning (DP-FedAvg) | next-word prediction | real-world deployment | production-quality models | challenges | empirical measurement of unintended memorization  
"By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the program, optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.",Automatic Prompt Engineer | task performance optimization | zero-shot learning | automatic instruction generation | guiding models,Automatic Prompt Engineer (APE) | task performance optimization | zero-shot learning | large language models | automatic instruction generation | guiding models towards truthfulness and informativeness,Automatic Prompt Engineer (APE) | task performance | zero-shot performance | large language models | automatic instruction generation | steering models towards truthfulness and informativeness  
"The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.",Knowledge in large language models | pre-training datasets | retrieval-augmentation methods | document relevance | competitive QA,Knowledge in large language models | pre-training datasets | document relevance | long-tail knowledge | retrieval-augmentation methods | competitive question answering (QA) | document count correlation,Knowledge in large language models | pre-training datasets | relevance of documents | long-tail knowledge | retrieval-augmentation | competitive QA performance | document count correlation  
"State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.",GSM8K dataset | multi-step reasoning tasks | verification methods | improving performance | verification of results,GSM8K dataset | multi-step mathematical reasoning tasks | transformer models | verification methods | improving model performance | verification of results | scalability with increased data,GSM8K dataset | multi-step mathematical reasoning | transformer models | verifiers | improving model performance | verification method | scalability with data  
"Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR",Warm restart techniques | deep neural networks | stochastic gradient descent | CIFAR-10 | improving model results,Warm restart techniques | stochastic gradient descent optimization | deep neural networks | CIFAR-10 | CIFAR-100 datasets | EEG signal processing | ImageNet | state-of-the-art performance | improving model results,Warm restart techniques | stochastic gradient descent | deep neural networks | CIFAR-10 | CIFAR-100 | EEG recordings | ImageNet | state-of-the-art results | performance improvement  
"An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.",Emerging methods for weaker models | finetuning techniques | NLP benchmarks | proprietary comparison | capabilities gap,Emerging methods to improve weaker models | finetuning techniques | language model imitation | proprietary model comparison | open-source models | capabilities gap | ChatGPT performance | NLP benchmarks,Emerging method to improve weaker models | finetuning | language model imitation | proprietary model | open-source model | capabilities gap | ChatGPT | NLP benchmarks
"Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multi-class classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.",Semi-supervised learning | self-training models | pseudo-labeling | binary and multi-class classification | decision boundary optimization,Semi-supervised learning | self-training models | binary and multi-class classification | pseudo-labeling | decision boundary optimization | unlabeled data | model confidence improvement,Semi-supervised learning | self-training | binary classification | multi-class classification | pseudo-labels | decision boundary | unlabeled data | model confidence
"As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., helping a friend is generally good, while helping a friend spread fake news is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.",Machine morality | Delphi framework | neural network biases | moral reasoning,Machine morality | Delphi framework for ethics | ethical judgments in AI | neural network biases | machine ethics | moral reasoning in AI systems | generalization of ethical models | bias correction methods,Machine morality | Delphi framework | ethical judgments | neural network biases | machine ethics | moral reasoning | generalization | bias correction
"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.   In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.   We illustrate our notion using a case study of FICO credit scores.",Discrimination in supervised learning | fair classification | protected group fairness | decision-making,"Discrimination in supervised learning | sensitive attributes in classification | fair classification practices | protected group fairness | oblivious measures | classification accuracy | decision-making in financial scoring (e.g., FICO)",Discrimination in supervised learning | sensitive attribute | fair classification | protected groups | oblivious measure | classification accuracy | decision maker | FICO credit scores
"Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py",Large batch optimization | deep neural network | LARS optimization | training efficiency,Large batch optimization | deep neural network training | LARS and LAMB optimization algorithms | training efficiency | BERT models | ResNet-50 | large mini-batches for fast training,Large batch optimization | deep neural networks | LARS | LAMB | training efficiency | BERT | ResNet-50 | large mini-batches
"Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.",Safety in conversational AI | neural agents | toxic behavior | harmful model behavior,Safety in conversational AI | neural conversational agents | toxic behavior | ethical considerations in AI | value-sensitive design | release decision-making | harmful model behavior | training models for safety,Safety in conversational AI | neural conversational agents | toxic language | value-sensitive design | release decisions | ethical implications | training models | harmful behavior
"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.",Text classification models | fastText | training efficiency | accuracy improvement,Text classification models | fastText | deep learning techniques | training efficiency | training speed | multicore CPU optimization | accuracy improvement | fast evaluation methods,Text classification | fastText | deep learning | efficiency | training speed | multicore CPU | accuracy | fast evaluation
"The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and ImageNet16$\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32_Scripts",ImageNet dataset | downsampled ImageNet | hyperparameter tuning | computational costs,ImageNet dataset | downsampled versions of ImageNet | ImageNet32x32 | ImageNet64x64 | image classification tasks | hyperparameter tuning | dataset efficiency | reducing computational costs,ImageNet dataset | downsampled ImageNet | ImageNet32x32 | ImageNet64x64 | image classification | hyperparameter tuning | dataset efficiency | cost reduction
"This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.",Augmented language models | tool usage | task decomposition | model interpretability,Augmented language models | reasoning capabilities | tool usage in AI models | language models for task decomposition | external modules integration | model interpretability | consistency in results,Augmented Language Models | reasoning skills | tool usage | language models | task decomposition | external modules | interpretability | consistency
"Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.",Autoregressive language models | GPT-like models | multilingual NLP tasks | zero-shot learning,Autoregressive language models | zero-shot learning techniques | GPT-like models | multilingual NLP tasks | low-resource languages | model training and evaluation | comparison with XGLM,Autoregressive language models | zero-shot learning | GPT-like models | multilingual NLP | low-resource languages | model training | evaluation | XGLM comparison
"Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.",Multi-task learning models | task performance | computational cost | target tasks,Multi-task learning models | downstream task performance | task transferability | scale of tasks | relatedness between tasks | computational cost considerations | representation learning for target tasks,Multi-task learning | downstream performance | task transferability | task scale | relatedness of tasks | computational cost | representation learning | target tasks
"Abstractive summarization systems today produce fluent and relevant output, but often hallucinate statements not supported by the source text. We analyze the connection between hallucinations and training data, and find evidence that models hallucinate because they train on target summaries that are unsupported by the source. Based on our findings, we present PINOCCHIO, a new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations. Given the model states and outputs at a given step, PINOCCHIO detects likely model hallucinations based on various measures of attribution to the source text. PINOCCHIO backtracks to find more consistent output, and can opt to produce no summary at all when no consistent generation can be found. In experiments, we find that PINOCCHIO improves the consistency of generation (in terms of F1) by an average of~67% on two abstractive summarization datasets.",Abstractive summarization | PINOCCHIO method | transformer-based models | beam search,Abstractive summarization | reducing hallucinations | PINOCCHIO method for summarization | transformer-based summarization models | improving consistency | beam search for decoding | F1 score evaluation method,Abstractive summarization | hallucinations | PINOCCHIO method | transformer-based summarizer | consistency improvement | beam search | F1 score | decoding method
"The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.",Toxicity detection models | annotator impact | political biases | contextualizing toxicity,Toxicity detection models | bias in annotations | annotator identity impact | political biases | racism detection | social psychology in AI | contextualizing toxicity | identifying hate speech,Toxicity detection | biases in annotations | annotator identity | political beliefs | racism | social psychology | contextualizing toxicity | hate speech
"We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The task involves three subtasks corresponding to the hierarchical taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The task featured five languages: English, Arabic, Danish, Greek, and Turkish for Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020 was one of the most popular tasks at SemEval-2020 attracting a large number of participants across all subtasks and also across all languages. A total of 528 teams signed up to participate in the task, 145 teams submitted systems during the evaluation period, and 70 submitted system description papers.",Offensive language detection | SemEval-2020 Task | multilingual datasets | system evaluation,Offensive language detection | SemEval-2020 Task 12 | multilingual social media datasets | hierarchical taxonomy of offensive language | participant analysis | system evaluation methods,Offensive language identification | SemEval-2020 | multilingual datasets | social media | Task 12 | hierarchical taxonomy | participant analysis | system evaluation
"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/",Math word problems | MathQA dataset | neural network solvers | AQuA dataset,Math word problems | MathQA dataset | neural network-based math solvers | operation programs for math problems | AQuA dataset for QA tasks | sequence-to-program models | improving interpretability and performance,Math word problems | MathQA dataset | neural math solver | operation programs | AQuA dataset | sequence-to-program model | interpretability | performance improvement
"Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.",Text generation models | misinformation detection systems | adversarial attack identification | text categorization | NLP techniques,Text generative models | fake news detection systems | adversarial misuse detection | text classifiers | NLP techniques | machine learning methods | model evaluation metrics | error analysis in AI,Text generative models | fake news detection | adversarial misuse | text detectors | NLP | machine learning | model evaluation | error analysis
"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.",SQuAD 2.0 | reading comprehension | unanswerable queries challenge | Stanford QA dataset | crowdsource responses,SQuAD 2.0 | reading comprehension tasks | unanswerable questions challenge | Stanford Question Answering Dataset | crowdsource workers | natural language understanding | neural network systems | F1 score evaluation,SQuAD 2.0 | reading comprehension | unanswerable questions | Stanford Question Answering Dataset | crowdworkers | natural language understanding | neural systems | F1 score
"Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively good prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.",Zero-shot models | few-shot techniques | prompt-driven models | task instructions | natural inference,Zero-shot learning models | few-shot learning techniques | prompt-based models | task instructions in NLP | natural language inference | irrelevant prompts challenge | instruction-tuned models | model limitations analysis,Zero-shot learning | few-shot learning | prompt-based models | task instructions | natural language inference | irrelevant prompts | instruction-tuned models | model limitations
"Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.",Generative models | multimodal networks | scaling principles | token generation | data modality fusion,Generative language models | multimodal models | scaling laws in AI | token sequence generation | data modality integration | multi-modal training methods | synergy of modalities | training stability in large models,Generative language models | mixed-modal models | scaling laws | token sequences | data modalities | multi-modal training | synergy | training stability
"As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.",Social biases in AI | fairness issues in models | representational bias | gender and racial stereotyping,Social biases in machine learning | fairness in AI models | representational biases | language model stereotyping | gender and race bias in models | biased text generation,Social biases | machine learning fairness | representational biases | language models | stereotyping | gender bias | race bias | text generation
"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.",LLaMA models | foundational AI models | parameter optimization | publicly accessible datasets | GPT-3 comparison,LLaMA | foundational language models | model parameters optimization | publicly available datasets | GPT-3 comparison | cutting-edge models | performance benchmarks | contributions to research,LLaMA | foundation language models | model parameters | publicly available datasets | GPT-3 | state-of-the-art models | performance benchmarks | research community
"We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.",Tensor-Product Transformer | relational attention structure | TP-Attention in math models | mathematical dataset | attention performance,Tensor-Product Transformer | relational structure in attention | TP-Attention mechanism | Transformer models for math | Mathematics Dataset | solving free-form math problems | attention mechanism performance | state-of-the-art results,Tensor-Product Transformer | relation structure | TP-Attention | Transformer models | Mathematics Dataset | free-form math problems | attention mechanism | state-of-the-art performance
"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.",Lifelong learning in agents | neural training | forgetting issues | non-stationary data | cognitive learning mechanisms,Lifelong learning in autonomous agents | neural network training | catastrophic forgetting issue | non-stationary data challenges | memory consolidation in AI | neurocognitive mechanisms in learning | sensorimotor skills development,Lifelong learning | autonomous agents | neural networks | catastrophic forgetting | non-stationary data | memory consolidation | neurocognitive mechanisms | sensorimotor skills
"With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations.   Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.",Bias in embeddings | human biases in NLP | language bias detection | contextual embeddings | CEAT framework,Biases in word embeddings | human biases in NLP models | language model bias detection | contextualized embeddings | CEAT framework for bias analysis | intersectional and emergent biases,Biases in word embeddings | human biases | language models | contextualized embeddings | CEAT | intersectional biases | emergent biases | bias detection
"Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.",Chain-of-thought prompting | reasoning in models | knowledge transfer for reasoning | T5 XXL for tasks | commonsense reasoning,Chain-of-thought prompting | reasoning abilities in large language models | knowledge distillation for reasoning | T5 XXL model for reasoning tasks | arithmetic and symbolic reasoning | commonsense reasoning methods,Chain of thought prompting | reasoning capabilities | large language models | knowledge distillation | T5 XXL | arithmetic reasoning | symbolic reasoning | commonsense reasoning
"When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.",CommonsenseQA | answering tasks | prior knowledge usage | ConceptNet integration | multiple-choice answering,CommonsenseQA | question answering tasks | prior knowledge integration | ConceptNet in QA models | multiple-choice question answering | commonsense reasoning techniques | BERT-large performance comparison | human-level performance,CommonsenseQA | question answering | prior knowledge | ConceptNet | multiple-choice questions | commonsense reasoning | BERT-large | human performance
