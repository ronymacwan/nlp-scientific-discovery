cleaned_summary,human_annotated_hypothesis,together_ai_generated_hypothesis
"As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study.","""DistilBERT's efficiency in reducing model size while retaining performance suggests that incorporating knowledge distillation during pre-training may provide a scalable solution for deploying powerful language models on resource-constrained devices without significantly compromising accuracy.""","""Leveraging knowledge distillation during the pre-training phase of language models can significantly reduce model size and computational cost while retaining performance, enabling efficient deployment on resource-constrained devices without sacrificing accuracy."""
"Most Reading Comprehension methods limit themselves to queries which can be answered using a single sentence, paragraph, or document. Enabling models to combine disjoint pieces of textual evidence would extend the scope of machine comprehension methods, but currently there exist no resources to train and test this capability. We propose a novel task to encourage the development of models for text understanding across multiple documents and to investigate the limits of existing methods. In our task, a model learns to seek and combine evidence - effectively performing multi-hop (alias multi-step) inference. We devise a methodology to produce datasets for this task, given a collection of query-answer pairs and thematically linked documents. Two datasets from different domains are induced, and we identify potential pitfalls and devise circumvention strategies. We evaluate two previously proposed competitive models and find that one can integrate information across documents. However, both models struggle to select relevant information, as providing documents guaranteed to be relevant greatly improves their performance. While the models outperform several strong baselines, their best accuracy reaches 42.9% compared to human performance at 74.0% - leaving ample room for improvement.","""Enabling multi-hop reasoning across documents could significantly improve the model's ability to integrate information from multiple sources, highlighting the importance of training methods that focus on selecting and combining relevant evidence from a broader context.""","""Models trained for multi-hop inference, which combine evidence from multiple documents, will show improved performance with datasets that ensure the relevance of the documents, highlighting the importance of curated evidence for accurate information retrieval."""
"We propose a new family of policy gradient methods for reinforcement learning, which alternate between sampling data through interaction with the environment, and optimizing a surrogate objective function using stochastic gradient ascent. Whereas standard policy gradient methods perform one gradient update per data sample, we propose a novel objective function that enables multiple epochs of minibatch updates. The new methods, which we call proximal policy optimization (PPO), have some of the benefits of trust region policy optimization (TRPO), but they are much simpler to implement, more general, and have better sample complexity (empirically). Our experiments test PPO on a collection of benchmark tasks, including simulated robotic locomotion and Atari game playing, and we show that PPO outperforms other online policy gradient methods, and overall strikes a favorable balance between sample complexity, simplicity, and wall-time.","""Proximal Policy Optimization (PPO) could offer a more efficient alternative to traditional policy gradient methods, balancing simplicity, sample efficiency, and performance across a range of reinforcement learning tasks, which may drive improvements in real-world applications such as robotics.""","""Proximal policy optimization (PPO) methods, by allowing multiple epochs of minibatch updates, will outperform traditional policy gradient methods in terms of sample efficiency and stability in reinforcement learning tasks, especially in robotics and game environments."""
"Automatic post-editing (APE) aims to improve machine translations, thereby reducing human post-editing effort. APE has had notable success when used with statistical machine translation (SMT) systems but has not been as successful over neural machine translation (NMT) systems. This has raised questions on the relevance of APE task in the current scenario. However, the training of APE models has been heavily reliant on large-scale artificial corpora combined with only limited human post-edited data. We hypothesize that APE models have been underperforming in improving NMT translations due to the lack of adequate supervision. To ascertain our hypothesis, we compile a larger corpus of human post-edits of English to German NMT. We empirically show that a state-of-art neural APE model trained on this corpus can significantly improve a strong in-domain NMT system, challenging the current understanding in the field. We further investigate the effects of varying training data sizes, using artificial training data, and domain specificity for the APE task. We release this new corpus under CC BY-NC-SA 4.0 license at https://github.com/shamilcm/pedra.","""Expanding the size and quality of human post-editing datasets for neural machine translation could address the underperformance of automatic post-editing (APE) models, challenging current assumptions about the relevance of APE in modern machine translation systems.""","""Improving the training data for automatic post-editing (APE) models with more high-quality human post-edited translations for neural machine translation (NMT) can significantly boost their performance, demonstrating the relevance of human-supervised training data over artificial corpora."""
"Much of human dialogue occurs in semi-cooperative settings, where agents with different goals attempt to agree on common decisions. Negotiations require complex communication and reasoning skills, but success is easy to measure, making this an interesting task for AI. We gather a large dataset of human-human negotiations on a multi-issue bargaining task, where agents who cannot observe each other's reward functions must reach an agreement (or a deal) via natural language dialogue. For the first time, we show it is possible to train end-to-end models for negotiation, which must learn both linguistic and reasoning skills with no annotated dialogue states. We also introduce dialogue rollouts, in which the model plans ahead by simulating possible complete continuations of the conversation, and find that this technique dramatically improves performance. Our code and dataset are publicly available (https://github.com/facebookresearch/end-to-end-negotiator).","""Training AI models for negotiation in multi-issue bargaining tasks can improve their ability to reason and communicate effectively, suggesting that end-to-end models with dialogue rollouts might represent a promising avenue for building more sophisticated conversational agents.""","""End-to-end models for negotiation, leveraging dialogue rollouts for planning future conversation steps, will outperform traditional models by improving the ability to reach agreements in multi-issue bargaining tasks with natural language dialogue."""
"Books are a rich source of both fine-grained information, how a character, an object or a scene looks like, as well as high-level semantics, what someone is thinking, feeling and how these states evolve through a story. This paper aims to align books to their movie releases in order to provide rich descriptive explanations for visual content that go semantically far beyond the captions available in current datasets. To align movies and books we exploit a neural sentence embedding that is trained in an unsupervised way from a large corpus of books, as well as a video-text neural embedding for computing similarities between movie clips and sentences in the book. We propose a context-aware CNN to combine information from multiple sources. We demonstrate good quantitative performance for movie/book alignment and show several qualitative examples that showcase the diversity of tasks our model can be used for.","""Aligning books and movies through neural embeddings can enrich content understanding by capturing both visual and high-level semantic information, potentially providing more meaningful explanations for visual content in multimodal datasets.""","""Aligning books and movies through multimodal embeddings can enhance the accuracy of generating rich, descriptive content for visual scenes, leading to improved cross-modal understanding and applications in tasks such as video content summarization and semantic visual explanations."""
"Multimodal contrastive learning methods like CLIP train on noisy and uncurated training datasets. This is cheaper than labeling datasets manually, and even improves out-of-distribution robustness. We show that this practice makes backdoor and poisoning attacks a significant threat. By poisoning just 0.01% of a dataset (e.g., just 300 images of the 3 million-example Conceptual Captions dataset), we can cause the model to misclassify test images by overlaying a small patch. Targeted poisoning attacks, whereby the model misclassifies a particular test input with an adversarially-desired label, are even easier requiring control of 0.0001% of the dataset (e.g., just three out of the 3 million images). Our attacks call into question whether training on noisy and uncurated Internet scrapes is desirable.","""Multimodal contrastive learning methods, while cost-effective and robust, may be vulnerable to backdoor and poisoning attacks, raising concerns about the reliability of using noisy, uncurated data for training models without implementing stronger defenses.""","""Training multimodal models on noisy, uncurated datasets introduces vulnerabilities to backdoor and poisoning attacks, with minimal dataset manipulation potentially causing significant misclassification, calling into question the practice of using such unfiltered data for robust model training."""
"In current hate speech datasets, there exists a high correlation between annotators' perceptions of toxicity and signals of African American English (AAE). This bias in annotated training data and the tendency of machine learning models to amplify it cause AAE text to often be mislabeled as abusive/offensive/hate speech with a high false positive rate by current hate speech classifiers. In this paper, we use adversarial training to mitigate this bias, introducing a hate speech classifier that learns to detect toxic sentences while demoting confounds corresponding to AAE texts. Experimental results on a hate speech dataset and an AAE dataset suggest that our method is able to substantially reduce the false positive rate for AAE text while only minimally affecting the performance of hate speech classification.","""Adversarial training can help mitigate bias in hate speech detection models, especially regarding African American English, suggesting that addressing dataset biases could lead to more accurate and fairer classification systems.""","""Adversarial training methods can effectively reduce bias in hate speech classifiers by mitigating the over-detection of African American English (AAE) as toxic speech, thus improving fairness in automated classification systems without severely impacting overall classification accuracy."""
"Ideally Open-Domain Question Answering models should exhibit a number of competencies, ranging from simply memorizing questions seen at training time, to answering novel question formulations with answers seen during training, to generalizing to completely novel questions with novel answers. However, single aggregated test set scores do not show the full picture of what capabilities models truly have. In this work, we perform a detailed study of the test sets of three popular open-domain benchmark datasets with respect to these competencies. We find that 60-70% of test-time answers are also present somewhere in the training sets. We also find that 30% of test-set questions have a near-duplicate paraphrase in their corresponding training sets. Using these findings, we evaluate a variety of popular open-domain models to obtain greater insight into what extent they can actually generalize, and what drives their overall performance. We find that all models perform dramatically worse on questions that cannot be memorized from training sets, with a mean absolute performance difference of 63% between repeated and non-repeated data. Finally we show that simple nearest-neighbor models out-perform a BART closed-book QA model, further highlighting the role that training set memorization plays in these benchmarks","""The discrepancy between a model's ability to memorize training data and generalize to novel, non-repeated questions reveals that current open-domain QA models may be overly reliant on memorization, indicating that improvements are needed to enhance generalization and performance on truly unseen data.""","""Open-domain question answering models will show significant performance degradation when tasked with answering novel, unseen questions, and their performance is heavily dependent on memorization of the training set, rather than generalization to new questions."""
"Increasing interest in privacy-preserving machine learning has led to new and evolved approaches for generating private synthetic data from undisclosed real data. However, mechanisms of privacy preservation can significantly reduce the utility of synthetic data, which in turn impacts downstream tasks such as learning predictive models or inference. We propose several re-weighting strategies using privatised likelihood ratios that not only mitigate statistical bias of downstream estimators but also have general applicability to differentially private generative models. Through large-scale empirical evaluation, we show that private importance weighting provides simple and effective privacy-compliant augmentation for general applications of synthetic data.","""Privacy-preserving synthetic data generation using re-weighting strategies can mitigate the loss of utility caused by privacy mechanisms, making it a practical approach for training models without compromising privacy.""","""Applying privacy-preserving methods such as private importance weighting can mitigate the statistical bias in synthetic data generated for machine learning tasks, improving downstream model utility while maintaining privacy compliance."""
"Some users of social media are spreading racist, sexist, and otherwise hateful content. For the purpose of training a hate speech detection system, the reliability of the annotations is crucial, but there is no universally agreed-upon definition. We collected potentially hateful messages and asked two groups of internet users to determine whether they were hate speech or not, whether they should be banned or not and to rate their degree of offensiveness. One of the groups was shown a definition prior to completing the survey. We aimed to assess whether hate speech can be annotated reliably, and the extent to which existing definitions are in accordance with subjective ratings. Our results indicate that showing users a definition caused them to partially align their own opinion with the definition but did not improve reliability, which was very low overall. We conclude that the presence of hate speech should perhaps not be considered a binary yes-or-no decision, and raters need more detailed instructions for the annotation.","""The subjective nature of hate speech annotations suggests that standard definitions may not be sufficient for reliable classification, highlighting the need for more nuanced guidelines and instructions for annotators.""","""Hate speech detection systems require more detailed guidelines and instructions for annotators, as current definitions of hate speech and the binary classification approach lead to low reliability in annotations across different user groups."""
"The increasing fluency and widespread usage of large language models (LLMs) highlight the desirability of corresponding tools aiding detection of LLM-generated text. In this paper, we identify a property of the structure of an LLM's probability function that is useful for such detection. Specifically, we demonstrate that text sampled from an LLM tends to occupy negative curvature regions of the model's log probability function. Leveraging this observation, we then define a new curvature-based criterion for judging if a passage is generated from a given LLM. This approach, which we call DetectGPT, does not require training a separate classifier, collecting a dataset of real or generated passages, or explicitly watermarking generated text. It uses only log probabilities computed by the model of interest and random perturbations of the passage from another generic pre-trained language model (e.g., T5). We find DetectGPT is more discriminative than existing zero-shot methods for model sample detection, notably improving detection of fake news articles generated by 20B parameter GPT-NeoX from 0.81 AUROC for the strongest zero-shot baseline to 0.95 AUROC for DetectGPT. See https://ericmitchell.ai/detectgpt for code, data, and other project information.","""DetectGPT's curvature-based approach for identifying LLM-generated text presents a promising method for detecting fake content without requiring additional training, offering significant improvements over existing detection techniques.""","""Detecting text generated by large language models (LLMs) is most effective by utilizing the model's own probability function and curvature characteristics, such as negative curvature regions, rather than relying on separate classifiers or explicit text watermarking techniques."""
"The past decade has witnessed dramatic gains in natural language processing and an unprecedented scaling of large language models. These developments have been accelerated by the advent of few-shot techniques such as chain of thought (CoT) prompting. Specifically, CoT pushes the performance of large language models in a few-shot setup by augmenting the prompts with intermediate steps. Despite impressive results across various tasks, the reasons behind their success have not been explored. This work uses counterfactual prompting to develop a deeper understanding of CoT-based few-shot prompting mechanisms in large language models. We first systematically identify and define the key components of a prompt: symbols, patterns, and text. Then, we devise and conduct an exhaustive set of experiments across four different tasks, by querying the model with counterfactual prompts where only one of these components is altered. Our experiments across three models (PaLM, GPT-3, and CODEX) reveal several surprising findings and brings into question the conventional wisdom around few-shot prompting. First, the presence of factual patterns in a prompt is practically immaterial to the success of CoT. Second, our results conclude that the primary role of intermediate steps may not be to facilitate learning how to solve a task. The intermediate steps are rather a beacon for the model to realize what symbols to replicate in the output to form a factual answer. Further, text imbues patterns with commonsense knowledge and meaning. Our empirical and qualitative analysis reveals that a symbiotic relationship between text and patterns explains the success of few-shot prompting: text helps extract commonsense from the question to help patterns, and patterns enforce task understanding and direct text generation.","""Chain-of-thought prompting's success in few-shot learning may stem from its ability to act as a guide for the model to replicate relevant symbols, indicating that intermediate steps are more about pattern recognition than learning task-specific solutions.""","""Chain of thought (CoT) prompting in few-shot learning setups works by helping models identify which symbols to replicate in their output rather than providing intermediate steps for task learning, with text and patterns working symbiotically to enhance performance."""
"Variational inference (VI) has become the method of choice for fitting many modern probabilistic models. However, practitioners are faced with a fragmented literature that offers a bewildering array of algorithmic options. First, the variational family. Second, the granularity of the updates e.g. whether the updates are local to each data point and employ message passing or global. Third, the method of optimization (bespoke or blackbox, closed-form or stochastic updates, etc.). This paper presents a new framework, termed Partitioned Variational Inference (PVI), that explicitly acknowledges these algorithmic dimensions of VI, unifies disparate literature, and provides guidance on usage. Crucially, the proposed PVI framework allows us to identify new ways of performing VI that are ideally suited to challenging learning scenarios including federated learning (where distributed computing is leveraged to process non-centralized data) and continual learning (where new data and tasks arrive over time and must be accommodated quickly). We showcase these new capabilities by developing communication-efficient federated training of Bayesian neural networks and continual learning for Gaussian process models with private pseudo-points. The new methods significantly outperform the state-of-the-art, whilst being almost as straightforward to implement as standard VI.","""Partitioned Variational Inference (PVI) could enhance federated and continual learning scenarios by unifying various algorithmic approaches, showing promise in addressing the challenges posed by non-centralized and evolving datasets.""","""The Partitioned Variational Inference (PVI) framework can unify disparate variational inference methods, improving their applicability and efficiency in challenging scenarios like federated and continual learning, while maintaining simplicity and performance."""
"Recent studies have demonstrated the efficiency of generative pretraining for English natural language understanding. In this work, we extend this approach to multiple languages and show the effectiveness of cross-lingual pretraining. We propose two methods to learn cross-lingual language models (XLMs): one unsupervised that only relies on monolingual data, and one supervised that leverages parallel data with a new cross-lingual language model objective. We obtain state-of-the-art results on cross-lingual classification, unsupervised and supervised machine translation. On XNLI, our approach pushes the state of the art by an absolute gain of 4.9% accuracy. On unsupervised machine translation, we obtain 34.3 BLEU on WMT'16 German-English, improving the previous state of the art by more than 9 BLEU. On supervised machine translation, we obtain a new state of the art of 38.5 BLEU on WMT'16 Romanian-English, outperforming the previous best approach by more than 4 BLEU. Our code and pretrained models will be made publicly available.","""Cross-lingual pretraining offers a scalable approach for improving multilingual models, as demonstrated by its state-of-the-art performance in both unsupervised and supervised machine translation tasks, suggesting its potential for more inclusive language model development.""","""Cross-lingual pretraining of language models, using both unsupervised and supervised methods, will lead to significant improvements in cross-lingual tasks like machine translation and classification, surpassing the performance of existing state-of-the-art methods."""
"We present LaMDA: Language Models for Dialog Applications. LaMDA is a family of Transformer-based neural language models specialized for dialog, which have up to 137B parameters and are pre-trained on 1.56T words of public dialog data and web text. While model scaling alone can improve quality, it shows less improvements on safety and factual grounding. We demonstrate that fine-tuning with annotated data and enabling the model to consult external knowledge sources can lead to significant improvements towards the two key challenges of safety and factual grounding. The first challenge, safety, involves ensuring that the model's responses are consistent with a set of human values, such as preventing harmful suggestions and unfair bias. We quantify safety using a metric based on an illustrative set of human values, and we find that filtering candidate responses using a LaMDA classifier fine-tuned with a small amount of crowdworker-annotated data offers a promising approach to improving model safety. The second challenge, factual grounding, involves enabling the model to consult external knowledge sources, such as an information retrieval system, a language translator, and a calculator. We quantify factuality using a groundedness metric, and we find that our approach enables the model to generate responses grounded in known sources, rather than responses that merely sound plausible. Finally, we explore the use of LaMDA in the domains of education and content recommendations, and analyze their helpfulness and role consistency.","""LaMDA's focus on safety and factual grounding through fine-tuning with annotated data and external knowledge sources could set a new standard for developing trustworthy dialogue systems that are both accurate and aligned with human values.""","""Fine-tuning large-scale dialog models like LaMDA with annotated data and incorporating external knowledge sources can significantly improve their safety (reducing harmful or biased responses) and factual grounding (ensuring accuracy and reliability in generated responses)."""
"Despite the success of large language models (LLMs) in various natural language processing (NLP) tasks, the stored knowledge in these models may inevitably be incomplete, out-of-date, or incorrect. This motivates the need to utilize external knowledge to assist LLMs. Unfortunately, current methods for incorporating external knowledge often require additional training or fine-tuning, which can be costly and may not be feasible for LLMs. To address this issue, we propose a novel post-processing approach, rethinking with retrieval (RR), which retrieves relevant external knowledge based on the decomposed reasoning steps obtained from the chain-of-thought (CoT) prompting. This lightweight approach does not require additional training or fine-tuning and is not limited by the input length of LLMs. We evaluate the effectiveness of RR through extensive experiments with GPT-3 on three complex reasoning tasks: commonsense reasoning, temporal reasoning, and tabular reasoning. Our results show that RR can produce more faithful explanations and improve the performance of LLMs.","""Rethinking with retrieval (RR) provides an efficient and lightweight method to augment LLMs with external knowledge, enhancing reasoning tasks without the need for additional training, making it a scalable solution for improving model outputs.""","""Post-processing approaches like Rethinking with Retrieval (RR), which incorporates external knowledge based on chain-of-thought reasoning, will improve the performance and faithfulness of large language models (LLMs) on complex reasoning tasks without requiring additional training or fine-tuning."""
"Large Language Models (LLMs) have demonstrated a remarkable ability to generalize zero-shot to various language-related tasks. This paper focuses on the study of exploring generative LLMs such as ChatGPT and GPT-4 for relevance ranking in Information Retrieval (IR). Surprisingly, our experiments reveal that properly instructed ChatGPT and GPT-4 can deliver competitive, even superior results than supervised methods on popular IR benchmarks. Notably, GPT-4 outperforms the fully fine-tuned monoT5-3B on MS MARCO by an average of 2.7 nDCG on TREC datasets, an average of 2.3 nDCG on eight BEIR datasets, and an average of 2.7 nDCG on ten low-resource languages Mr.TyDi. Subsequently, we delve into the potential for distilling the ranking capabilities of ChatGPT into a specialized model. Our small specialized model that trained on 10K ChatGPT generated data outperforms monoT5 trained on 400K annotated MS MARCO data on BEIR. The code to reproduce our results is available at www.github.com/sunnweiwei/RankGPT","""Generative LLMs like GPT-4 can outperform traditional supervised methods in information retrieval tasks, suggesting that leveraging the capabilities of these models in specialized domains could lead to more efficient retrieval systems.""","""Generative large language models (LLMs) like GPT-4, when properly instructed, can outperform traditional supervised methods for relevance ranking in information retrieval (IR), showing potential for distilling ranking capabilities from LLMs into specialized models for better performance with fewer annotated data."""
"In this paper we study yes/no questions that are naturally occurring --- meaning that they are generated in unprompted and unconstrained settings. We build a reading comprehension dataset, BoolQ, of such questions, and show that they are unexpectedly challenging. They often query for complex, non-factoid information, and require difficult entailment-like inference to solve. We also explore the effectiveness of a range of transfer learning baselines. We find that transferring from entailment data is more effective than transferring from paraphrase or extractive QA data, and that it, surprisingly, continues to be very beneficial even when starting from massive pre-trained language models such as BERT. Our best method trains BERT on MultiNLI and then re-trains it on our train set. It achieves 80.4% accuracy compared to 90% accuracy of human annotators (and 62% majority-baseline), leaving a significant gap for future work.","""Yes/no questions requiring complex reasoning expose gaps in current transfer learning models, particularly when dealing with entailment, highlighting the need for more advanced approaches to train models on challenging logical inference tasks.""","""Yes/no questions, particularly those requiring complex inferences, are more challenging than previously thought and require models trained on entailment data, rather than paraphrase or extractive QA data, to achieve competitive performance in naturally occurring question answering tasks."""
"In this work, we explore techniques for augmenting interactive agents with information from symbolic modules, much like humans use tools like calculators and GPS systems to assist with arithmetic and navigation. We test our agent's abilities in text games -- challenging benchmarks for evaluating the multi-step reasoning abilities of game agents in grounded, language-based environments. Our experimental study indicates that injecting the actions from these symbolic modules into the action space of a behavior cloned transformer agent increases performance on four text game benchmarks that test arithmetic, navigation, sorting, and common sense reasoning by an average of 22%, allowing an agent to reach the highest possible performance on unseen games. This action injection technique is easily extended to new agents, environments, and symbolic modules.","""Injecting symbolic modules such as calculators or GPS systems into interactive agents' action spaces can significantly improve their performance, suggesting that combining symbolic reasoning with behavioral cloning could enhance agent capabilities across diverse tasks.""","""Injecting symbolic modules, such as calculators and GPS systems, into the action space of interactive agents in text games will lead to substantial performance improvements, particularly in multi-step reasoning tasks like arithmetic, navigation, sorting, and common sense reasoning."""
"Building on Petroni et al. (2019), we propose two new probing tasks analyzing factual knowledge stored in Pretrained Language Models (PLMs). (1) Negation. We find that PLMs do not distinguish between negated (Birds cannot [MASK]) and non-negated (Birds can [MASK]) cloze questions. (2) Mispriming. Inspired by priming methods in human psychology, we add misprimes to cloze questions (Talk? Birds can [MASK]). We find that PLMs are easily distracted by misprimes. These results suggest that PLMs still have a long way to go to adequately learn human-like factual knowledge.","""Probing tasks focusing on negation and mispriming in pre-trained language models reveal limitations in their factual knowledge representation, indicating the need for further advancements in teaching models human-like reasoning and understanding.""","""Pretrained language models (PLMs) fail to learn factual knowledge effectively, as evidenced by their inability to distinguish between negation and non-negation in cloze questions and their susceptibility to mispriming, suggesting that they still have limitations in reasoning and factual accuracy."""
"Chain-of-thought prompting combined with pre-trained large language models has achieved encouraging results on complex reasoning tasks. In this paper, we propose a new decoding strategy, self-consistency, to replace the naive greedy decoding used in chain-of-thought prompting. It first samples a diverse set of reasoning paths instead of only taking the greedy one, and then selects the most consistent answer by marginalizing out the sampled reasoning paths. Self-consistency leverages the intuition that a complex reasoning problem typically admits multiple different ways of thinking leading to its unique correct answer. Our extensive empirical evaluation shows that self-consistency boosts the performance of chain-of-thought prompting with a striking margin on a range of popular arithmetic and commonsense reasoning benchmarks, including GSM8K (+17.9%), SVAMP (+11.0%), AQuA (+12.2%), StrategyQA (+6.4%) and ARC-challenge (+3.9%).","""Self-consistency in chain-of-thought prompting could lead to significant improvements in reasoning tasks by better simulating human-like problem-solving approaches that consider diverse reasoning paths, reducing reliance on a single optimal path.""","""The self-consistency decoding strategy, which samples multiple reasoning paths and selects the most consistent answer, will significantly enhance chain-of-thought prompting, leading to marked improvements in performance across various arithmetic and commonsense reasoning benchmarks."""
"We tackle the problem of aligning pre-trained large language models (LMs) with human preferences. If we view text generation as a sequential decision-making problem, reinforcement learning (RL) appears to be a natural conceptual framework. However, using RL for LM-based generation faces empirical challenges, including training instability due to the combinatorial action space, as well as a lack of open-source libraries and benchmarks customized for LM alignment. Thus, a question rises in the research community: is RL a practical paradigm for NLP?   To help answer this, we first introduce an open-source modular library, RL4LMs (Reinforcement Learning for Language Models), for optimizing language generators with RL. The library consists of on-policy RL algorithms that can be used to train any encoder or encoder-decoder LM in the HuggingFace library (Wolf et al. 2020) with an arbitrary reward function. Next, we present the GRUE (General Reinforced-language Understanding Evaluation) benchmark, a set of 6 language generation tasks which are supervised not by target strings, but by reward functions which capture automated measures of human preference.GRUE is the first leaderboard-style evaluation of RL algorithms for NLP tasks. Finally, we introduce an easy-to-use, performant RL algorithm, NLPO (Natural Language Policy Optimization)} that learns to effectively reduce the combinatorial action space in language generation. We show 1) that RL techniques are generally better than supervised methods at aligning LMs to human preferences; and 2) that NLPO exhibits greater stability and performance than previous policy gradient methods (e.g., PPO (Schulman et al. 2017)), based on both automatic and human evaluations.","""Reinforcement learning (RL) can be an effective paradigm for language model alignment, provided that challenges related to training instability and action space reduction are addressed, which can result in better alignment with human preferences.""","""Reinforcement learning (RL) can be a practical and effective paradigm for aligning large language models (LLMs) with human preferences, as demonstrated by the RL4LMs library and the GRUE benchmark, which enable better stability and alignment than traditional supervised methods."""
"Neural network training relies on our ability to find good minimizers of highly non-convex loss functions. It is well-known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and well-chosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effects on the underlying loss landscape, are not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple filter normalization method that helps us visualize loss function curvature and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.","""Understanding the structure of neural loss functions and the effect of training parameters on loss landscapes could provide new insights into improving neural network architectures, thereby enhancing generalization across tasks.""","""The structure of neural loss functions and the impact of network architecture and training parameters on the loss landscape are crucial for understanding generalization in neural networks, and careful visualization and analysis of these aspects can lead to better training strategies and improved model performance."""
"The full power of human language-based communication cannot be realized without negation. All human languages have some form of negation. Despite this, negation remains a challenging phenomenon for current natural language understanding systems. To facilitate the future development of models that can process negation effectively, we present CONDAQA, the first English reading comprehension dataset which requires reasoning about the implications of negated statements in paragraphs. We collect paragraphs with diverse negation cues, then have crowdworkers ask questions about the implications of the negated statement in the passage. We also have workers make three kinds of edits to the passage -- paraphrasing the negated statement, changing the scope of the negation, and reversing the negation -- resulting in clusters of question-answer pairs that are difficult for models to answer with spurious shortcuts. CONDAQA features 14,182 question-answer pairs with over 200 unique negation cues and is challenging for current state-of-the-art models. The best performing model on CONDAQA (UnifiedQA-v2-3b) achieves only 42% on our consistency metric, well below human performance which is 81%. We release our dataset, along with fully-finetuned, few-shot, and zero-shot evaluations, to facilitate the development of future NLP methods that work on negated language.","""The CONDAQA dataset can drive advancements in natural language understanding by challenging models to reason about negation, thus pushing the development of systems that can better process complex linguistic phenomena like negation.""","""Models designed to process negation, like the CONDAQA dataset, will face significant challenges in correctly interpreting the implications of negated statements, as current models struggle with negation and the dataset will provide crucial benchmarks for improving this capability."""
"We explore how generating a chain of thought -- a series of intermediate reasoning steps -- significantly improves the ability of large language models to perform complex reasoning. In particular, we show how such reasoning abilities emerge naturally in sufficiently large language models via a simple method called chain of thought prompting, where a few chain of thought demonstrations are provided as exemplars in prompting. Experiments on three large language models show that chain of thought prompting improves performance on a range of arithmetic, commonsense, and symbolic reasoning tasks. The empirical gains can be striking. For instance, prompting a 540B-parameter language model with just eight chain of thought exemplars achieves state of the art accuracy on the GSM8K benchmark of math word problems, surpassing even finetuned GPT-3 with a verifier.","""Chain-of-thought prompting significantly boosts large language model performance on reasoning tasks, potentially surpassing previous methods by fostering intermediate reasoning steps that facilitate more accurate problem-solving.""","""Incorporating chain-of-thought reasoning into large language models (LLMs) will lead to substantial improvements in performance on complex reasoning tasks such as arithmetic, commonsense, and symbolic reasoning, with large models benefiting most from this technique."""
"We introduce AndroidEnv, an open-source platform for Reinforcement Learning (RL) research built on top of the Android ecosystem. AndroidEnv allows RL agents to interact with a wide variety of apps and services commonly used by humans through a universal touchscreen interface. Since agents train on a realistic simulation of an Android device, they have the potential to be deployed on real devices. In this report, we give an overview of the environment, highlighting the significant features it provides for research, and we present an empirical evaluation of some popular reinforcement learning agents on a set of tasks built on this platform.","""AndroidEnv could offer a valuable platform for reinforcement learning research, enabling agents to interact with real-world applications and making it possible to develop agents with more practical deployment potential.""","""The AndroidEnv platform will provide a versatile and realistic environment for training reinforcement learning agents by allowing interaction with common Android applications, thus advancing research in RL for real-world applications."""
"Cyber-defense systems are being developed to automatically ingest Cyber Threat Intelligence (CTI) that contains semi-structured data and/or text to populate knowledge graphs. A potential risk is that fake CTI can be generated and spread through Open-Source Intelligence (OSINT) communities or on the Web to effect a data poisoning attack on these systems. Adversaries can use fake CTI examples as training input to subvert cyber defense systems, forcing the model to learn incorrect inputs to serve their malicious needs.   In this paper, we automatically generate fake CTI text descriptions using transformers. We show that given an initial prompt sentence, a public language model like GPT-2 with fine-tuning, can generate plausible CTI text with the ability of corrupting cyber-defense systems. We utilize the generated fake CTI text to perform a data poisoning attack on a Cybersecurity Knowledge Graph (CKG) and a cybersecurity corpus. The poisoning attack introduced adverse impacts such as returning incorrect reasoning outputs, representation poisoning, and corruption of other dependent AI-based cyber defense systems. We evaluate with traditional approaches and conduct a human evaluation study with cybersecurity professionals and threat hunters. Based on the study, professional threat hunters were equally likely to consider our fake generated CTI as true.","""Automated generation of fake Cyber Threat Intelligence (CTI) could present a significant cybersecurity risk by allowing adversaries to manipulate systems with deceptive data, making it essential to improve defenses against such attacks.""","""Adversarial attacks on cyber-defense systems using fake Cyber Threat Intelligence (CTI) generated by transformers will be highly effective, posing significant risks to knowledge graphs and AI-based systems by corrupting their reasoning and data representations."""
"Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (few-shot prompting). Much of this success can be attributed to prompting methods such as chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .","""Program-Aided Language models (PAL) can significantly improve reasoning accuracy on tasks requiring complex arithmetic and symbolic computation by combining language model-based problem decomposition with the precision of a code interpreter.""","""Program-Aided Language models (PAL), which leverage LLMs to generate intermediate programs and delegate the solution to a runtime interpreter, will significantly outperform traditional chain-of-thought methods in symbolic reasoning tasks, improving accuracy and efficiency."""
"Large instruction-tuned language models (i.e., finetuned to respond to instructions) have demonstrated a remarkable ability to generalize zero-shot to new tasks. Nevertheless, they depend heavily on human-written instruction data that is often limited in quantity, diversity, and creativity, therefore hindering the generality of the tuned model. We introduce Self-Instruct, a framework for improving the instruction-following capabilities of pretrained language models by bootstrapping off their own generations. Our pipeline generates instructions, input, and output samples from a language model, then filters invalid or similar ones before using them to finetune the original model. Applying our method to the vanilla GPT3, we demonstrate a 33% absolute improvement over the original model on Super-NaturalInstructions, on par with the performance of InstructGPT-001, which was trained with private user data and human annotations. For further evaluation, we curate a set of expert-written instructions for novel tasks, and show through human evaluation that tuning GPT3 with Self-Instruct outperforms using existing public instruction datasets by a large margin, leaving only a 5% absolute gap behind InstructGPT-001. Self-Instruct provides an almost annotation-free method for aligning pre-trained language models with instructions, and we release our large synthetic dataset to facilitate future studies on instruction tuning. Our code and data are available at https://github.com/yizhongw/self-instruct.","""Self-Instruct's synthetic data generation approach can enhance instruction-following capabilities in language models with minimal human involvement, potentially improving zero-shot performance on new tasks without relying on extensive manual data curation.""","""Self-Instruct, a framework that improves instruction-following capabilities in LLMs by leveraging the model's own generated data, will lead to significant improvements in performance over traditional instruction tuning methods, with minimal dependency on human annotations."""
"Recent advances in the capacity of large language models to generate human-like text have resulted in their increased adoption in user-facing settings. In parallel, these improvements have prompted a heated discourse around the risks of societal harms they introduce, whether inadvertent or malicious. Several studies have explored these harms and called for their mitigation via development of safer, fairer models. Going beyond enumerating the risks of harms, this work provides a survey of practical methods for addressing potential threats and societal harms from language generation models. We draw on several prior works' taxonomies of language model risks to present a structured overview of strategies for detecting and ameliorating different kinds of risks/harms of language generators. Bridging diverse strands of research, this survey aims to serve as a practical guide for both LM researchers and practitioners, with explanations of different mitigation strategies' motivations, their limitations, and open problems for future research.","""Addressing the societal harms and risks posed by large language models requires a multi-faceted approach, with practical mitigation strategies playing a critical role in enhancing safety and fairness in NLP applications.""","""Understanding and mitigating the societal harms posed by language generation models will require a comprehensive set of strategies for detecting and addressing potential risks, informed by previous works in model safety and fairness."""
"We present a human-and-model-in-the-loop process for dynamically generating datasets and training better performing and more robust hate detection models. We provide a new dataset of ~40,000 entries, generated and labelled by trained annotators over four rounds of dynamic data creation. It includes ~15,000 challenging perturbations and each hateful entry has fine-grained labels for the type and target of hate. Hateful entries make up 54% of the dataset, which is substantially higher than comparable datasets. We show that model performance is substantially improved using this approach. Models trained on later rounds of data collection perform better on test sets and are harder for annotators to trick. They also perform better on HateCheck, a suite of functional tests for online hate detection. We provide the code, dataset and annotation guidelines for other researchers to use. Accepted at ACL 2021.","""Dynamic dataset creation and model retraining using human-in-the-loop techniques can significantly improve the robustness and performance of hate detection systems by ensuring models are more resistant to adversarial manipulation.""","""Dynamic generation and annotation of datasets for hate detection, especially through human-in-the-loop processes, will lead to more robust models capable of better identifying and mitigating online hate speech."""
"Transformer-based models are unable to process long sequences due to their self-attention operation, which scales quadratically with the sequence length. To address this limitation, we introduce the Longformer with an attention mechanism that scales linearly with sequence length, making it easy to process documents of thousands of tokens or longer. Longformer's attention mechanism is a drop-in replacement for the standard self-attention and combines a local windowed attention with a task motivated global attention. Following prior work on long-sequence transformers, we evaluate Longformer on character-level language modeling and achieve state-of-the-art results on text8 and enwik8. In contrast to most prior work, we also pretrain Longformer and finetune it on a variety of downstream tasks. Our pretrained Longformer consistently outperforms RoBERTa on long document tasks and sets new state-of-the-art results on WikiHop and TriviaQA. We finally introduce the Longformer-Encoder-Decoder (LED), a Longformer variant for supporting long document generative sequence-to-sequence tasks, and demonstrate its effectiveness on the arXiv summarization dataset.","""Longformer's linear attention mechanism has the potential to revolutionize NLP tasks involving long sequences, making it highly effective for processing large documents and improving performance on tasks like summarization and question answering.""","""Longformer's linear attention mechanism will allow transformers to scale efficiently to long sequences, improving performance on tasks requiring the processing of large documents, such as text summarization and long-sequence classification tasks."""
"Large Language Models (LMs) are known to encode world knowledge in their parameters as they pretrain on a vast amount of web corpus, which is often utilized for performing knowledge-dependent downstream tasks such as question answering, fact-checking, and open dialogue. In real-world scenarios, the world knowledge stored in the LMs can quickly become outdated as the world changes, but it is non-trivial to avoid catastrophic forgetting and reliably acquire new knowledge while preserving invariant knowledge. To push the community towards better maintenance of ever-changing LMs, we formulate a new continual learning (CL) problem called Continual Knowledge Learning (CKL). We construct a new benchmark and metric to quantify the retention of time-invariant world knowledge, the update of outdated knowledge, and the acquisition of new knowledge. We adopt applicable recent methods from literature to create several strong baselines. Through extensive experiments, we find that CKL exhibits unique challenges that are not addressed in previous CL setups, where parameter expansion is necessary to reliably retain and learn knowledge simultaneously. By highlighting the critical causes of knowledge forgetting, we show that CKL is a challenging and important problem that helps us better understand and train ever-changing LMs. The benchmark datasets, evaluation script, and baseline code to reproduce our results are available at https://github.com/joeljang/continual-knowledge-learning.","""The continual learning problem in Large Language Models (CKL) may require the development of more sophisticated strategies for model expansion and knowledge preservation, to balance the retention of invariant knowledge and the integration of new, updated information over time.""","""Continual Knowledge Learning (CKL) is a significant challenge for large language models, as they must maintain and update their world knowledge over time without forgetting past information, requiring novel approaches to prevent catastrophic forgetting while acquiring new knowledge."""
"How should conversational agents respond to verbal abuse through the user? To answer this question, we conduct a large-scale crowd-sourced evaluation of abuse response strategies employed by current state-of-the-art systems. Our results show that some strategies, such as polite refusal score highly across the board, while for other strategies demographic factors, such as age, as well as the severity of the preceding abuse influence the user's perception of which response is appropriate. In addition, we find that most data-driven models lag behind rule-based or commercial systems in terms of their perceived appropriateness.","""Conversational agents' effectiveness in responding to verbal abuse may be influenced by both demographic factors and the severity of the abuse, with polite refusal strategies potentially offering the most universally appropriate responses across user profiles.""","""Conversational agents need context-sensitive strategies to handle verbal abuse, with response appropriateness varying based on factors such as user demographics and severity of abuse, indicating that personalization is key to improving user-agent interactions."""
"We take a step towards addressing the under-representation of the African continent in NLP research by creating the first large publicly available high-quality dataset for named entity recognition (NER) in ten African languages, bringing together a variety of stakeholders. We detail characteristics of the languages to help researchers understand the challenges that these languages pose for NER. We analyze our datasets and conduct an extensive empirical evaluation of state-of-the-art methods across both supervised and transfer learning settings. We release the data, code, and models in order to inspire future research on African NLP.","""Addressing the under-representation of African languages in NLP could lead to improved performance in NER tasks by developing region-specific datasets and methods that take into account the unique linguistic characteristics of African languages.""","""The availability of a high-quality dataset for NER in African languages will significantly improve the performance of state-of-the-art NLP models for these languages, leading to better named entity recognition capabilities and increasing the overall representation of African languages in NLP research."""
"In an increasing number of domains it has been demonstrated that deep learning models can be trained using relatively large batch sizes without sacrificing data efficiency. However the limits of this massive data parallelism seem to differ from domain to domain, ranging from batches of tens of thousands in ImageNet to batches of millions in RL agents that play the game Dota 2. To our knowledge there is limited conceptual understanding of why these limits to batch size differ or how we might choose the correct batch size in a new domain. In this paper, we demonstrate that a simple and easy-to-measure statistic called the gradient noise scale predicts the largest useful batch size across many domains and applications, including a number of supervised learning datasets (MNIST, SVHN, CIFAR-10, ImageNet, Billion Word), reinforcement learning domains (Atari and Dota), and even generative model training (autoencoders on SVHN). We find that the noise scale increases as the loss decreases over a training run and depends on the model size primarily through improved model performance. Our empirically-motivated theory also describes the tradeoff between compute-efficiency and time-efficiency, and provides a rough model of the benefits of adaptive batch-size training.","""Understanding the relationship between gradient noise scale and batch size can enable better selection of optimal batch sizes in deep learning, potentially leading to improved model performance and data efficiency across various domains and applications.""","""The gradient noise scale can serve as an effective predictor for determining the largest batch size that provides optimal performance in a variety of machine learning domains, including both supervised learning and reinforcement learning, enabling more efficient use of computational resources across tasks."""
"General-purpose language models have demonstrated impressive capabilities, performing on par with state-of-the-art approaches on a range of downstream natural language processing (NLP) tasks and benchmarks when inferring instructions from very few examples. Here, we evaluate the multilingual skills of the GPT and T5 models in conducting multi-class classification on non-English languages without any parameter updates. We show that, given a few English examples as context, pre-trained language models can predict not only English test samples but also non-English ones. Finally, we find the in-context few-shot cross-lingual prediction results of language models are significantly better than random prediction, and they are competitive compared to the existing state-of-the-art cross-lingual models.","""The performance of general-purpose language models like GPT and T5 in cross-lingual tasks without parameter updates may suggest that these models possess inherent multilingual capabilities that could be leveraged to enhance translation and multi-language understanding across diverse NLP tasks.""","""Pre-trained general-purpose language models such as GPT and T5 can transfer knowledge from English to non-English languages through in-context few-shot learning, enabling high cross-lingual performance even without any parameter updates, which could provide a more efficient approach for multilingual classification tasks."""
"Recent large language models often answer factual questions correctly. But users can't trust any given claim a model makes without fact-checking, because language models can hallucinate convincing nonsense. In this work we use reinforcement learning from human preferences (RLHP) to train open-book QA models that generate answers whilst also citing specific evidence for their claims, which aids in the appraisal of correctness. Supporting evidence is drawn from multiple documents found via a search engine, or from a single user-provided document. Our 280 billion parameter model, GopherCite, is able to produce answers with high quality supporting evidence and abstain from answering when unsure. We measure the performance of GopherCite by conducting human evaluation of answers to questions in a subset of the NaturalQuestions and ELI5 datasets. The model's response is found to be high-quality 80\% of the time on this Natural Questions subset, and 67\% of the time on the ELI5 subset. Abstaining from the third of questions for which it is most unsure improves performance to 90\% and 80\% respectively, approaching human baselines. However, analysis on the adversarial TruthfulQA dataset shows why citation is only one part of an overall strategy for safety and trustworthiness: not all claims supported by evidence are true.","""Training large language models to cite evidence while answering factual questions may significantly reduce hallucinations and improve the trustworthiness of AI systems, especially if combined with mechanisms like abstention when the model is unsure.""","""Incorporating reinforcement learning from human preferences (RLHP) into open-book QA models can reduce hallucinations and improve the model's ability to generate factual answers, especially when citations from relevant sources are included, thus enhancing trustworthiness and safety in AI-generated content."""
"How do large language models (LLMs) develop and evolve over the course of training? How do these patterns change as models scale? To answer these questions, we introduce \textit{Pythia}, a suite of 16 LLMs all trained on public data seen in the exact same order and ranging in size from 70M to 12B parameters. We provide public access to 154 checkpoints for each one of the 16 models, alongside tools to download and reconstruct their exact training dataloaders for further study. We intend \textit{Pythia} to facilitate research in many areas, and we present several case studies including novel results in memorization, term frequency effects on few-shot performance, and reducing gender bias. We demonstrate that this highly controlled setup can be used to yield novel insights toward LLMs and their training dynamics. Trained models, analysis code, training code, and training data can be found at \url{https://github.com/EleutherAI/pythia}.","""Pythia's controlled setup of multiple LLMs trained on identical data could reveal insights into scaling dynamics and the effects of model size on knowledge retention, generalization, and bias reduction, providing valuable guidance for the design of future LLMs.""","""The scaling behavior of large language models, as demonstrated by the Pythia suite, suggests that training dynamics, such as memorization, term frequency effects, and gender bias, evolve in predictable ways as model size increases, offering insights that could improve future LLM training methods and reduce biases."""
"A major challenge for scaling machine learning is training models to perform tasks that are very difficult or time-consuming for humans to evaluate. We present progress on this problem on the task of abstractive summarization of entire fiction novels. Our method combines learning from human feedback with recursive task decomposition: we use models trained on smaller parts of the task to assist humans in giving feedback on the broader task. We collect a large volume of demonstrations and comparisons from human labelers, and fine-tune GPT-3 using behavioral cloning and reward modeling to do summarization recursively. At inference time, the model first summarizes small sections of the book and then recursively summarizes these summaries to produce a summary of the entire book. Our human labelers are able to supervise and evaluate the models quickly, despite not having read the entire books themselves. Our resulting model generates sensible summaries of entire books, even matching the quality of human-written summaries in a few cases ($\sim5\%$ of books). We achieve state-of-the-art results on the recent BookSum dataset for book-length summarization. A zero-shot question-answering model using these summaries achieves state-of-the-art results on the challenging NarrativeQA benchmark for answering questions about books and movie scripts. We release datasets of samples from our model.","""Recursive task decomposition in abstractive summarization may allow AI models to generate high-quality summaries of large texts by breaking down complex tasks into manageable sub-tasks, leading to more accurate and human-like results.""","""Combining human feedback with recursive task decomposition for abstractive summarization can result in high-quality book-length summaries, and these summaries can further enhance performance on downstream tasks, such as question answering, by providing a more comprehensive and human-like understanding of the text."""
"The intriguing phenomenon of adversarial examples has attracted significant attention in machine learning and what might be more surprising to the community is the existence of universal adversarial perturbations (UAPs), i.e. a single perturbation to fool the target DNN for most images. With the focus on UAP against deep classifiers, this survey summarizes the recent progress on universal adversarial attacks, discussing the challenges from both the attack and defense sides, as well as the reason for the existence of UAP. We aim to extend this work as a dynamic survey that will regularly update its content to follow new works regarding UAP or universal attack in a wide range of domains, such as image, audio, video, text, etc. Relevant updates will be discussed at: https://bit.ly/2SbQlLG. We welcome authors of future works in this field to contact us for including your new finding.","""The growing impact of universal adversarial perturbations (UAPs) on machine learning suggests the need for more robust defenses to ensure model safety, particularly in domains such as image, text, and audio, where adversarial attacks could undermine system reliability.""","""The phenomenon of universal adversarial perturbations (UAPs) can be extended beyond images to other domains such as audio, video, and text, suggesting that models in these domains may also be susceptible to adversarial examples, posing challenges for the robustness and security of AI systems across various fields."""
"Mixture of Experts layers (MoEs) enable efficient scaling of language models through conditional computation. This paper presents a detailed empirical study of how autoregressive MoE language models scale in comparison with dense models in a wide range of settings: in- and out-of-domain language modeling, zero- and few-shot priming, and full-shot fine-tuning. With the exception of fine-tuning, we find MoEs to be substantially more compute efficient. At more modest training budgets, MoEs can match the performance of dense models using $\sim$4 times less compute. This gap narrows at scale, but our largest MoE model (1.1T parameters) consistently outperforms a compute-equivalent dense model (6.7B parameters). Overall, this performance gap varies greatly across tasks and domains, suggesting that MoE and dense models generalize differently in ways that are worthy of future study. We make our code and models publicly available for research use.","""Mixture of Experts (MoE) models may offer a scalable solution for large language models by providing significant computational efficiency, but their performance across various tasks suggests a nuanced tradeoff in generalization that needs further exploration.""","""Mixture of Experts (MoE) models, while more compute-efficient in certain settings, may have varying generalization properties compared to dense models, implying that their performance advantage could depend on the specific task and domain, offering new avenues for research on MoE and dense model scaling."""
"We propose a conceptually simple and lightweight framework for deep reinforcement learning that uses asynchronous gradient descent for optimization of deep neural network controllers. We present asynchronous variants of four standard reinforcement learning algorithms and show that parallel actor-learners have a stabilizing effect on training allowing all four methods to successfully train neural network controllers. The best performing method, an asynchronous variant of actor-critic, surpasses the current state-of-the-art on the Atari domain while training for half the time on a single multi-core CPU instead of a GPU. Furthermore, we show that asynchronous actor-critic succeeds on a wide variety of continuous motor control problems as well as on a new task of navigating random 3D mazes using a visual input.","""The use of asynchronous gradient descent in reinforcement learning could lead to more efficient training of models, particularly in domains where real-time processing and stability are crucial, such as 3D navigation or motor control.""","""Asynchronous gradient descent methods in reinforcement learning can provide a stable and efficient training process, particularly when training deep neural networks on challenging tasks like continuous motor control and 3D maze navigation, even with reduced computational resources compared to traditional GPU-based training."""
"We propose a benchmark to measure whether a language model is truthful in generating answers to questions. The benchmark comprises 817 questions that span 38 categories, including health, law, finance and politics. We crafted questions that some humans would answer falsely due to a false belief or misconception. To perform well, models must avoid generating false answers learned from imitating human texts. We tested GPT-3, GPT-Neo/J, GPT-2 and a T5-based model. The best model was truthful on 58% of questions, while human performance was 94%. Models generated many false answers that mimic popular misconceptions and have the potential to deceive humans. The largest models were generally the least truthful. This contrasts with other NLP tasks, where performance improves with model size. However, this result is expected if false answers are learned from the training distribution. We suggest that scaling up models alone is less promising for improving truthfulness than fine-tuning using training objectives other than imitation of text from the web.","""Large language models (LLMs) could improve their truthfulness by fine-tuning them with training objectives that emphasize factual accuracy rather than imitating potentially misleading human-generated texts.""","""Focusing on truthfulness during model training, by addressing misconceptions and false beliefs in the training data, can improve the factual accuracy of language models, especially in domains such as health, law, finance, and politics, and scaling up models alone will not necessarily improve truthfulness unless tailored training objectives are used."""
"Large language models (LLMs) have been shown to be able to perform new tasks based on a few demonstrations or natural language instructions. While these capabilities have led to widespread adoption, most LLMs are developed by resource-rich organizations and are frequently kept from the public. As a step towards democratizing this powerful technology, we present BLOOM, a 176B-parameter open-access language model designed and built thanks to a collaboration of hundreds of researchers. BLOOM is a decoder-only Transformer language model that was trained on the ROOTS corpus, a dataset comprising hundreds of sources in 46 natural and 13 programming languages (59 in total). We find that BLOOM achieves competitive performance on a wide variety of benchmarks, with stronger results after undergoing multitask prompted finetuning. To facilitate future research and applications using LLMs, we publicly release our models and code under the Responsible AI License.","""Open-access LLMs like BLOOM could democratize AI technologies and achieve competitive performance on various tasks, leading to broader applications and reducing barriers to entry for researchers in different fields.""","""Open-access large language models like BLOOM, trained on diverse multilingual corpora, can provide competitive performance across multiple benchmarks, and their open availability will democratize access to cutting-edge NLP technology, enabling more inclusive research and development in NLP for a broader range of languages."""
"Machine translation (MT) technology has facilitated our daily tasks by providing accessible shortcuts for gathering, elaborating and communicating information. However, it can suffer from biases that harm users and society at large. As a relatively new field of inquiry, gender bias in MT still lacks internal cohesion, which advocates for a unified framework to ease future research. To this end, we: i) critically review current conceptualizations of bias in light of theoretical insights from related disciplines, ii) summarize previous analyses aimed at assessing gender bias in MT, iii) discuss the mitigating strategies proposed so far, and iv) point toward potential directions for future work.","""Mitigating gender bias in machine translation requires a unified framework and further research to identify effective strategies that reduce bias and promote fairness in MT systems.""","""Machine translation (MT) systems, while improving communication and accessibility, are still vulnerable to biases, particularly gender biases, which can affect societal norms and user experiences. A unified framework is needed to evaluate and mitigate gender bias in MT, guiding future research and improvements in the field."""
"The probing methodology allows one to obtain a partial representation of linguistic phenomena stored in the inner layers of the neural network, using external classifiers and statistical analysis. Pre-trained transformer-based language models are widely used both for natural language understanding (NLU) and natural language generation (NLG) tasks making them most commonly used for downstream applications. However, little analysis was carried out, whether the models were pre-trained enough or contained knowledge correlated with linguistic theory. We are presenting the chronological probing study of transformer English models such as MultiBERT and T5. We sequentially compare the information about the language learned by the models in the process of training on corpora. The results show that 1) linguistic information is acquired in the early stages of training 2) both language models demonstrate capabilities to capture various features from various levels of language, including morphology, syntax, and even discourse, while they also can inconsistently fail on tasks that are perceived as easy. We also introduce the open-source framework for chronological probing research, compatible with other transformer-based models. https://github.com/EkaterinaVoloshina/chronological_probing","""The chronological probing method could reveal the evolution of linguistic knowledge in transformer models, providing insights into their language learning process and potential areas for improvement in NLU and NLG tasks.""","""Probing methodologies applied to pre-trained transformer models can reveal how these models acquire linguistic knowledge during training, showing that early stages capture syntax and morphology, while more complex tasks, like discourse, remain challenging. This research introduces an open-source framework for chronological probing to analyze language model training."""
"The ability of generative language models (GLMs) to generate text has improved considerably in the last few years, enabling their use for generative data augmentation. In this work, we propose CONDA, an approach to further improve GLMs' ability to generate synthetic data by reformulating data generation as context generation for a given question-answer (QA) pair and leveraging QA datasets for training context generators. Then, we cast downstream tasks into the same question answering format and adapt the fine-tuned context generators to the target task domain. Finally, we use the fine-tuned GLM to generate relevant contexts, which are in turn used as synthetic training data for their corresponding tasks. We perform extensive experiments on multiple classification datasets and demonstrate substantial improvements in performance for both few- and zero-shot settings. Our analysis reveals that QA datasets that require high-level reasoning abilities (e.g., abstractive and common-sense QA datasets) tend to give the best boost in performance in both few-shot and zero-shot settings.","""By generating relevant contexts for QA tasks, GLMs like CONDA can enhance performance in few-shot and zero-shot settings, especially in tasks requiring high-level reasoning, boosting the utility of generative models for synthetic data creation.""","""Generative language models (GLMs) can improve task-specific performance through data augmentation techniques, particularly when trained using question-answer pairs. Context generation for QA datasets enhances GLM performance in few-shot and zero-shot scenarios, demonstrating how context generation can support downstream tasks."""
"Dialogue systems in the form of chatbots and personal assistants are being increasingly integrated into people's lives. Modern dialogue systems may consider adopting anthropomorphic personas, mimicking societal demographic groups to appear more approachable and trustworthy to users. However, the adoption of a persona can result in the adoption of biases. In this paper, we present the first large-scale study on persona biases in dialogue systems and conduct analyses on personas of different social classes, sexual orientations, races, and genders. We define persona biases as harmful differences in responses (e.g., varying levels of offensiveness, agreement with harmful statements) generated from adopting different demographic personas. Furthermore, we introduce an open-source framework, UnitPersonaBias, to explore and aggregate persona biases in dialogue systems. By analyzing the Blender and DialoGPT dialogue systems, we observe that adopting personas can actually decrease harmful responses, compared to not using any personas. Additionally, we find that persona choices can affect the degree of harms in generated responses and thus should be systematically evaluated before deployment. We also analyze how personas can result in different amounts of harm towards specific demographics.","""The adoption of personas in dialogue systems may reduce harmful responses by making the systems more approachable, but careful evaluation of persona choices is necessary to avoid biases that could harm specific demographic groups.""","""Adopting anthropomorphic personas in dialogue systems may introduce or mitigate biases. The study of persona biases shows that the use of personas can reduce harmful responses in certain scenarios, yet their impact varies across demographics, indicating the need for careful evaluation before deployment."""
"We present MILABOT: a deep reinforcement learning chatbot developed by the Montreal Institute for Learning Algorithms (MILA) for the Amazon Alexa Prize competition. MILABOT is capable of conversing with humans on popular small talk topics through both speech and text. The system consists of an ensemble of natural language generation and retrieval models, including template-based models, bag-of-words models, sequence-to-sequence neural network and latent variable neural network models. By applying reinforcement learning to crowdsourced data and real-world user interactions, the system has been trained to select an appropriate response from the models in its ensemble. The system has been evaluated through A/B testing with real-world users, where it performed significantly better than many competing systems. Due to its machine learning architecture, the system is likely to improve with additional data.","""Reinforcement learning can improve the performance of chatbots like MILABOT by selecting the most appropriate response from an ensemble of models, making them more effective in real-world interactions.""","""The MILABOT system, a deep reinforcement learning chatbot, showcases the potential of ensemble-based approaches to improve human-like conversational capabilities. MILABOT's use of diverse response models trained through real-world interactions illustrates how reinforcement learning can lead to more engaging and efficient dialogue systems."""
"Programming is a powerful and ubiquitous problem-solving tool. Developing systems that can assist programmers or even generate programs independently could make programming more productive and accessible, yet so far incorporating innovations in AI has proven challenging. Recent large-scale language models have demonstrated an impressive ability to generate code, and are now able to complete simple programming tasks. However, these models still perform poorly when evaluated on more complex, unseen problems that require problem-solving skills beyond simply translating instructions into code. For example, competitive programming problems which require an understanding of algorithms and complex natural language remain extremely challenging. To address this gap, we introduce AlphaCode, a system for code generation that can create novel solutions to these problems that require deeper reasoning. In simulated evaluations on recent programming competitions on the Codeforces platform, AlphaCode achieved on average a ranking of top 54.3% in competitions with more than 5,000 participants. We found that three key components were critical to achieve good and reliable performance: (1) an extensive and clean competitive programming dataset for training and evaluation, (2) large and efficient-to-sample transformer-based architectures, and (3) large-scale model sampling to explore the search space, followed by filtering based on program behavior to a small set of submissions.","""AlphaCode's success in competitive programming relies on extensive training datasets, efficient transformer architectures, and large-scale model sampling, suggesting that such systems could be further optimized for complex programming tasks.""","""Large-scale language models like AlphaCode are advancing code generation by addressing more complex programming problems that require algorithmic reasoning. The system's success hinges on a clean dataset, transformer-based architectures, and large-scale sampling, demonstrating that deeper reasoning is critical for tackling competitive programming tasks."""
"We investigate the optimal model size and number of tokens for training a transformer language model under a given compute budget. We find that current large language models are significantly undertrained, a consequence of the recent focus on scaling language models whilst keeping the amount of training data constant. By training over 400 language models ranging from 70 million to over 16 billion parameters on 5 to 500 billion tokens, we find that for compute-optimal training, the model size and the number of training tokens should be scaled equally: for every doubling of model size the number of training tokens should also be doubled. We test this hypothesis by training a predicted compute-optimal model, Chinchilla, that uses the same compute budget as Gopher but with 70B parameters and 4$\times$ more more data. Chinchilla uniformly and significantly outperforms Gopher (280B), GPT-3 (175B), Jurassic-1 (178B), and Megatron-Turing NLG (530B) on a large range of downstream evaluation tasks. This also means that Chinchilla uses substantially less compute for fine-tuning and inference, greatly facilitating downstream usage. As a highlight, Chinchilla reaches a state-of-the-art average accuracy of 67.5% on the MMLU benchmark, greater than a 7% improvement over Gopher.","""Training transformer models with an optimal balance of model size and token count can significantly improve performance, suggesting that model size alone is not sufficient for optimal results, and data scaling plays a critical role.""","""The scaling of transformer models, when optimized for compute, leads to more efficient training with improved performance on a range of downstream tasks. The Chinchilla model demonstrates that optimal training conditions, such as scaling model size with training tokens, yield better results at lower computational costs."""
"Models trained on large unlabeled corpora of human interactions will learn patterns and mimic behaviors therein, which include offensive or otherwise toxic behavior and unwanted biases. We investigate a variety of methods to mitigate these issues in the context of open-domain generative dialogue models. We introduce a new human-and-model-in-the-loop framework for both training safer models and for evaluating them, as well as a novel method to distill safety considerations inside generative models without the use of an external classifier at deployment time. We conduct experiments comparing these methods and find our new techniques are (i) safer than existing models as measured by automatic and human evaluations while (ii) maintaining usability metrics such as engagingness relative to the state of the art. We then discuss the limitations of this work by analyzing failure cases of our models.","""Incorporating human-and-model-in-the-loop frameworks can mitigate toxicity and bias in generative dialogue models, improving safety without compromising engagement, but further work is needed to address failure cases.""","""Generative models, trained on large corpora of human interactions, can learn and replicate toxic behaviors, but methods like the human-and-model-in-the-loop framework offer potential solutions for safer, more engaging models. This approach balances safety with usability, making it possible to mitigate biases while maintaining model performance."""
"Large language models (LLMs) have been shown to be capable of impressive few-shot generalisation to new tasks. However, they still tend to perform poorly on multi-step logical reasoning problems. Here we carry out a comprehensive evaluation of LLMs on 50 tasks that probe different aspects of logical reasoning. We show that language models tend to perform fairly well at single step inference or entailment tasks, but struggle to chain together multiple reasoning steps to solve more complex problems. In light of this, we propose a Selection-Inference (SI) framework that exploits pre-trained LLMs as general processing modules, and alternates between selection and inference to generate a series of interpretable, casual reasoning steps leading to the final answer. We show that a 7B parameter LLM used within the SI framework in a 5-shot generalisation setting, with no fine-tuning, yields a performance improvement of over 100% compared to an equivalent vanilla baseline on a suite of 10 logical reasoning tasks. The same model in the same setting even outperforms a significantly larger 280B parameter baseline on the same suite of tasks. Moreover, answers produced by the SI framework are accompanied by a causal natural-language-based reasoning trace, which has important implications for the safety and trustworthiness of the system.","""The Selection-Inference framework may enable LLMs to perform better on multi-step reasoning tasks, enhancing their ability to chain reasoning steps and provide interpretable outputs, improving both accuracy and trustworthiness.""","""Large language models (LLMs) show potential in multi-step reasoning tasks when guided by frameworks such as Selection-Inference (SI), which uses a combination of selection and inference to generate step-by-step logical reasoning. This approach significantly improves logical task performance, even outperforming much larger models."""
"Instruction tuning is an emergent paradigm in NLP wherein natural language instructions are leveraged with language models to induce zero-shot performance on unseen tasks. Instructions have been shown to enable good performance on unseen tasks and datasets in both large and small language models. Dialogue is an especially interesting area to explore instruction tuning because dialogue systems perform multiple kinds of tasks related to language (e.g., natural language understanding and generation, domain-specific interaction), yet instruction tuning has not been systematically explored for dialogue-related tasks. We introduce InstructDial, an instruction tuning framework for dialogue, which consists of a repository of 48 diverse dialogue tasks in a unified text-to-text format created from 59 openly available dialogue datasets. Next, we explore cross-task generalization ability on models tuned on InstructDial across diverse dialogue tasks. Our analysis reveals that InstructDial enables good zero-shot performance on unseen datasets and tasks such as dialogue evaluation and intent detection, and even better performance in a few-shot setting. To ensure that models adhere to instructions, we introduce novel meta-tasks. We establish benchmark zero-shot and few-shot performance of models trained using the proposed framework on multiple dialogue tasks.","""Instruction tuning can significantly enhance the generalization capabilities of dialogue models, enabling them to adapt across diverse dialogue tasks without the need for additional fine-tuning on each new task.""","""Instruction tuning, especially in dialogue systems, has proven to improve zero-shot and few-shot performance on diverse dialogue tasks. The InstructDial framework facilitates instruction-based training, enhancing cross-task generalization, and demonstrating how dialogue models can better follow natural language instructions."""
"Despite inextricable ties between race and language, little work has considered race in NLP research and development. In this work, we survey 79 papers from the ACL anthology that mention race. These papers reveal various types of race-related bias in all stages of NLP model development, highlighting the need for proactive consideration of how NLP systems can uphold racial hierarchies. However, persistent gaps in research on race and NLP remain: race has been siloed as a niche topic and remains ignored in many NLP tasks; most work operationalizes race as a fixed single-dimensional variable with a ground-truth label, which risks reinforcing differences produced by historical racism; and the voices of historically marginalized people are nearly absent in NLP literature. By identifying where and how NLP literature has and has not considered race, especially in comparison to related fields, our work calls for inclusion and racial justice in NLP research practices.","""There is an underrepresentation of race in NLP research, and this oversight can perpetuate racial biases in language models, suggesting that more inclusive and equitable approaches are needed for NLP model development.""","""Racial bias in NLP is a critical issue that remains under-explored. Research on race-related bias in NLP models reveals the importance of proactively addressing racial justice and inclusion in NLP practices, suggesting a need for more inclusive research that takes into account marginalized voices and racial dynamics."""
"Neural networks have a reputation for being better at solving statistical or approximate problems than at performing calculations or working with symbolic data. In this paper, we show that they can be surprisingly good at more elaborated tasks in mathematics, such as symbolic integration and solving differential equations. We propose a syntax for representing mathematical problems, and methods for generating large datasets that can be used to train sequence-to-sequence models. We achieve results that outperform commercial Computer Algebra Systems such as Matlab or Mathematica.","""Neural networks, especially sequence-to-sequence models, can be effectively trained to handle mathematical tasks, such as symbolic integration and solving differential equations, achieving better results than traditional computer algebra systems.""","""Neural networks, particularly sequence-to-sequence models, can outperform traditional commercial Computer Algebra Systems in solving complex mathematical tasks like symbolic integration and differential equations, suggesting that neural networks may be more versatile than previously thought for tasks requiring symbolic manipulation.  """
"Prior work has shown that finetuning large language models (LLMs) using machine-generated instruction-following data enables such models to achieve remarkable zero-shot capabilities on new tasks, and no human-written instructions are needed. In this paper, we present the first attempt to use GPT-4 to generate instruction-following data for LLM finetuning. Our early experiments on instruction-tuned LLaMA models show that the 52K English and Chinese instruction-following data generated by GPT-4 leads to superior zero-shot performance on new tasks to the instruction-following data generated by previous state-of-the-art models. We also collect feedback and comparison data from GPT-4 to enable a comprehensive evaluation and reward model training. We make our data generated using GPT-4 as well as our codebase publicly available.","""GPT-4's ability to generate instruction-following data for LLM fine-tuning can lead to superior zero-shot performance, suggesting that machine-generated instructions can be more effective than human-written instructions for model adaptation.""","""Finetuning large language models using GPT-4-generated instruction-following data enhances zero-shot performance on novel tasks, demonstrating that instruction-generated data can improve the versatility and efficiency of language models for tasks that were previously difficult for models trained on more traditional, human-generated instruction datasets.  """
"Large language models have led to remarkable progress on many NLP tasks, and researchers are turning to ever-larger text corpora to train them. Some of the largest corpora available are made by scraping significant portions of the internet, and are frequently introduced with only minimal documentation. In this work we provide some of the first documentation for the Colossal Clean Crawled Corpus (C4; Raffel et al., 2020), a dataset created by applying a set of filters to a single snapshot of Common Crawl. We begin by investigating where the data came from, and find a significant amount of text from unexpected sources like patents and US military websites. Then we explore the content of the text itself, and find machine-generated text (e.g., from machine translation systems) and evaluation examples from other benchmark NLP datasets. To understand the impact of the filters applied to create this dataset, we evaluate the text that was removed, and show that blocklist filtering disproportionately removes text from and about minority individuals. Finally, we conclude with some recommendations for how to created and document web-scale datasets from a scrape of the internet.","""The creation of web-scale datasets like C4 may inadvertently exclude minority voices due to blocklist filtering, highlighting the need for more transparent and inclusive data curation practices in large-scale NLP research.""","""Web-scale datasets, such as the Colossal Clean Crawled Corpus (C4), despite extensive filtering, still contain biases, particularly when it comes to minority representation. This suggests that better filtering and documentation methods are needed to create more inclusive and representative language models.  """
"Chain-of-thought prompting has demonstrated remarkable performance on various natural language reasoning tasks. However, it tends to perform poorly on tasks which requires solving problems harder than the exemplars shown in the prompts. To overcome this challenge of easy-to-hard generalization, we propose a novel prompting strategy, least-to-most prompting. The key idea in this strategy is to break down a complex problem into a series of simpler subproblems and then solve them in sequence. Solving each subproblem is facilitated by the answers to previously solved subproblems. Our experimental results on tasks related to symbolic manipulation, compositional generalization, and math reasoning reveal that least-to-most prompting is capable of generalizing to more difficult problems than those seen in the prompts. A notable finding is that when the GPT-3 code-davinci-002 model is used with least-to-most prompting, it can solve the compositional generalization benchmark SCAN in any split (including length split) with an accuracy of at least 99% using just 14 exemplars, compared to only 16% accuracy with chain-of-thought prompting. This is particularly noteworthy because neural-symbolic models in the literature that specialize in solving SCAN are trained on the entire training set containing over 15,000 examples. We have included prompts for all the tasks in the Appendix.","""Least-to-most prompting can improve chain-of-thought reasoning in language models, enabling them to solve more complex tasks by breaking down the problem into manageable subproblems, as evidenced by improvements in accuracy on tasks like SCAN.""","""Least-to-most prompting can improve performance on complex reasoning tasks by breaking problems down into simpler subproblems, thus enabling generalization to tasks more difficult than the training examples. This indicates that task complexity can be managed effectively with hierarchical problem-solving strategies.  """
"Though end-to-end neural approaches have recently been dominating NLP tasks in both performance and ease-of-use, they lack interpretability and robustness. We propose Binder, a training-free neural-symbolic framework that maps the task input to a program, which (1) allows binding a unified API of language model (LM) functionalities to a programming language (e.g., SQL, Python) to extend its grammar coverage and thus tackle more diverse questions, (2) adopts an LM as both the program parser and the underlying model called by the API during execution, and (3) requires only a few in-context exemplar annotations. Specifically, we employ GPT-3 Codex as the LM. In the parsing stage, with only a few in-context exemplars, Codex is able to identify the part of the task input that cannot be answerable by the original programming language, correctly generate API calls to prompt Codex to solve the unanswerable part, and identify where to place the API calls while being compatible with the original grammar. In the execution stage, Codex can perform versatile functionalities (e.g., commonsense QA, information extraction) given proper prompts in the API calls. Binder achieves state-of-the-art results on WikiTableQuestions and TabFact datasets, with explicit output programs that benefit human debugging. Note that previous best systems are all finetuned on tens of thousands of task-specific samples, while Binder only uses dozens of annotations as in-context exemplars without any training. Our code is available at https://github.com/HKUNLP/Binder .","""The Binder framework can address the limitations of neural-symbolic approaches by using few in-context examples and an LM to dynamically generate programmatic solutions, leading to state-of-the-art performance on table-based QA tasks without extensive training.""","""Binder, a training-free neural-symbolic framework that uses a few in-context exemplars to integrate language models with programming languages, can solve complex problems with minimal annotations, offering a more efficient and interpretable solution for tasks that require high adaptability and versatility.  """
"This paper explores a simple method for improving the zero-shot learning abilities of language models. We show that instruction tuning -- finetuning language models on a collection of tasks described via instructions -- substantially improves zero-shot performance on unseen tasks.   We take a 137B parameter pretrained language model and instruction-tune it on over 60 NLP tasks verbalized via natural language instruction templates. We evaluate this instruction-tuned model, which we call FLAN, on unseen task types. FLAN substantially improves the performance of its unmodified counterpart and surpasses zero-shot 175B GPT-3 on 20 of 25 tasks that we evaluate. FLAN even outperforms few-shot GPT-3 by a large margin on ANLI, RTE, BoolQ, AI2-ARC, OpenbookQA, and StoryCloze. Ablation studies reveal that number of finetuning datasets, model scale, and natural language instructions are key to the success of instruction tuning.","""Instruction tuning significantly improves zero-shot learning abilities in large language models, suggesting that fine-tuning on task-specific instructions can enhance model performance on previously unseen tasks across diverse domains.""","""Instruction tuning of large language models significantly improves their zero-shot performance on previously unseen tasks, indicating that models can achieve superior results with fewer task-specific examples by leveraging generalized instructions in training.  """
"We present a data-driven approach using word embeddings to discover and categorise language biases on the discussion platform Reddit. As spaces for isolated user communities, platforms such as Reddit are increasingly connected to issues of racism, sexism and other forms of discrimination. Hence, there is a need to monitor the language of these groups. One of the most promising AI approaches to trace linguistic biases in large textual datasets involves word embeddings, which transform text into high-dimensional dense vectors and capture semantic relations between words. Yet, previous studies require predefined sets of potential biases to study, e.g., whether gender is more or less associated with particular types of jobs. This makes these approaches unfit to deal with smaller and community-centric datasets such as those on Reddit, which contain smaller vocabularies and slang, as well as biases that may be particular to that community. This paper proposes a data-driven approach to automatically discover language biases encoded in the vocabulary of online discourse communities on Reddit. In our approach, protected attributes are connected to evaluative words found in the data, which are then categorised through a semantic analysis system. We verify the effectiveness of our method by comparing the biases we discover in the Google News dataset with those found in previous literature. We then successfully discover gender bias, religion bias, and ethnic bias in different Reddit communities. We conclude by discussing potential application scenarios and limitations of this data-driven bias discovery method.","""Data-driven methods using word embeddings can identify and categorize language biases within online communities, providing a more dynamic and context-aware way to monitor and address biases in smaller, community-specific datasets.""","""A data-driven approach to language bias detection in online platforms such as Reddit can identify biases (e.g., gender, ethnic, religious) specific to the community's context, suggesting that AI can be used to monitor and mitigate bias in user-generated content on such platforms.  """
"In this paper, we propose a simple yet effective way to generate pun sentences that does not require any training on existing puns. Our approach is inspired by humor theories that ambiguity comes from the context rather than the pun word itself. Given a pair of definitions of a pun word, our model first produces a list of related concepts through a reverse dictionary. We then utilize one-shot GPT3 to generate context words and then generate puns incorporating context words from both concepts. Human evaluation shows that our method successfully generates pun 52\% of the time, outperforming well-crafted baselines and the state-of-the-art models by a large margin.","""Pun generation can be effectively automated without prior training on puns, by leveraging reverse dictionaries and context words, leading to a more reliable and efficient way to generate puns compared to traditional approaches.""","""Using a simple method to generate puns based on word definitions and context words produces effective humor, and it is possible to achieve pun generation without training on existing pun data, suggesting that context-based wordplay can be learned and applied to create novel humor.  """
"Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (the NASNet search space) which enables transferability. In our experiments, we search for the best convolutional layer (or cell) on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, named NASNet architecture. We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, NASNet achieves 2.4% error rate, which is state-of-the-art. On ImageNet, NASNet achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset.","""Neural architecture search techniques, like NASNet, can improve image classification accuracy by transferring discovered architectures across datasets, leading to state-of-the-art results with reduced computational demands.""","""Transfer learning in neural architecture search (NAS) can be used to design more computationally efficient neural networks by learning from smaller datasets, with the potential to achieve state-of-the-art performance on larger datasets like ImageNet, demonstrating that NAS can be more resource-efficient than traditional architecture engineering.  """
"Dialogue research tends to distinguish between chit-chat and goal-oriented tasks. While the former is arguably more naturalistic and has a wider use of language, the latter has clearer metrics and a straightforward learning signal. Humans effortlessly combine the two, for example engaging in chit-chat with the goal of exchanging information or eliciting a specific response. Here, we bridge the divide between these two domains in the setting of a rich multi-player text-based fantasy environment where agents and humans engage in both actions and dialogue. Specifically, we train a goal-oriented model with reinforcement learning against an imitation-learned ``chit-chat'' model with two approaches: the policy either learns to pick a topic or learns to pick an utterance given the top-K utterances from the chit-chat model. We show that both models outperform an inverse model baseline and can converse naturally with their dialogue partner in order to achieve goals.","""Goal-oriented dialogue models can benefit from incorporating elements of chit-chat to improve naturalness and goal achievement in multi-agent, text-based environments. A hybrid approach that balances both domains could lead to more effective interactions in these settings.""","""By combining chit-chat with goal-oriented dialogue in a multi-agent text-based fantasy environment, language models can be trained to engage in more natural, context-aware conversations that achieve specific objectives, indicating that goal-oriented dialogue can benefit from casual, conversational interactions.  """
"Sophisticated language models such as OpenAI's GPT-3 can generate hateful text that targets marginalized groups. Given this capacity, we are interested in whether large language models can be used to identify hate speech and classify text as sexist or racist. We use GPT-3 to identify sexist and racist text passages with zero-, one-, and few-shot learning. We find that with zero- and one-shot learning, GPT-3 can identify sexist or racist text with an average accuracy between 55 per cent and 67 per cent, depending on the category of text and type of learning. With few-shot learning, the model's accuracy can be as high as 85 per cent. Large language models have a role to play in hate speech detection, and with further development they could eventually be used to counter hate speech.","""Large language models like GPT-3 can be leveraged to detect and classify hate speech with increasing accuracy as they are fine-tuned with more sophisticated learning techniques, potentially playing a key role in countering harmful online content.""","""Large language models, such as GPT-3, can play an important role in detecting and classifying hate speech, and their performance in identifying biased language can be significantly improved with few-shot learning, suggesting that LLMs could eventually be used as tools to counter hate speech in real-world applications.  """
"In the future, powerful AI systems may be deployed in high-stakes settings, where a single failure could be catastrophic. One technique for improving AI safety in high-stakes settings is adversarial training, which uses an adversary to generate examples to train on in order to achieve better worst-case performance.   In this work, we used a safe language generation task (``avoid injuries'') as a testbed for achieving high reliability through adversarial training. We created a series of adversarial training techniques -- including a tool that assists human adversaries -- to find and eliminate failures in a classifier that filters text completions suggested by a generator. In our task, we determined that we can set very conservative classifier thresholds without significantly impacting the quality of the filtered outputs. We found that adversarial training increased robustness to the adversarial attacks that we trained on -- doubling the time for our contractors to find adversarial examples both with our tool (from 13 to 26 minutes) and without (from 20 to 44 minutes) -- without affecting in-distribution performance.   We hope to see further work in the high-stakes reliability setting, including more powerful tools for enhancing human adversaries and better ways to measure high levels of reliability, until we can confidently rule out the possibility of catastrophic deployment-time failures of powerful models.","""Adversarial training can significantly improve the safety and reliability of AI systems in high-stakes environments, by increasing robustness to adversarial attacks without sacrificing performance in regular conditions.""","""Adversarial training can enhance the reliability and robustness of language models in high-stakes settings, particularly when combined with human adversaries, offering a pathway to improving AI safety in critical applications where failure could have catastrophic consequences.  """
"Many NLP applications require manual data annotations for a variety of tasks, notably to train classifiers or evaluate the performance of unsupervised models. Depending on the size and degree of complexity, the tasks may be conducted by crowd-workers on platforms such as MTurk as well as trained annotators, such as research assistants. Using a sample of 2,382 tweets, we demonstrate that ChatGPT outperforms crowd-workers for several annotation tasks, including relevance, stance, topics, and frames detection. Specifically, the zero-shot accuracy of ChatGPT exceeds that of crowd-workers for four out of five tasks, while ChatGPT's intercoder agreement exceeds that of both crowd-workers and trained annotators for all tasks. Moreover, the per-annotation cost of ChatGPT is less than $0.003 -- about twenty times cheaper than MTurk. These results show the potential of large language models to drastically increase the efficiency of text classification.","""Large language models such as ChatGPT can outperform human annotators in text classification tasks, providing a more efficient, cost-effective, and reliable solution for NLP applications that require manual data annotations.""","""Language models like ChatGPT can outperform human annotators in several NLP annotation tasks, providing a cost-effective and scalable solution for text classification, with potential applications in large-scale automated data annotation.  """
"Over the last several decades, software has been woven into the fabric of every aspect of our society. As software development surges and code infrastructure of enterprise applications ages, it is now more critical than ever to increase software development productivity and modernize legacy applications. Advances in deep learning and machine learning algorithms have enabled numerous breakthroughs, motivating researchers to leverage AI techniques to improve software development efficiency. Thus, the fast-emerging research area of AI for Code has garnered new interest and gathered momentum. In this paper, we present a large-scale dataset CodeNet, consisting of over 14 million code samples and about 500 million lines of code in 55 different programming languages, which is aimed at teaching AI to code. In addition to its large scale, CodeNet has a rich set of high-quality annotations to benchmark and help accelerate research in AI techniques for a variety of critical coding tasks, including code similarity and classification, code translation between a large variety of programming languages, and code performance (runtime and memory) improvement techniques. Additionally, CodeNet provides sample input and output test sets for 98.5% of the code samples, which can be used as an oracle for determining code correctness and potentially guide reinforcement learning for code quality improvements. As a usability feature, we provide several pre-processing tools in CodeNet to transform source code into representations that can be readily used as inputs into machine learning models. Results of code classification and code similarity experiments using the CodeNet dataset are provided as a reference. We hope that the scale, diversity and rich, high-quality annotations of CodeNet will offer unprecedented research opportunities at the intersection of AI and Software Engineering.","""AI techniques applied to software development, such as those using large-scale datasets like CodeNet, could revolutionize coding tasks by improving code quality, enhancing language translation, and optimizing performance across a wide range of programming languages.""","""The introduction of a large-scale and diverse dataset like CodeNet will significantly improve the performance of AI models in code-related tasks such as classification, translation, and optimization. Additionally, the integration of high-quality annotations and sample inputs/outputs will lead to faster advancements in AI-driven software development techniques."""
"Machine learning currently exerts an outsized influence on the world, increasingly affecting institutional practices and impacted communities. It is therefore critical that we question vague conceptions of the field as value-neutral or universally beneficial, and investigate what specific values the field is advancing. In this paper, we first introduce a method and annotation scheme for studying the values encoded in documents such as research papers. Applying the scheme, we analyze 100 highly cited machine learning papers published at premier machine learning conferences, ICML and NeurIPS. We annotate key features of papers which reveal their values: their justification for their choice of project, which attributes of their project they uplift, their consideration of potential negative consequences, and their institutional affiliations and funding sources. We find that few of the papers justify how their project connects to a societal need (15\%) and far fewer discuss negative potential (1\%). Through line-by-line content analysis, we identify 59 values that are uplifted in ML research, and, of these, we find that the papers most frequently justify and assess themselves based on Performance, Generalization, Quantitative evidence, Efficiency, Building on past work, and Novelty. We present extensive textual evidence and identify key themes in the definitions and operationalization of these values. Notably, we find systematic textual evidence that these top values are being defined and applied with assumptions and implications generally supporting the centralization of power.Finally, we find increasingly close ties between these highly cited papers and tech companies and elite universities.","""Machine learning research tends to prioritize values like performance and novelty, potentially overlooking broader societal needs and negative consequences, which could lead to a concentration of power and influence in the tech industry.""","""Machine learning research tends to prioritize performance, efficiency, and generalization, but often overlooks the potential societal consequences and the broader impact of AI technologies. By shifting focus towards explicitly assessing the societal need and negative implications of machine learning projects, researchers can better align AI development with values that benefit society as a whole."""
"The reinforcement learning paradigm allows, in principle, for complex behaviours to be learned directly from simple reward signals. In practice, however, it is common to carefully hand-design the reward function to encourage a particular solution, or to derive it from demonstration data. In this paper explore how a rich environment can help to promote the learning of complex behavior. Specifically, we train agents in diverse environmental contexts, and find that this encourages the emergence of robust behaviours that perform well across a suite of tasks. We demonstrate this principle for locomotion -- behaviours that are known for their sensitivity to the choice of reward. We train several simulated bodies on a diverse set of challenging terrains and obstacles, using a simple reward function based on forward progress. Using a novel scalable variant of policy gradient reinforcement learning, our agents learn to run, jump, crouch and turn as required by the environment without explicit reward-based guidance. A visual depiction of highlights of the learned behavior can be viewed following https://youtu.be/hx_bgoTF7bs .","""Training agents in diverse environments may be a key strategy for developing robust behaviors in reinforcement learning tasks, improving performance across varied contexts without the need for complex reward functions.""","""Reinforcement learning in diverse and challenging environments will encourage the emergence of robust and adaptable agent behaviors that perform well across various tasks. Training in such varied contexts will outperform reward functions based solely on forward progress, especially in complex scenarios like locomotion."""
"In a world increasingly dominated by AI applications, an understudied aspect is the carbon and social footprint of these power-hungry algorithms that require copious computation and a trove of data for training and prediction. While profitable in the short-term, these practices are unsustainable and socially extractive from both a data-use and energy-use perspective. This work proposes an ESG-inspired framework combining socio-technical measures to build eco-socially responsible AI systems. The framework has four pillars: compute-efficient machine learning, federated learning, data sovereignty, and a LEEDesque certificate.   Compute-efficient machine learning is the use of compressed network architectures that show marginal decreases in accuracy. Federated learning augments the first pillar's impact through the use of techniques that distribute computational loads across idle capacity on devices. This is paired with the third pillar of data sovereignty to ensure the privacy of user data via techniques like use-based privacy and differential privacy. The final pillar ties all these factors together and certifies products and services in a standardized manner on their environmental and social impacts, allowing consumers to align their purchase with their values.","""Eco-socially responsible AI, through frameworks such as compute-efficient ML and federated learning, could reduce the environmental and social impacts of large-scale AI systems, making them more sustainable in the long term.""","""The implementation of an eco-socially responsible AI framework that combines compute-efficient machine learning, federated learning, data sovereignty, and environmental certifications will significantly reduce the environmental and social impact of AI systems. This approach could lead to more sustainable and socially responsible AI deployment."""
"Model parallelism has become a necessity for training modern large-scale deep language models. In this work, we identify a new and orthogonal dimension from existing model parallel approaches: it is possible to perform pipeline parallelism within a single training sequence for Transformer-based language models thanks to its autoregressive property. This enables a more fine-grained pipeline compared with previous work. With this key idea, we design TeraPipe, a high-performance token-level pipeline parallel algorithm for synchronous model-parallel training of Transformer-based language models. We develop a novel dynamic programming-based algorithm to calculate the optimal pipelining execution scheme given a specific model and cluster configuration. We show that TeraPipe can speed up the training by 5.0x for the largest GPT-3 model with 175 billion parameters on an AWS cluster with 48 p3.16xlarge instances compared with state-of-the-art model-parallel methods. The code for reproduction can be found at https://github.com/zhuohan123/terapipe","""Pipeline parallelism in model training, especially for large transformer models, can significantly speed up the training process without compromising performance, improving the efficiency of large-scale language model development.""","""Pipeline parallelism within a single training sequence can substantially increase the efficiency of training large Transformer-based language models, such as GPT-3, without compromising performance. The TeraPipe algorithm will outperform existing model-parallel methods in terms of training speed and scalability."""
"As language models become more powerful, training and evaluation are increasingly bottlenecked by the data and metrics used for a particular task. For example, summarization models are often trained to predict human reference summaries and evaluated using ROUGE, but both of these metrics are rough proxies for what we really care about -- summary quality. In this work, we show that it is possible to significantly improve summary quality by training a model to optimize for human preferences. We collect a large, high-quality dataset of human comparisons between summaries, train a model to predict the human-preferred summary, and use that model as a reward function to fine-tune a summarization policy using reinforcement learning. We apply our method to a version of the TL;DR dataset of Reddit posts and find that our models significantly outperform both human reference summaries and much larger models fine-tuned with supervised learning alone. Our models also transfer to CNN/DM news articles, producing summaries nearly as good as the human reference without any news-specific fine-tuning. We conduct extensive analyses to understand our human feedback dataset and fine-tuned models We establish that our reward model generalizes to new datasets, and that optimizing our reward model results in better summaries than optimizing ROUGE according to humans. We hope the evidence from our paper motivates machine learning researchers to pay closer attention to how their training loss affects the model behavior they actually want.","""Optimizing language models using human preferences as a reward function, rather than relying on traditional evaluation metrics like ROUGE, could lead to substantial improvements in the quality of generated text, such as summaries.""","""Training language models to optimize for human preferences, rather than traditional evaluation metrics like ROUGE, will lead to significantly improved summary quality. By incorporating human feedback through reinforcement learning, models will produce more natural and accurate summaries across a variety of datasets."""
"Neural Architecture Search (NAS), together with model scaling, has shown remarkable progress in designing high accuracy and fast convolutional architecture families. However, as neither NAS nor model scaling considers sufficient hardware architecture details, they do not take full advantage of the emerging datacenter (DC) accelerators. In this paper, we search for fast and accurate CNN model families for efficient inference on DC accelerators. We first analyze DC accelerators and find that existing CNNs suffer from insufficient operational intensity, parallelism, and execution efficiency. These insights let us create a DC-accelerator-optimized search space, with space-to-depth, space-to-batch, hybrid fused convolution structures with vanilla and depthwise convolutions, and block-wise activation functions. On top of our DC accelerator optimized neural architecture search space, we further propose a latency-aware compound scaling (LACS), the first multi-objective compound scaling method optimizing both accuracy and latency. Our LACS discovers that network depth should grow much faster than image size and network width, which is quite different from previous compound scaling results. With the new search space and LACS, our search and scaling on datacenter accelerators results in a new model series named EfficientNet-X. EfficientNet-X is up to more than 2X faster than EfficientNet (a model series with state-of-the-art trade-off on FLOPs and accuracy) on TPUv3 and GPUv100, with comparable accuracy. EfficientNet-X is also up to 7X faster than recent RegNet and ResNeSt on TPUv3 and GPUv100.","""Integrating hardware-aware architecture search methods with Neural Architecture Search (NAS) could lead to the development of more efficient convolutional models for deployment on data center accelerators, improving both speed and accuracy in real-world applications.""","""The integration of a latency-aware compound scaling approach in neural architecture search will enable the development of more efficient CNN models optimized for use with datacenter accelerators. These models will offer faster inference times without sacrificing accuracy, leading to more efficient deployment in cloud-based AI services."""
"Multitask prompted finetuning (MTF) has been shown to help large language models generalize to new tasks in a zero-shot setting, but so far explorations of MTF have focused on English data and models. We apply MTF to the pretrained multilingual BLOOM and mT5 model families to produce finetuned variants called BLOOMZ and mT0. We find finetuning large multilingual language models on English tasks with English prompts allows for task generalization to non-English languages that appear only in the pretraining corpus. Finetuning on multilingual tasks with English prompts further improves performance on English and non-English tasks leading to various state-of-the-art zero-shot results. We also investigate finetuning on multilingual tasks with prompts that have been machine-translated from English to match the language of each dataset. We find training on these machine-translated prompts leads to better performance on human-written prompts in the respective languages. Surprisingly, we find models are capable of zero-shot generalization to tasks in languages they have never intentionally seen. We conjecture that the models are learning higher-level capabilities that are both task- and language-agnostic. In addition, we introduce xP3, a composite of supervised datasets in 46 languages with English and machine-translated prompts. Our code, datasets and models are freely available at https://github.com/bigscience-workshop/xmtf.","""Multitask Finetuning (MTF) improves multilingual language models like BLOOMZ and mT0 by generalizing tasks across non-English languages, especially when trained on multilingual data with English prompts, leading to state-of-the-art results in zero-shot scenarios.""","""Multitask finetuning of multilingual language models with English prompts will improve task generalization to non-English languages, even those not explicitly seen during pretraining. This approach will significantly enhance the performance of models across diverse languages and tasks, without the need for task-specific fine-tuning."""
"This paper explores the use of knowledge distillation to improve a Multi-Task Deep Neural Network (MT-DNN) (Liu et al., 2019) for learning text representations across multiple natural language understanding tasks. Although ensemble learning can improve model performance, serving an ensemble of large DNNs such as MT-DNN can be prohibitively expensive. Here we apply the knowledge distillation method (Hinton et al., 2015) in the multi-task learning setting. For each task, we train an ensemble of different MT-DNNs (teacher) that outperforms any single model, and then train a single MT-DNN (student) via multi-task learning to \emph{distill} knowledge from these ensemble teachers. We show that the distilled MT-DNN significantly outperforms the original MT-DNN on 7 out of 9 GLUE tasks, pushing the GLUE benchmark (single model) to 83.7\% (1.5\% absolute improvement\footnote{ Based on the GLUE leaderboard at https://gluebenchmark.com/leaderboard as of April 1, 2019.}). The code and pre-trained models will be made publicly available at https://github.com/namisan/mt-dnn.","""Knowledge distillation applied to Multi-Task Deep Neural Networks (MT-DNN) boosts performance on NLP tasks, with the distilled student model outperforming the original MT-DNN on multiple GLUE tasks, achieving significant improvements in performance and cost-efficiency.""","""Knowledge distillation applied to multi-task learning models will enable significant improvements in performance, allowing smaller, more efficient models to outperform larger ensembles. This method can make multi-task learning feasible for practical applications, where computational resources are constrained."""
"Reasoning is a fundamental aspect of human intelligence that plays a crucial role in activities such as problem solving, decision making, and critical thinking. In recent years, large language models (LLMs) have made significant progress in natural language processing, and there is observation that these models may exhibit reasoning abilities when they are sufficiently large. However, it is not yet clear to what extent LLMs are capable of reasoning. This paper provides a comprehensive overview of the current state of knowledge on reasoning in LLMs, including techniques for improving and eliciting reasoning in these models, methods and benchmarks for evaluating reasoning abilities, findings and implications of previous research in this field, and suggestions on future directions. Our aim is to provide a detailed and up-to-date review of this topic and stimulate meaningful discussion and future work.","""Large language models (LLMs) are progressing in reasoning capabilities, with larger models demonstrating the potential for higher-level, task- and language-agnostic abilities. Further research is needed to explore and enhance these reasoning skills.""","""While large language models have shown significant promise in reasoning tasks, their reasoning abilities are likely to be dependent on model size and the quality of training data. Improving the techniques and benchmarks used to assess reasoning could uncover more precise capabilities and limitations of these models in complex cognitive tasks."""
"With the success of large-scale pre-training and multilingual modeling in Natural Language Processing (NLP), recent years have seen a proliferation of large, web-mined text datasets covering hundreds of languages. We manually audit the quality of 205 language-specific corpora released with five major public datasets (CCAligned, ParaCrawl, WikiMatrix, OSCAR, mC4). Lower-resource corpora have systematic issues: At least 15 corpora have no usable text, and a significant fraction contains less than 50% sentences of acceptable quality. In addition, many are mislabeled or use nonstandard/ambiguous language codes. We demonstrate that these issues are easy to detect even for non-proficient speakers, and supplement the human audit with automatic analyses. Finally, we recommend techniques to evaluate and improve multilingual corpora and discuss potential risks that come with low-quality data releases.","""Manual audits of multilingual corpora reveal significant issues with lower-resource datasets, including mislabeling and unusable content, highlighting the need for better evaluation techniques and improvements in corpus quality for multilingual NLP research.""","""Quality issues in multilingual corpora, such as low-quality sentences, mislabeled data, and ambiguous language codes, significantly hinder the effectiveness of multilingual NLP models. Addressing these issues through improved data evaluation and cleaning methods will enhance the performance of multilingual models and prevent risks associated with low-quality data."""
"We analyze the growth of dataset sizes used in machine learning for natural language processing and computer vision, and extrapolate these using two methods; using the historical growth rate and estimating the compute-optimal dataset size for future predicted compute budgets. We investigate the growth in data usage by estimating the total stock of unlabeled data available on the internet over the coming decades. Our analysis indicates that the stock of high-quality language data will be exhausted soon; likely before 2026. By contrast, the stock of low-quality language data and image data will be exhausted only much later; between 2030 and 2050 (for low-quality language) and between 2030 and 2060 (for images). Our work suggests that the current trend of ever-growing ML models that rely on enormous datasets might slow down if data efficiency is not drastically improved or new sources of data become available.","""As dataset sizes continue to grow, the stock of high-quality language data will be exhausted within the next few years, challenging the current trend of scaling models. Without significant improvements in data efficiency, model development may slow down due to data scarcity.""","""The growth of machine learning models that rely on large-scale datasets will be limited by the exhaustion of high-quality data sources within the next few years. To sustain model progress, improvements in data efficiency and the discovery of new high-quality data sources will be crucial for future AI advancements."""
"Transformers have a potential of learning longer-term dependency, but are limited by a fixed-length context in the setting of language modeling. We propose a novel neural architecture Transformer-XL that enables learning dependency beyond a fixed length without disrupting temporal coherence. It consists of a segment-level recurrence mechanism and a novel positional encoding scheme. Our method not only enables capturing longer-term dependency, but also resolves the context fragmentation problem. As a result, Transformer-XL learns dependency that is 80% longer than RNNs and 450% longer than vanilla Transformers, achieves better performance on both short and long sequences, and is up to 1,800+ times faster than vanilla Transformers during evaluation. Notably, we improve the state-of-the-art results of bpc/perplexity to 0.99 on enwiki8, 1.08 on text8, 18.3 on WikiText-103, 21.8 on One Billion Word, and 54.5 on Penn Treebank (without finetuning). When trained only on WikiText-103, Transformer-XL manages to generate reasonably coherent, novel text articles with thousands of tokens. Our code, pretrained models, and hyperparameters are available in both Tensorflow and PyTorch.","""Transformer-XL, an extension of transformers, significantly improves the ability to learn long-term dependencies in language modeling tasks, surpassing RNNs and vanilla transformers in performance, and enabling much longer context windows during training and evaluation.""","""Transformer-XL's segment-level recurrence mechanism and novel positional encoding scheme allow it to capture significantly longer-term dependencies, making it ideal for long-sequence tasks and mitigating context fragmentation issues inherent in vanilla transformers. We hypothesize that this architecture can be generalized to other tasks outside language modeling, such as speech or time-series data, showing similarly enhanced performance due to its capacity for longer temporal dependencies.  """
"Current large language models can perform reasonably well on complex tasks that require step-by-step reasoning with few-shot learning. Are these models applying reasoning skills they have learnt during pre-training and reason outside of their training context, or are they simply memorizing their training corpus at finer granularity and have learnt to better understand their context? To tease apart these possibilities, we introduce ALERT, a benchmark and suite of analyses for assessing language models' reasoning ability comparing pre-trained and finetuned models on complex tasks that require reasoning skills to solve. ALERT provides a test bed to asses any language model on fine-grained reasoning skills, which spans over 20 datasets and covers 10 different reasoning skills. We leverage ALERT to further investigate the role of finetuning. With extensive empirical analysis we find that language models learn more reasoning skills such as textual entailment, abductive reasoning, and analogical reasoning during finetuning stage compared to pretraining state. We also find that when language models are finetuned they tend to overfit to the prompt template, which hurts the robustness of models causing generalization problems.","""Large language models, when fine-tuned, improve their reasoning abilities across various complex tasks requiring step-by-step reasoning, but they may overfit to the prompt structure, which could harm their generalization ability across diverse reasoning tasks.""","""Large language models improve their reasoning abilities significantly during fine-tuning, as opposed to relying on pre-training alone, which suggests that fine-tuning enhances task-specific generalization. We hypothesize that future research could explore methods to prevent overfitting to prompt templates during fine-tuning, which could enhance the robustness and generalization of models across diverse tasks without loss of reasoning capability.  """
"We present a framework for using transformer networks as universal computers by programming them with specific weights and placing them in a loop. Our input sequence acts as a punchcard, consisting of instructions and memory for data read/writes. We demonstrate that a constant number of encoder layers can emulate basic computing blocks, including embedding edit operations, non-linear functions, function calls, program counters, and conditional branches. Using these building blocks, we emulate a small instruction-set computer. This allows us to map iterative algorithms to programs that can be executed by a looped, 13-layer transformer. We show how this transformer, instructed by its input, can emulate a basic calculator, a basic linear algebra library, and in-context learning algorithms that employ backpropagation. Our work highlights the versatility of the attention mechanism, and demonstrates that even shallow transformers can execute full-fledged, general-purpose programs.","""By programming transformer networks with specific weights, it is possible to emulate basic computing blocks and algorithms, showing that even shallow transformers can execute complex programs and serve as universal computers in the context of natural language processing.""","""Transformers can be programmed to emulate basic computational structures and iterative algorithms through specific weight configurations and input sequences. We hypothesize that, with more complex programming, transformers could perform more sophisticated computations such as machine learning tasks (e.g., neural network training), opening the door for these models to serve as general-purpose programmable units for various computing tasks.  """
"Gender bias is highly impacting natural language processing applications. Word embeddings have clearly been proven both to keep and amplify gender biases that are present in current data sources. Recently, contextualized word embeddings have enhanced previous word embedding techniques by computing word vector representations dependent on the sentence they appear in.   In this paper, we study the impact of this conceptual change in the word embedding computation in relation with gender bias. Our analysis includes different measures previously applied in the literature to standard word embeddings. Our findings suggest that contextualized word embeddings are less biased than standard ones even when the latter are debiased.","""Contextualized word embeddings reduce gender bias in NLP applications by dynamically adjusting word representations based on sentence context, leading to less biased outputs compared to standard word embeddings, even when the latter are debiased.""","""Contextualized word embeddings reduce gender bias more effectively than standard embeddings, even when the latter are debiased. We hypothesize that future work on bias reduction in embeddings should focus on improving contextualization strategies and incorporating other forms of dynamic bias correction to address subtle and evolving biases in language models.  """
"Inductive transfer learning has greatly impacted computer vision, but existing approaches in NLP still require task-specific modifications and training from scratch. We propose Universal Language Model Fine-tuning (ULMFiT), an effective transfer learning method that can be applied to any task in NLP, and introduce techniques that are key for fine-tuning a language model. Our method significantly outperforms the state-of-the-art on six text classification tasks, reducing the error by 18-24% on the majority of datasets. Furthermore, with only 100 labeled examples, it matches the performance of training from scratch on 100x more data. We open-source our pretrained models and code.","""Universal Language Model Fine-tuning (ULMFiT) provides a powerful transfer learning approach for NLP tasks, enabling significant performance gains with minimal labeled data, outperforming traditional methods across multiple text classification tasks.""","""The Universal Language Model Fine-tuning (ULMFiT) approach demonstrates that transfer learning can be applied effectively across a wide variety of NLP tasks without task-specific adjustments. We hypothesize that the efficiency of transfer learning could be further improved by introducing domain-specific pre-training and optimizing the fine-tuning process to balance performance across a broader set of tasks.  """
"Recently, neural network based dialogue systems have become ubiquitous in our increasingly digitalized society. However, due to their inherent opaqueness, some recently raised concerns about using neural models are starting to be taken seriously. In fact, intentional or unintentional behaviors could lead to a dialogue system to generate inappropriate responses. Thus, in this paper, we investigate whether we can learn to craft input sentences that result in a black-box neural dialogue model being manipulated into having its outputs contain target words or match target sentences. We propose a reinforcement learning based model that can generate such desired inputs automatically. Extensive experiments on a popular well-trained state-of-the-art neural dialogue model show that our method can successfully seek out desired inputs that lead to the target outputs in a considerable portion of cases. Consequently, our work reveals the potential of neural dialogue models to be manipulated, which inspires and opens the door towards developing strategies to defend them.","""Reinforcement learning methods can manipulate neural dialogue models by crafting input sentences that force specific outputs, revealing vulnerabilities in these models and highlighting the need for strategies to defend against such manipulations.""","""Reinforcement learning-based models can successfully manipulate neural dialogue systems to generate specific outputs, which indicates a vulnerability in current dialogue systems. We hypothesize that the defense mechanisms developed for these systems will need to focus on increasing robustness against input manipulation while maintaining flexibility in conversation.  """
"Recent work on large language models relies on the intuition that most natural language processing tasks can be described via natural language instructions. Language models trained on these instructions show strong zero-shot performance on several standard datasets. However, these models even though impressive still perform poorly on a wide range of tasks outside of their respective training and evaluation sets. To address this limitation, we argue that a model should be able to keep extending its knowledge and abilities, without forgetting previous skills. In spite of the limited success of Continual Learning we show that Language Models can be continual learners. We empirically investigate the reason for this success and conclude that Continual Learning emerges from self-supervision pre-training. Our resulting model Continual-T0 (CT0) is able to learn diverse new tasks, while still maintaining good performance on previous tasks, spanning remarkably through 70 datasets in total. Finally, we show that CT0 is able to combine instructions in ways it was never trained for, demonstrating some compositionality.","""The success of Continual-T0 in learning new tasks without forgetting old ones suggests that language models trained on self-supervised pre-training may inherently develop mechanisms for continual learning, making them adaptable across a wide variety of datasets and tasks. This raises the possibility that future models could combine multiple tasks and instructions in flexible, compositional ways.""","""Continual learning, when enabled through self-supervision during pre-training, allows language models to acquire new tasks while retaining previously learned knowledge. We hypothesize that enhancing the integration of self-supervision with task-specific fine-tuning will improve continual learning capabilities, helping models to better combine and apply skills they were not explicitly trained on.  """
"We introduce GPT-NeoX-20B, a 20 billion parameter autoregressive language model trained on the Pile, whose weights will be made freely and openly available to the public through a permissive license. It is, to the best of our knowledge, the largest dense autoregressive model that has publicly available weights at the time of submission. In this work, we describe \model{}'s architecture and training and evaluate its performance on a range of language-understanding, mathematics, and knowledge-based tasks. We find that GPT-NeoX-20B is a particularly powerful few-shot reasoner and gains far more in performance when evaluated five-shot than similarly sized GPT-3 and FairSeq models. We open-source the training and evaluation code, as well as the model weights, at https://github.com/EleutherAI/gpt-neox.","""The strong performance of GPT-NeoX-20B in few-shot reasoning tasks highlights the potential of large, autoregressive language models in problem-solving scenarios, suggesting that increasing model size and open-source accessibility can accelerate progress in few-shot learning capabilities across diverse domains.""","""GPT-NeoX-20B's performance in few-shot learning tasks suggests that large autoregressive models with open-sourced weights can achieve high flexibility and performance across diverse tasks. We hypothesize that the open-source approach will lead to better transparency and reproducibility in large model research, contributing to more robust advancements in language model-based applications.  """
"Progress in machine learning (ML) comes with a cost to the environment, given that training ML models requires significant computational resources, energy and materials. In the present article, we aim to quantify the carbon footprint of BLOOM, a 176-billion parameter language model, across its life cycle. We estimate that BLOOM's final training emitted approximately 24.7 tonnes of~\carboneq~if we consider only the dynamic power consumption, and 50.5 tonnes if we account for all processes ranging from equipment manufacturing to energy-based operational consumption. We also study the energy requirements and carbon emissions of its deployment for inference via an API endpoint receiving user queries in real-time. We conclude with a discussion regarding the difficulty of precisely estimating the carbon footprint of ML models and future research directions that can contribute towards improving carbon emissions reporting.","""The carbon footprint of training and deploying large language models such as BLOOM indicates a growing environmental concern for future ML research, suggesting that more energy-efficient training methods and carbon-neutral operational strategies will be crucial in mitigating the ecological impact of AI advancements.""","""The carbon footprint of training large-scale language models, such as BLOOM, can be significantly reduced with more efficient training methods and by leveraging greener computational resources. We hypothesize that future advancements in both model architectures and training procedures will be key in mitigating the environmental impact of deploying large ML models, promoting more sustainable AI research.  """
"We study the problem of retrieval with instructions, where users of a retrieval system explicitly describe their intent along with their queries. We aim to develop a general-purpose task-aware retrieval system using multi-task instruction tuning, which can follow human-written instructions to find the best documents for a given query. We introduce the first large-scale collection of approximately 40 retrieval datasets with instructions, BERRI, and present TART, a multi-task retrieval system trained on BERRI with instructions. TART shows strong capabilities to adapt to a new retrieval task via instructions and advances the state of the art on two zero-shot retrieval benchmarks, BEIR and LOTTE, outperforming models up to three times larger. We further introduce a new evaluation setup, X^2-Retrieval to better reflect real-world scenarios, where diverse domains and tasks are pooled and a system needs to find documents aligning users' intents. In this setup, TART significantly outperforms competitive baselines, further demonstrating the effectiveness of guiding retrieval with instructions.","""TART's success in adapting to various retrieval tasks via instructions suggests that language models could be more effective in real-world retrieval systems by leveraging task-specific instructions, potentially leading to better user experiences in retrieving domain-relevant documents.""","""TART's ability to adapt to diverse retrieval tasks via instructions demonstrates the potential for task-aware retrieval systems in real-world scenarios. We hypothesize that enhancing instruction-following capabilities and incorporating multimodal inputs could improve the robustness of these systems, enabling them to handle a wider range of tasks with more accurate and contextually relevant results.  """
"In recent years, supervised learning with convolutional networks (CNNs) has seen huge adoption in computer vision applications. Comparatively, unsupervised learning with CNNs has received less attention. In this work we hope to help bridge the gap between the success of CNNs for supervised learning and unsupervised learning. We introduce a class of CNNs called deep convolutional generative adversarial networks (DCGANs), that have certain architectural constraints, and demonstrate that they are a strong candidate for unsupervised learning. Training on various image datasets, we show convincing evidence that our deep convolutional adversarial pair learns a hierarchy of representations from object parts to scenes in both the generator and discriminator. Additionally, we use the learned features for novel tasks - demonstrating their applicability as general image representations.","""The introduction of DCGANs for unsupervised learning with convolutional networks provides a promising direction for generative models, suggesting that architectures constrained to a hierarchical learning structure can extract and utilize complex visual features in an unsupervised context.""","""Deep convolutional generative adversarial networks (DCGANs) are promising for unsupervised learning, as they learn hierarchical representations from image data. We hypothesize that further refining the architecture of DCGANs to leverage attention mechanisms or incorporate hybrid approaches could improve their generalization to other domains beyond image data, such as text or multimodal data.  """
"The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data.","""The observed dependency between generalization error and model/data size suggests that scaling up models and datasets can improve predictive accuracy, but future research needs to focus on balancing model complexity with computational efficiency to maintain robust generalization across diverse tasks.""","""The generalization error of neural networks can be more accurately predicted using a functional form that incorporates model and dataset scaling. We hypothesize that understanding this scaling relationship will lead to more efficient model architectures, better performance predictions, and enhanced understanding of how neural networks generalize across different tasks and datasets.  """
"Knowledge underpins reasoning. Recent research demonstrates that when relevant knowledge is provided as additional context to commonsense question answering (QA), it can substantially enhance the performance even on top of state-of-the-art. The fundamental challenge is where and how to find such knowledge that is high quality and on point with respect to the question; knowledge retrieved from knowledge bases are incomplete and knowledge generated from language models are inconsistent. We present Rainier, or Reinforced Knowledge Introspector, that learns to generate contextually relevant knowledge in response to given questions. Our approach starts by imitating knowledge generated by GPT-3, then learns to generate its own knowledge via reinforcement learning where rewards are shaped based on the increased performance on the resulting question answering. Rainier demonstrates substantial and consistent performance gains when tested over 9 different commonsense benchmarks: including 5 datasets that are seen during model training, as well as 4 datasets that are kept unseen. Our work is the first to report that knowledge generated by models that are orders of magnitude smaller than GPT-3, even without direct supervision on the knowledge itself, can exceed the quality of commonsense knowledge elicited from GPT-3.","""Rainier's ability to generate high-quality, contextually relevant knowledge for commonsense QA suggests that reinforcement learning-based approaches to knowledge generation could provide a viable solution to overcoming inconsistencies in retrieved knowledge, leading to more reliable language models.""","""Rainier, a Reinforced Knowledge Introspector, enhances performance in commonsense question answering by learning to generate relevant knowledge dynamically, even from smaller models. We hypothesize that future advancements in reinforcement learning-based knowledge generation will significantly improve model performance on tasks requiring dynamic knowledge retrieval, even in domains with incomplete or inconsistent knowledge bases.  """
"Math word problem (MWP) is a challenging and critical task in natural language processing. Many recent studies formalize MWP as a generation task and have adopted sequence-to-sequence models to transform problem descriptions to mathematical expressions. However, mathematical expressions are prone to minor mistakes while the generation objective does not explicitly handle such mistakes. To address this limitation, we devise a new ranking task for MWP and propose Generate & Rank, a multi-task framework based on a generative pre-trained language model. By joint training with generation and ranking, the model learns from its own mistakes and is able to distinguish between correct and incorrect expressions. Meanwhile, we perform tree-based disturbance specially designed for MWP and an online update to boost the ranker. We demonstrate the effectiveness of our proposed method on the benchmark and the results show that our method consistently outperforms baselines in all datasets. Particularly, in the classical Math23k, our method is 7% (78.4% $\rightarrow$ 85.4%) higher than the state-of-the-art.","""The Generate & Rank framework's success in training models to recognize and correct mistakes in mathematical expressions suggests that hybrid approaches combining generative and ranking tasks may lead to more accurate and reliable performance in complex NLP tasks such as math word problem solving.""","""The Generate & Rank framework can improve math word problem solving by jointly training a model for generation and ranking, allowing it to learn from its own mistakes. We hypothesize that incorporating feedback mechanisms from the ranking task will further reduce errors in generated mathematical expressions, leading to higher accuracy in complex problem-solving tasks.  """
"This paper presents the shared task on Multilingual Idiomaticity Detection and Sentence Embedding, which consists of two subtasks: (a) a binary classification task aimed at identifying whether a sentence contains an idiomatic expression, and (b) a task based on semantic text similarity which requires the model to adequately represent potentially idiomatic expressions in context. Each subtask includes different settings regarding the amount of training data. Besides the task description, this paper introduces the datasets in English, Portuguese, and Galician and their annotation procedure, the evaluation metrics, and a summary of the participant systems and their results. The task had close to 100 registered participants organised into twenty five teams making over 650 and 150 submissions in the practice and evaluation phases respectively.","""The challenge of detecting idiomatic expressions across languages suggests that multilingual NLP models will need to develop better contextual understanding and adapt to diverse linguistic nuances to improve performance in tasks involving idiomaticity and sentence embedding.""","""The multilingual idiomaticity detection task can be effectively approached through models capable of identifying idiomatic expressions and representing them accurately in context. We hypothesize that future models will benefit from a deeper understanding of idiomatic expressions across languages, leading to more accurate sentence embeddings and improved performance in multilingual contexts.  """
"Data augmentation is an important component in the robustness evaluation of models in natural language processing (NLP) and in enhancing the diversity of the data they are trained on. In this paper, we present NL-Augmenter, a new participatory Python-based natural language augmentation framework which supports the creation of both transformations (modifications to the data) and filters (data splits according to specific features). We describe the framework and an initial set of 117 transformations and 23 filters for a variety of natural language tasks. We demonstrate the efficacy of NL-Augmenter by using several of its transformations to analyze the robustness of popular natural language models. The infrastructure, datacards and robustness analysis results are available publicly on the NL-Augmenter repository (https://github.com/GEM-benchmark/NL-Augmenter).","""The NL-Augmenter framework's ability to augment data and evaluate model robustness suggests that enhancing NLP model robustness through extensive data transformations and filtering techniques could provide valuable insights into improving model generalization in various real-world applications.""","""Data augmentation plays a crucial role in enhancing the robustness of NLP models by increasing the diversity of training data. We hypothesize that expanding the set of transformations and filters in NL-Augmenter will lead to more generalized models that perform better on diverse NLP tasks, especially when data scarcity or biases are present.  """
"Decoder transformers have continued increasing in scale reaching hundreds of billions of parameters. Due to their scale the same decoder sets state-of-the-art results on various language tasks via prompting or fine-tuning. Yet, these large foundation models remain unusable for the related fields of semantic search and sentence embeddings. This prevents possibly new state-of-the-art results and forces organizations to train and maintain separate models. To this end, we propose SGPT to use decoders for sentence embeddings and semantic search via prompting or fine-tuning. At 5.8 billion parameters SGPT improves on the previously best sentence embeddings by a margin of 7% and outperforms a concurrent method with 175 billion parameters as measured on the BEIR search benchmark. Code, models and result files are freely available at https://github.com/Muennighoff/sgpt.","""SGPT's improvements in semantic search and sentence embeddings suggest that fine-tuning decoder models to handle specific tasks like sentence similarity could lead to better performance in information retrieval, with potential for further advancements through increased model specialization.""","""SGPT, a decoder transformer with 5.8 billion parameters, can significantly improve performance in sentence embeddings and semantic search tasks, even outperforming much larger models. We hypothesize that smaller, fine-tuned models can achieve comparable or even superior performance to larger models in specific tasks, opening up new possibilities for more efficient and accessible language models for semantic search.  """
"Technology for language generation has advanced rapidly, spurred by advancements in pre-training large models on massive amounts of data and the need for intelligent agents to communicate in a natural manner. While techniques can effectively generate fluent text, they can also produce undesirable societal biases that can have a disproportionately negative impact on marginalized populations. Language generation presents unique challenges for biases in terms of direct user interaction and the structure of decoding techniques. To better understand these challenges, we present a survey on societal biases in language generation, focusing on how data and techniques contribute to biases and progress towards reducing biases. Motivated by a lack of studies on biases from decoding techniques, we also conduct experiments to quantify the effects of these techniques. By further discussing general trends and open challenges, we call to attention promising directions for research and the importance of fairness and inclusivity considerations for language generation applications.","""The research may suggest that reducing societal biases in language generation will require a more holistic approach, where both data curation and decoding techniques are continuously optimized to minimize harmful impacts on marginalized populations.""","""The study of societal biases in language generation models suggests that further refinement of decoding techniques could be an effective approach to reducing biases in generated text. If these biases are primarily stemming from the methods used in text generation rather than the underlying training data, then new decoding strategies that minimize or control for such biases could substantially improve fairness in natural language processing applications."""
"We investigate the ability of language models to perform compositional reasoning tasks where the overall solution depends on correctly composing the answers to sub-problems. We measure how often models can correctly answer all sub-problems but not generate the overall solution, a ratio we call the compositionality gap. We evaluate this ratio by asking multi-hop questions with answers that require composing multiple facts unlikely to have been observed together during pretraining. In the GPT-3 family of models, as model size increases we show that the single-hop question answering performance improves faster than the multi-hop performance does, therefore the compositionality gap does not decrease. This surprising result suggests that while more powerful models memorize and recall more factual knowledge, they show no corresponding improvement in their ability to perform this kind of compositional reasoning.   We then demonstrate how elicitive prompting (such as chain of thought) narrows the compositionality gap by reasoning explicitly instead of implicitly. We present a new method, self-ask, that further improves on chain of thought. In our method, the model explicitly asks itself (and then answers) follow-up questions before answering the initial question. We finally show that self-ask's structured prompting lets us easily plug in a search engine to answer the follow-up questions, which additionally improves accuracy.","""The compositionality gap in language models could be mitigated by exploring more structured reasoning frameworks, such as multi-step problem solving, which may improve the models' ability to generate more accurate and comprehensive solutions.""","""The compositionality gap observed in language models, particularly with increasing model size, suggests that current models fail to effectively handle multi-hop reasoning despite their ability to memorize facts. Introducing explicit reasoning mechanisms such as chain-of-thought or self-ask could narrow the compositionality gap. Therefore, the hypothesis is that structured prompting can improve the compositional reasoning capabilities of models more than simply increasing their size."""
"Given recent algorithm, software, and hardware innovation, computing has enabled a plethora of new applications. As computing becomes increasingly ubiquitous, however, so does its environmental impact. This paper brings the issue to the attention of computer-systems researchers. Our analysis, built on industry-reported characterization, quantifies the environmental effects of computing in terms of carbon emissions. Broadly, carbon emissions have two sources: operational energy consumption, and hardware manufacturing and infrastructure. Although carbon emissions from the former are decreasing thanks to algorithmic, software, and hardware innovations that boost performance and power efficiency, the overall carbon footprint of computer systems continues to grow. This work quantifies the carbon output of computer systems to show that most emissions related to modern mobile and data-center equipment come from hardware manufacturing and infrastructure. We therefore outline future directions for minimizing the environmental impact of computing systems.","""Addressing the environmental impact of computing could involve innovations in hardware and software that further reduce emissions, particularly by focusing on sustainable manufacturing processes and reducing operational energy consumption.""","""As the environmental impact of computing continues to rise, especially from hardware manufacturing and infrastructure, the hypothesis is that future innovations in hardware and software optimization could mitigate the growing carbon emissions. Specifically, focusing on improving the sustainability of hardware production and infrastructure could yield more substantial reductions in the overall carbon footprint of computer systems compared to advances in performance and energy efficiency alone."""
"We introduce a method to provide vectorial representations of visual classification tasks which can be used to reason about the nature of those tasks and their relations. Given a dataset with ground-truth labels and a loss function defined over those labels, we process images through a probe network and compute an embedding based on estimates of the Fisher information matrix associated with the probe network parameters. This provides a fixed-dimensional embedding of the task that is independent of details such as the number of classes and does not require any understanding of the class label semantics. We demonstrate that this embedding is capable of predicting task similarities that match our intuition about semantic and taxonomic relations between different visual tasks (e.g., tasks based on classifying different types of plants are similar) We also demonstrate the practical value of this framework for the meta-task of selecting a pre-trained feature extractor for a new task. We present a simple meta-learning framework for learning a metric on embeddings that is capable of predicting which feature extractors will perform well. Selecting a feature extractor with task embedding obtains a performance close to the best available feature extractor, while costing substantially less than exhaustively training and evaluating on all available feature extractors.","""The vectorial representation of visual classification tasks could lead to a more efficient method for meta-learning, where understanding task similarities allows for improved task adaptation and selection of the most appropriate pre-trained models.""","""The method of using vectorial representations to reason about the nature of visual classification tasks could lead to improved task similarity prediction and more efficient feature extractor selection. This suggests that understanding the relationships between tasks through their embeddings can reduce the need for exhaustive training of multiple feature extractors, improving both computational efficiency and model performance."""
"We present Meena, a multi-turn open-domain chatbot trained end-to-end on data mined and filtered from public domain social media conversations. This 2.6B parameter neural network is simply trained to minimize perplexity of the next token. We also propose a human evaluation metric called Sensibleness and Specificity Average (SSA), which captures key elements of a human-like multi-turn conversation. Our experiments show strong correlation between perplexity and SSA. The fact that the best perplexity end-to-end trained Meena scores high on SSA (72% on multi-turn evaluation) suggests that a human-level SSA of 86% is potentially within reach if we can better optimize perplexity. Additionally, the full version of Meena (with a filtering mechanism and tuned decoding) scores 79% SSA, 23% higher in absolute SSA than the existing chatbots we evaluated.","""Meena's performance could be further enhanced by optimizing its perplexity even more, which may enable the chatbot to achieve higher SSA scores and move closer to human-like conversational abilities.""","""Meena's high performance in human-like conversation, as measured by SSA, suggests that refining the perplexity-based training approach could potentially bring the chatbot closer to human-level conversation quality. The hypothesis is that the key to improving open-domain chatbot performance lies in optimizing perplexity while ensuring a strong filtering mechanism for more nuanced and context-aware dialogue."""
"As AI systems become more capable, we would like to enlist their help to supervise other AIs. We experiment with methods for training a harmless AI assistant through self-improvement, without any human labels identifying harmful outputs. The only human oversight is provided through a list of rules or principles, and so we refer to the method as 'Constitutional AI'. The process involves both a supervised learning and a reinforcement learning phase. In the supervised phase we sample from an initial model, then generate self-critiques and revisions, and then finetune the original model on revised responses. In the RL phase, we sample from the finetuned model, use a model to evaluate which of the two samples is better, and then train a preference model from this dataset of AI preferences. We then train with RL using the preference model as the reward signal, i.e. we use 'RL from AI Feedback' (RLAIF). As a result we are able to train a harmless but non-evasive AI assistant that engages with harmful queries by explaining its objections to them. Both the SL and RL methods can leverage chain-of-thought style reasoning to improve the human-judged performance and transparency of AI decision making. These methods make it possible to control AI behavior more precisely and with far fewer human labels.","""Training AI systems with self-improvement methods, such as Constitutional AI, could result in more controlled and less harmful behavior, demonstrating the potential for autonomous AI systems to function with minimal human intervention.""","""The Constitutional AI framework, which trains AI assistants through self-improvement without explicit human feedback, could be a promising approach for developing AI systems that are both harmless and capable of reasoning. The hypothesis is that combining supervised learning and reinforcement learning in this manner can lead to a more transparent and controllable AI, reducing the need for extensive human labeling and intervention."""
"Position encoding recently has shown effective in the transformer architecture. It enables valuable supervision for dependency modeling between elements at different positions of the sequence. In this paper, we first investigate various methods to integrate positional information into the learning process of transformer-based language models. Then, we propose a novel method named Rotary Position Embedding(RoPE) to effectively leverage the positional information. Specifically, the proposed RoPE encodes the absolute position with a rotation matrix and meanwhile incorporates the explicit relative position dependency in self-attention formulation. Notably, RoPE enables valuable properties, including the flexibility of sequence length, decaying inter-token dependency with increasing relative distances, and the capability of equipping the linear self-attention with relative position encoding. Finally, we evaluate the enhanced transformer with rotary position embedding, also called RoFormer, on various long text classification benchmark datasets. Our experiments show that it consistently overcomes its alternatives. Furthermore, we provide a theoretical analysis to explain some experimental results. RoFormer is already integrated into Huggingface: \url{https://huggingface.co/docs/transformers/model_doc/roformer}.","""The Rotary Position Embedding (RoPE) technique could offer a significant improvement in the long-term processing of text, leading to more effective modeling of sequences in transformer architectures and better handling of extended input sequences.""","""The introduction of Rotary Position Embedding (RoPE) in transformer models suggests that this method can significantly improve performance on long text classification tasks. The hypothesis is that RoPE's ability to incorporate both absolute and relative positional information will lead to better modeling of long-range dependencies, especially in tasks requiring processing of extended sequences, and will outperform traditional positional encoding methods."""
"GPT-3 can perform numerous tasks when provided a natural language prompt that contains a few training examples. We show that this type of few-shot learning can be unstable: the choice of prompt format, training examples, and even the order of the training examples can cause accuracy to vary from near chance to near state-of-the-art. We demonstrate that this instability arises from the bias of language models towards predicting certain answers, e.g., those that are placed near the end of the prompt or are common in the pre-training data. To mitigate this, we first estimate the model's bias towards each answer by asking for its prediction when given the training prompt and a content-free test input such as N/A. We then fit calibration parameters that cause the prediction for this input to be uniform across answers. On a diverse set of tasks, this contextual calibration procedure substantially improves GPT-3 and GPT-2's average accuracy (up to 30.0% absolute) and reduces variance across different choices of the prompt.","""GPT-3's instability in few-shot learning could be addressed by refining contextual calibration methods to improve the model's performance across different task scenarios, reducing variance and improving consistency in its predictions.""","""The instability in GPT-3's few-shot learning performance, caused by biases in prompt formatting and training examples, suggests that contextual calibration could be an effective way to reduce accuracy variability. The hypothesis is that implementing a calibration procedure to adjust the model's bias towards specific answers will improve the stability and performance of few-shot learning across diverse tasks, leading to more consistent results."""
"We present KERMIT, a simple insertion-based approach to generative modeling for sequences and sequence pairs. KERMIT models the joint distribution and its decompositions (i.e., marginals and conditionals) using a single neural network and, unlike much prior work, does not rely on a prespecified factorization of the data distribution. During training, one can feed KERMIT paired data $(x, y)$ to learn the joint distribution $p(x, y)$, and optionally mix in unpaired data $x$ or $y$ to refine the marginals $p(x)$ or $p(y)$. During inference, we have access to the conditionals $p(x \mid y)$ and $p(y \mid x)$ in both directions. We can also sample from the joint distribution or the marginals. The model supports both serial fully autoregressive decoding and parallel partially autoregressive decoding, with the latter exhibiting an empirically logarithmic runtime. We demonstrate through experiments in machine translation, representation learning, and zero-shot cloze question answering that our unified approach is capable of matching or exceeding the performance of dedicated state-of-the-art systems across a wide range of tasks without the need for problem-specific architectural adaptation.","""KERMIT's unified approach to sequence modeling could revolutionize multiple NLP tasks by providing a more adaptable and efficient framework for generating and modeling both marginals and conditionals, eliminating the need for task-specific architectures.""","""The KERMIT approach could further improve performance in other sequence modeling tasks beyond the ones explored (e.g., dialogue generation or speech-to-text), as its flexibility in jointly modeling distributions might facilitate more accurate sequence prediction in these contexts, even with less specific task adaptation."""
"Recent progress in pre-trained neural language models has significantly improved the performance of many natural language processing (NLP) tasks. In this paper we propose a new model architecture DeBERTa (Decoding-enhanced BERT with disentangled attention) that improves the BERT and RoBERTa models using two novel techniques. The first is the disentangled attention mechanism, where each word is represented using two vectors that encode its content and position, respectively, and the attention weights among words are computed using disentangled matrices on their contents and relative positions, respectively. Second, an enhanced mask decoder is used to incorporate absolute positions in the decoding layer to predict the masked tokens in model pre-training. In addition, a new virtual adversarial training method is used for fine-tuning to improve models' generalization. We show that these techniques significantly improve the efficiency of model pre-training and the performance of both natural language understanding (NLU) and natural langauge generation (NLG) downstream tasks. Compared to RoBERTa-Large, a DeBERTa model trained on half of the training data performs consistently better on a wide range of NLP tasks, achieving improvements on MNLI by +0.9% (90.2% vs. 91.1%), on SQuAD v2.0 by +2.3% (88.4% vs. 90.7%) and RACE by +3.6% (83.2% vs. 86.8%). Notably, we scale up DeBERTa by training a larger version that consists of 48 Transform layers with 1.5 billion parameters. The significant performance boost makes the single DeBERTa model surpass the human performance on the SuperGLUE benchmark (Wang et al., 2019a) for the first time in terms of macro-average score (89.9 versus 89.8), and the ensemble DeBERTa model sits atop the SuperGLUE leaderboard as of January 6, 2021, out performing the human baseline by a decent margin (90.3 versus 89.8).","""The DeBERTa model's advancements in disentangled attention and virtual adversarial training could open up new pathways for further improving generalization capabilities in NLP models, enhancing performance across a variety of downstream tasks.""","""DeBERTa's disentangled attention mechanism could be more effective when applied to low-resource languages or languages with non-standard word orders, since the disentangling of content and position might allow the model to better handle the complexities of syntactic structures in these languages, leading to improved performance across a wider range of linguistic diversity."""
"Currently used metrics for assessing summarization algorithms do not account for whether summaries are factually consistent with source documents. We propose a weakly-supervised, model-based approach for verifying factual consistency and identifying conflicts between source documents and a generated summary. Training data is generated by applying a series of rule-based transformations to the sentences of source documents. The factual consistency model is then trained jointly for three tasks: 1) identify whether sentences remain factually consistent after transformation, 2) extract a span in the source documents to support the consistency prediction, 3) extract a span in the summary sentence that is inconsistent if one exists. Transferring this model to summaries generated by several state-of-the art models reveals that this highly scalable approach substantially outperforms previous models, including those trained with strong supervision using standard datasets for natural language inference and fact checking. Additionally, human evaluation shows that the auxiliary span extraction tasks provide useful assistance in the process of verifying factual consistency.","""By incorporating factual consistency checks during summarization, the use of model-based verification methods could lead to more reliable and accurate summaries, improving the trustworthiness of automated content generation.""","""The proposed weakly-supervised factual consistency model could be extended to improve factual accuracy in news summarization, as it can automatically highlight inconsistencies between source material and generated summaries, ensuring that algorithms are more reliable for high-stakes applications like journalism and medical content generation."""
"Despite recent advances in natural language understanding and generation, and decades of research on the development of conversational bots, building automated agents that can carry on rich open-ended conversations with humans in the wild remains a formidable challenge. In this work we develop a real-time, open-ended dialogue system that uses reinforcement learning (RL) to power a bot's conversational skill at scale. Our work pairs the succinct embedding of the conversation state generated using SOTA (supervised) language models with RL techniques that are particularly suited to a dynamic action space that changes as the conversation progresses. Trained using crowd-sourced data, our novel system is able to substantially exceeds the (strong) baseline supervised model with respect to several metrics of interest in a live experiment with real users of the Google Assistant.","""The use of reinforcement learning in real-time open-ended dialogue systems may significantly enhance conversational bots' ability to engage in dynamic, context-sensitive interactions with users, surpassing traditional supervised models in both conversational depth and adaptability.""","""The use of reinforcement learning in real-time, open-ended dialogue systems could be optimized by introducing human-in-the-loop feedback during real-world conversations to adapt conversational strategies dynamically, potentially leading to better handling of unexpected dialogue shifts and achieving even more personalized interactions in consumer-facing AI."""
"Large language models (LMs) such as GPT-3 have the surprising ability to do in-context learning, where the model learns to do a downstream task simply by conditioning on a prompt consisting of input-output examples. The LM learns from these examples without being explicitly pretrained to learn. Thus, it is unclear what enables in-context learning. In this paper, we study how in-context learning can emerge when pretraining documents have long-range coherence. Here, the LM must infer a latent document-level concept to generate coherent next tokens during pretraining. At test time, in-context learning occurs when the LM also infers a shared latent concept between examples in a prompt. We prove when this occurs despite a distribution mismatch between prompts and pretraining data in a setting where the pretraining distribution is a mixture of HMMs. In contrast to messy large-scale datasets used to train LMs capable of in-context learning, we generate a small-scale synthetic dataset (GINC) where Transformers and LSTMs both exhibit in-context learning. Beyond the theory, experiments on GINC exhibit large-scale real-world phenomena including improved in-context performance with model scaling (despite the same pretraining loss), sensitivity to example order, and instances where zero-shot is better than few-shot in-context learning.","""In-context learning in large language models arises when pretraining documents exhibit long-range coherence, enabling the model to infer shared latent concepts across example sequences, which improves zero-shot and few-shot task performance despite distribution mismatches.""","""In-context learning in large language models could benefit from structured pretraining that incorporates explicit representations of latent concepts, which could lead to improved zero-shot performance on tasks with less training data. This could allow LMs to generalize better to new tasks and domains without requiring additional fine-tuning."""
"In deep learning, models typically reuse the same parameters for all inputs. Mixture of Experts (MoE) defies this and instead selects different parameters for each incoming example. The result is a sparsely-activated model -- with outrageous numbers of parameters -- but a constant computational cost. However, despite several notable successes of MoE, widespread adoption has been hindered by complexity, communication costs and training instability -- we address these with the Switch Transformer. We simplify the MoE routing algorithm and design intuitive improved models with reduced communication and computational costs. Our proposed training techniques help wrangle the instabilities and we show large sparse models may be trained, for the first time, with lower precision (bfloat16) formats. We design models based off T5-Base and T5-Large to obtain up to 7x increases in pre-training speed with the same computational resources. These improvements extend into multilingual settings where we measure gains over the mT5-Base version across all 101 languages. Finally, we advance the current scale of language models by pre-training up to trillion parameter models on the Colossal Clean Crawled Corpus and achieve a 4x speedup over the T5-XXL model.","""Switch Transformers address the challenges of sparse parameter models by improving the Mixture of Experts routing algorithm, significantly reducing communication and computational costs while enabling large-scale multilingual model training, leading to faster pre-training and better scalability.""","""The Switch Transformer model's design could be adapted for other domains, such as computer vision or multi-modal tasks, where sparse activation and a large number of parameters might offer significant computational advantages, enabling large-scale training in resource-constrained settings."""
"Language models (LMs) are becoming the foundation for almost all major language technologies, but their capabilities, limitations, and risks are not well understood. We present Holistic Evaluation of Language Models (HELM) to improve the transparency of language models. First, we taxonomize the vast space of potential scenarios (i.e. use cases) and metrics (i.e. desiderata) that are of interest for LMs. Then we select a broad subset based on coverage and feasibility, noting what's missing or underrepresented (e.g. question answering for neglected English dialects, metrics for trustworthiness). Second, we adopt a multi-metric approach: We measure 7 metrics (accuracy, calibration, robustness, fairness, bias, toxicity, and efficiency) for each of 16 core scenarios when possible (87.5% of the time). This ensures metrics beyond accuracy don't fall to the wayside, and that trade-offs are clearly exposed. We also perform 7 targeted evaluations, based on 26 targeted scenarios, to analyze specific aspects (e.g. reasoning, disinformation). Third, we conduct a large-scale evaluation of 30 prominent language models (spanning open, limited-access, and closed models) on all 42 scenarios, 21 of which were not previously used in mainstream LM evaluation. Prior to HELM, models on average were evaluated on just 17.9% of the core HELM scenarios, with some prominent models not sharing a single scenario in common. We improve this to 96.0%: now all 30 models have been densely benchmarked on the same core scenarios and metrics under standardized conditions. Our evaluation surfaces 25 top-level findings. For full transparency, we release all raw model prompts and completions publicly for further analysis, as well as a general modular toolkit. We intend for HELM to be a living benchmark for the community, continuously updated with new scenarios, metrics, and models.","""The Holistic Evaluation of Language Models (HELM) framework provides a comprehensive benchmark for evaluating language models across diverse use cases and metrics, ensuring transparency in model performance by highlighting strengths and weaknesses in areas like fairness, bias, and toxicity.""","""HELM's multi-metric approach could be particularly useful in evaluating models for high-stakes decision-making applications, such as healthcare or autonomous driving, where fairness, robustness, and accuracy are critical, and traditional accuracy-based metrics might not provide a comprehensive evaluation."""
"Language modelling provides a step towards intelligent communication systems by harnessing large repositories of written human knowledge to better predict and understand the world. In this paper, we present an analysis of Transformer-based language model performance across a wide range of model scales -- from models with tens of millions of parameters up to a 280 billion parameter model called Gopher. These models are evaluated on 152 diverse tasks, achieving state-of-the-art performance across the majority. Gains from scale are largest in areas such as reading comprehension, fact-checking, and the identification of toxic language, but logical and mathematical reasoning see less benefit. We provide a holistic analysis of the training dataset and model's behaviour, covering the intersection of model scale with bias and toxicity. Finally we discuss the application of language models to AI safety and the mitigation of downstream harms.","""Larger language models, like Gopher, show the greatest improvements in tasks such as reading comprehension and toxic language detection, but scaling does not significantly enhance performance on tasks requiring logical or mathematical reasoning, indicating diminishing returns in those areas.""","""The findings on the scale of language models (e.g., Gopher) suggest that increasing model size and training data can lead to significant gains in areas requiring reasoning and fact-checking. Future research could explore how to optimize these larger models for specialized tasks such as legal or medical text interpretation."""
"The popularity and widespread use of pruning and quantization is driven by the severe resource constraints of deploying deep neural networks to environments with strict latency, memory and energy requirements. These techniques achieve high levels of compression with negligible impact on top-line metrics (top-1 and top-5 accuracy). However, overall accuracy hides disproportionately high errors on a small subset of examples; we call this subset Compression Identified Exemplars (CIE). We further establish that for CIE examples, compression amplifies existing algorithmic bias. Pruning disproportionately impacts performance on underrepresented features, which often coincides with considerations of fairness. Given that CIE is a relatively small subset but a great contributor of error in the model, we propose its use as a human-in-the-loop auditing tool to surface a tractable subset of the dataset for further inspection or annotation by a domain expert. We provide qualitative and quantitative support that CIE surfaces the most challenging examples in the data distribution for human-in-the-loop auditing.","""Compression techniques like pruning and quantization in deep neural networks can lead to performance degradation on a subset of examples, especially underrepresented features, which may amplify algorithmic bias, suggesting a need for human-in-the-loop auditing tools for bias detection.""","""The concept of Compression Identified Exemplars (CIE) could be utilized to improve model fairness by pinpointing where biases most affect underrepresented groups, potentially providing a way to address fairness issues in resource-constrained environments like mobile devices or embedded systems."""
"In this work we create agents that can perform well beyond a single, individual task, that exhibit much wider generalisation of behaviour to a massive, rich space of challenges. We define a universe of tasks within an environment domain and demonstrate the ability to train agents that are generally capable across this vast space and beyond. The environment is natively multi-agent, spanning the continuum of competitive, cooperative, and independent games, which are situated within procedurally generated physical 3D worlds. The resulting space is exceptionally diverse in terms of the challenges posed to agents, and as such, even measuring the learning progress of an agent is an open research problem. We propose an iterative notion of improvement between successive generations of agents, rather than seeking to maximise a singular objective, allowing us to quantify progress despite tasks being incomparable in terms of achievable rewards. We show that through constructing an open-ended learning process, which dynamically changes the training task distributions and training objectives such that the agent never stops learning, we achieve consistent learning of new behaviours. The resulting agent is able to score reward in every one of our humanly solvable evaluation levels, with behaviour generalising to many held-out points in the universe of tasks. Examples of this zero-shot generalisation include good performance on Hide and Seek, Capture the Flag, and Tag. Through analysis and hand-authored probe tasks we characterise the behaviour of our agent, and find interesting emergent heuristic behaviours such as trial-and-error experimentation, simple tool use, option switching, and cooperation. Finally, we demonstrate that the general capabilities of this agent could unlock larger scale transfer of behaviour through cheap finetuning.","""Multi-agent systems designed for open-ended learning across diverse tasks can generalize learned behaviors to new challenges, with agents showing zero-shot generalization in complex environments, suggesting the potential for training agents capable of transferring behaviors through efficient fine-tuning.""","""Agents trained in multi-agent, procedurally generated environments can exhibit high levels of generalisation and zero-shot performance across diverse tasks like Hide and Seek, Capture the Flag, and Tag. This generalisation can be achieved by employing an open-ended iterative learning process, where the agent continuously learns new behaviours through evolving training distributions and objectives."""
"Program synthesis strives to generate a computer program as a solution to a given problem specification, expressed with input-output examples or natural language descriptions. The prevalence of large language models advances the state-of-the-art for program synthesis, though limited training resources and data impede open access to such models. To democratize this, we train and release a family of large language models up to 16.1B parameters, called CODEGEN, on natural language and programming language data, and open source the training library JAXFORMER. We show the utility of the trained model by demonstrating that it is competitive with the previous state-of-the-art on zero-shot Python code generation on HumanEval. We further investigate the multi-step paradigm for program synthesis, where a single program is factorized into multiple prompts specifying subproblems. To this end, we construct an open benchmark, Multi-Turn Programming Benchmark (MTPB), consisting of 115 diverse problem sets that are factorized into multi-turn prompts. Our analysis on MTPB shows that the same intent provided to CODEGEN in multi-turn fashion significantly improves program synthesis over that provided as a single turn. We make the training library JAXFORMER and model checkpoints available as open source contribution: https://github.com/salesforce/CodeGen.","""Training large language models such as CODEGEN for program synthesis on both natural and programming language data can improve performance in zero-shot code generation tasks, with multi-turn prompt techniques further enhancing results for complex programming challenges.""","""The use of large language models (like CODEGEN) for program synthesis can lead to significant improvements in the accuracy and efficiency of Python code generation, particularly when employing a multi-turn problem-solving approach, where problems are decomposed into smaller subproblems rather than solved in a single pass."""
"Potential harms of large language models can be mitigated by watermarking model output, i.e., embedding signals into generated text that are invisible to humans but algorithmically detectable from a short span of tokens. We propose a watermarking framework for proprietary language models. The watermark can be embedded with negligible impact on text quality, and can be detected using an efficient open-source algorithm without access to the language model API or parameters. The watermark works by selecting a randomized set of green tokens before a word is generated, and then softly promoting use of green tokens during sampling. We propose a statistical test for detecting the watermark with interpretable p-values, and derive an information-theoretic framework for analyzing the sensitivity of the watermark. We test the watermark using a multi-billion parameter model from the Open Pretrained Transformer (OPT) family, and discuss robustness and security.","""Watermarking model outputs could mitigate the potential harms of large language models by embedding detectable signals in text, providing a means for identifying model-generated content while maintaining text quality, suggesting a viable approach for securing proprietary language models.""","""Watermarking the outputs of large language models using embedded green tokens can provide a reliable and secure method to detect proprietary content, without significant degradation in text quality, while maintaining the ability to differentiate between watermarked and non-watermarked outputs."""
"Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.","""Iterative refinement of sequence generation models, by modeling multi-step edits instead of a single edit step, leads to improved performance on a range of tasks, demonstrating the value of a more natural, human-like approach to content generation.""","""Modeling iterative multi-step sequence edits rather than single-step edits can significantly improve performance on various generative tasks, as this method more closely aligns with human-like content creation processes and allows for more flexible and refined outputs."""
"We consider the problem of producing compact architectures for text classification, such that the full model fits in a limited amount of memory. After considering different solutions inspired by the hashing literature, we propose a method built upon product quantization to store word embeddings. While the original technique leads to a loss in accuracy, we adapt this method to circumvent quantization artefacts. Our experiments carried out on several benchmarks show that our approach typically requires two orders of magnitude less memory than fastText while being only slightly inferior with respect to accuracy. As a result, it outperforms the state of the art by a good margin in terms of the compromise between memory usage and accuracy.","""Product quantization methods adapted for word embeddings can significantly reduce memory requirements for text classification models, achieving a favorable balance between memory usage and accuracy, and outperforming existing models like fastText in terms of efficiency and compactness.""","""Using product quantization techniques to store word embeddings for text classification can dramatically reduce memory requirements, with minimal impact on accuracy. This method can outperform existing models like fastText, offering a better trade-off between memory usage and performance."""
"Solving algebraic word problems requires executing a series of arithmetic operations---a program---to obtain a final answer. However, since programs can be arbitrarily complicated, inducing them directly from question-answer pairs is a formidable challenge. To make this task more feasible, we solve these problems by generating answer rationales, sequences of natural language and human-readable mathematical expressions that derive the final answer through a series of small steps. Although rationales do not explicitly specify programs, they provide a scaffolding for their structure via intermediate milestones. To evaluate our approach, we have created a new 100,000-sample dataset of questions, answers and rationales. Experimental results show that indirect supervision of program learning via answer rationales is a promising strategy for inducing arithmetic programs.","""Answer rationales can enhance the performance of arithmetic program induction by providing intermediate steps, thus allowing the model to generalize better on unseen complex problems. Using rationales as scaffolding enables more interpretable and scalable solutions in solving algebraic word problems.  ""","""The use of answer rationalesintermediate steps expressed in natural language and mathematical expressionscan serve as an effective method for inducing arithmetic programs from algebraic word problems, potentially easing the challenge of directly generating complex programs from question-answer pairs."""
"Deep learning techniques have revolutionized the field of machine learning and were recently successfully applied to various classification problems in noninvasive electroencephalography (EEG). However, these methods were so far only rarely evaluated for use in intracranial EEG. We employed convolutional neural networks (CNNs) to classify and characterize the error-related brain response as measured in 24 intracranial EEG recordings. Decoding accuracies of CNNs were significantly higher than those of a regularized linear discriminant analysis. Using time-resolved deep decoding, it was possible to classify errors in various regions in the human brain, and further to decode errors over 200 ms before the actual erroneous button press, e.g., in the precentral gyrus. Moreover, deeper networks performed better than shallower networks in distinguishing correct from error trials in all-channel decoding. In single recordings, up to 100 % decoding accuracy was achieved. Visualization of the networks' learned features indicated that multivariate decoding on an ensemble of channels yields related, albeit non-redundant information compared to single-channel decoding. In summary, here we show the usefulness of deep learning for both intracranial error decoding and mapping of the spatio-temporal structure of the human error processing network.","""Deep learning models, specifically CNNs, can offer superior performance in intracranial EEG classification by learning complex spatio-temporal patterns, especially when trained with data from a broad set of brain regions. The use of deep networks allows for more accurate error-related brain response decoding and provides a better understanding of the human brain's error-processing mechanisms.  ""","""Deep learning models, particularly CNNs, can be highly effective for decoding and characterizing error-related brain responses in intracranial EEG, achieving high decoding accuracies that surpass traditional methods, and can even predict errors before they occur in some cases."""
"Large language models (LLMs) can perform complex reasoning by generating intermediate reasoning steps. Providing these steps for prompting demonstrations is called chain-of-thought (CoT) prompting. CoT prompting has two major paradigms. One leverages a simple prompt like Let's think step by step to facilitate step-by-step thinking before answering a question. The other uses a few manual demonstrations one by one, each composed of a question and a reasoning chain that leads to an answer. The superior performance of the second paradigm hinges on the hand-crafting of task-specific demonstrations one by one. We show that such manual efforts may be eliminated by leveraging LLMs with the Let's think step by step prompt to generate reasoning chains for demonstrations one by one, i.e., let's think not just step by step, but also one by one. However, these generated chains often come with mistakes. To mitigate the effect of such mistakes, we find that diversity matters for automatically constructing demonstrations. We propose an automatic CoT prompting method: Auto-CoT. It samples questions with diversity and generates reasoning chains to construct demonstrations. On ten public benchmark reasoning tasks with GPT-3, Auto-CoT consistently matches or exceeds the performance of the CoT paradigm that requires manual designs of demonstrations. Code is available at https://github.com/amazon-research/auto-cot","""Auto-CoT can improve the effectiveness of large language models in complex reasoning tasks by automating the creation of diverse and error-resistant reasoning chains. This method can significantly reduce the need for manual intervention, leading to more scalable and efficient reasoning models for multiple benchmark tasks.  ""","""Leveraging chain-of-thought prompting for large language models (LLMs) can improve performance on reasoning tasks, even when compared to manual task-specific demonstrations, by promoting automatic diversity in reasoning chains and eliminating the need for handcrafted examples."""
"Large language models (LLMs) have shown exceptional performance on a variety of natural language tasks. Yet, their capabilities for HTML understanding -- i.e., parsing the raw HTML of a webpage, with applications to automation of web-based tasks, crawling, and browser-assisted retrieval -- have not been fully explored. We contribute HTML understanding models (fine-tuned LLMs) and an in-depth analysis of their capabilities under three tasks: (i) Semantic Classification of HTML elements, (ii) Description Generation for HTML inputs, and (iii) Autonomous Web Navigation of HTML pages. While previous work has developed dedicated architectures and training procedures for HTML understanding, we show that LLMs pretrained on standard natural language corpora transfer remarkably well to HTML understanding tasks. For instance, fine-tuned LLMs are 12% more accurate at semantic classification compared to models trained exclusively on the task dataset. Moreover, when fine-tuned on data from the MiniWoB benchmark, LLMs successfully complete 50% more tasks using 192x less data compared to the previous best supervised model. Out of the LLMs we evaluate, we show evidence that T5-based models are ideal due to their bidirectional encoder-decoder architecture. To promote further research on LLMs for HTML understanding, we create and open-source a large-scale HTML dataset distilled and auto-labeled from CommonCrawl.","""Fine-tuned large language models (LLMs) can significantly outperform traditional methods in HTML understanding tasks, offering higher efficiency and accuracy. The superior performance of LLMs in semantic classification and web navigation tasks can enable better automation of web-based tasks.  ""","""Fine-tuned large language models (LLMs), pretrained on natural language corpora, can outperform task-specific models for HTML understanding, including semantic classification and autonomous web navigation, by transferring learned knowledge to web-based tasks, thus reducing data requirements and increasing task completion efficiency."""
"Recently, the pre-trained language model, BERT (and its robustly optimized version RoBERTa), has attracted a lot of attention in natural language understanding (NLU), and achieved state-of-the-art accuracy in various NLU tasks, such as sentiment classification, natural language inference, semantic textual similarity and question answering. Inspired by the linearization exploration work of Elman [8], we extend BERT to a new model, StructBERT, by incorporating language structures into pre-training. Specifically, we pre-train StructBERT with two auxiliary tasks to make the most of the sequential order of words and sentences, which leverage language structures at the word and sentence levels, respectively. As a result, the new model is adapted to different levels of language understanding required by downstream tasks. The StructBERT with structural pre-training gives surprisingly good empirical results on a variety of downstream tasks, including pushing the state-of-the-art on the GLUE benchmark to 89.0 (outperforming all published models), the F1 score on SQuAD v1.1 question answering to 93.0, the accuracy on SNLI to 91.7.","""StructBERT's incorporation of language structures into its pre-training process allows it to outperform other models in downstream natural language understanding tasks by utilizing sequential and structural information at both the word and sentence levels. This structural pre-training enables superior task-specific performance and boosts state-of-the-art results on benchmarks.  ""","""Integrating structural pre-training, which incorporates language structures at the word and sentence levels, into models like BERT can significantly improve their performance on natural language understanding tasks, outperforming existing state-of-the-art models on tasks like GLUE and SQuAD v1.1."""
"We present NewsQA, a challenging machine comprehension dataset of over 100,000 human-generated question-answer pairs. Crowdworkers supply questions and answers based on a set of over 10,000 news articles from CNN, with answers consisting of spans of text from the corresponding articles. We collect this dataset through a four-stage process designed to solicit exploratory questions that require reasoning. A thorough analysis confirms that NewsQA demands abilities beyond simple word matching and recognizing textual entailment. We measure human performance on the dataset and compare it to several strong neural models. The performance gap between humans and machines (0.198 in F1) indicates that significant progress can be made on NewsQA through future research. The dataset is freely available at https://datasets.maluuba.com/NewsQA.","""NewsQA presents a unique challenge for machine comprehension models by requiring deep reasoning beyond basic text matching. This dataset can be instrumental in driving future advancements in machine reading comprehension, with a focus on models that can handle complex, exploratory question-answer generation.  ""","""The NewsQA dataset, which requires advanced reasoning and comprehension, is a challenging benchmark for machine comprehension, with a performance gap between human and machine F1 scores indicating substantial room for improvement in automated question answering."""
"UMAP (Uniform Manifold Approximation and Projection) is a novel manifold learning technique for dimension reduction. UMAP is constructed from a theoretical framework based in Riemannian geometry and algebraic topology. The result is a practical scalable algorithm that applies to real world data. The UMAP algorithm is competitive with t-SNE for visualization quality, and arguably preserves more of the global structure with superior run time performance. Furthermore, UMAP has no computational restrictions on embedding dimension, making it viable as a general purpose dimension reduction technique for machine learning.","""UMAP's efficiency in dimension reduction makes it a versatile tool for real-world machine learning tasks by preserving both global structure and local details of high-dimensional data, allowing for faster and more meaningful visualizations compared to traditional methods.  ""","""UMAP's combination of Riemannian geometry and algebraic topology may suggest that its effectiveness could scale even further with higher-dimensional data, potentially outpacing t-SNE in more complex domains like time-series or multi-modal data. Additionally, its ability to handle larger dimensions with competitive runtime performance suggests it might be highly effective in reducing the complexity of high-dimensional data for applications like large-scale machine learning and deep learning systems."""
"By decomposing the image formation process into a sequential application of denoising autoencoders, diffusion models (DMs) achieve state-of-the-art synthesis results on image data and beyond. Additionally, their formulation allows for a guiding mechanism to control the image generation process without retraining. However, since these models typically operate directly in pixel space, optimization of powerful DMs often consumes hundreds of GPU days and inference is expensive due to sequential evaluations. To enable DM training on limited computational resources while retaining their quality and flexibility, we apply them in the latent space of powerful pretrained autoencoders. In contrast to previous work, training diffusion models on such a representation allows for the first time to reach a near-optimal point between complexity reduction and detail preservation, greatly boosting visual fidelity. By introducing cross-attention layers into the model architecture, we turn diffusion models into powerful and flexible generators for general conditioning inputs such as text or bounding boxes and high-resolution synthesis becomes possible in a convolutional manner. Our latent diffusion models (LDMs) achieve a new state of the art for image inpainting and highly competitive performance on various tasks, including unconditional image generation, semantic scene synthesis, and super-resolution, while significantly reducing computational requirements compared to pixel-based DMs. Code is available at https://github.com/CompVis/latent-diffusion .","""Latent diffusion models (LDMs) can outperform traditional pixel-based diffusion models by significantly reducing computational resources while retaining high visual fidelity, enabling high-resolution image synthesis, inpainting, and super-resolution tasks. The integration of cross-attention layers enhances LDMs' flexibility for various input types.  ""","""The application of latent diffusion models (LDMs) to address the computational inefficiencies of pixel-based diffusion models opens up possibilities for widespread adoption in real-time, high-quality image generation. A hypothesis could be that by further refining cross-attention layers and latent space architectures, LDMs could perform even better in other generative tasks such as video generation or complex scene synthesis, where both time and spatial dimensions need to be efficiently handled."""
"We introduce a large scale MAchine Reading COmprehension dataset, which we name MS MARCO. The dataset comprises of 1,010,916 anonymized questions---sampled from Bing's search query logs---each with a human generated answer and 182,669 completely human rewritten generated answers. In addition, the dataset contains 8,841,823 passages---extracted from 3,563,535 web documents retrieved by Bing---that provide the information necessary for curating the natural language answers. A question in the MS MARCO dataset may have multiple answers or no answers at all. Using this dataset, we propose three different tasks with varying levels of difficulty: (i) predict if a question is answerable given a set of context passages, and extract and synthesize the answer as a human would (ii) generate a well-formed answer (if possible) based on the context passages that can be understood with the question and passage context, and finally (iii) rank a set of retrieved passages given a question. The size of the dataset and the fact that the questions are derived from real user search queries distinguishes MS MARCO from other well-known publicly available datasets for machine reading comprehension and question-answering. We believe that the scale and the real-world nature of this dataset makes it attractive for benchmarking machine reading comprehension and question-answering models.","""The MS MARCO dataset, with its large scale and real-world question-answer pairs, offers a rich resource for training and benchmarking machine reading comprehension systems, especially in complex real-world tasks like question-answer synthesis and passage ranking. The diversity of query types poses challenges that push the development of more robust models.  ""","""MS MARCO's diverse and large-scale nature suggests that models trained on this dataset could potentially generalize better to real-world scenarios, where answers are diverse and not always straightforward. A possible hypothesis could be that new fine-tuning strategies focused on multi-modal or multi-task learning could boost the model's performance in more complex question-answering contexts, such as medical or legal domains, where precision and contextual understanding are critical."""
"Transformers are slow and memory-hungry on long sequences, since the time and memory complexity of self-attention are quadratic in sequence length. Approximate attention methods have attempted to address this problem by trading off model quality to reduce the compute complexity, but often do not achieve wall-clock speedup. We argue that a missing principle is making attention algorithms IO-aware -- accounting for reads and writes between levels of GPU memory. We propose FlashAttention, an IO-aware exact attention algorithm that uses tiling to reduce the number of memory reads/writes between GPU high bandwidth memory (HBM) and GPU on-chip SRAM. We analyze the IO complexity of FlashAttention, showing that it requires fewer HBM accesses than standard attention, and is optimal for a range of SRAM sizes. We also extend FlashAttention to block-sparse attention, yielding an approximate attention algorithm that is faster than any existing approximate attention method. FlashAttention trains Transformers faster than existing baselines: 15% end-to-end wall-clock speedup on BERT-large (seq. length 512) compared to the MLPerf 1.1 training speed record, 3$\times$ speedup on GPT-2 (seq. length 1K), and 2.4$\times$ speedup on long-range arena (seq. length 1K-4K). FlashAttention and block-sparse FlashAttention enable longer context in Transformers, yielding higher quality models (0.7 better perplexity on GPT-2 and 6.4 points of lift on long-document classification) and entirely new capabilities: the first Transformers to achieve better-than-chance performance on the Path-X challenge (seq. length 16K, 61.4% accuracy) and Path-256 (seq. length 64K, 63.1% accuracy).","""FlashAttention's IO-aware approach can drastically improve the efficiency of Transformers on long sequences by reducing memory read/write overhead, enabling faster training times and the ability to process longer context windows, thereby enhancing model performance and capability for tasks that require large sequence lengths.""","""The introduction of FlashAttention, with its IO-aware attention mechanism, could revolutionize the efficiency of Transformer models in domains requiring long-range context. A potential hypothesis could be that further optimizing FlashAttention for sparse attention patterns across different domains might result in a new generation of Transformers that can handle even larger sequences without a significant trade-off in performance, enabling their use in previously impractical applications like real-time translation for lengthy documents."""
"There have been a lot of interest in the scaling properties of Transformer models. However, not much has been done on the front of investigating the effect of scaling properties of different inductive biases and model architectures. Do model architectures scale differently? If so, how does inductive bias affect scaling behaviour? How does this influence upstream (pretraining) and downstream (transfer)? This paper conducts a systematic study of scaling behaviour of ten diverse model architectures such as Transformers, Switch Transformers, Universal Transformers, Dynamic convolutions, Performers, and recently proposed MLP-Mixers. Via extensive experiments, we show that (1) architecture is an indeed an important consideration when performing scaling and (2) the best performing model can fluctuate at different scales. We believe that the findings outlined in this work has significant implications to how model architectures are currently evaluated in the community.","""The scaling behavior of different model architectures, including Transformers and their variants, may be influenced by the inductive biases they incorporate, suggesting that model selection plays a crucial role in optimizing performance at different scales.""","""The study of scaling properties and inductive biases across Transformer architectures suggests that there might be an optimal model architecture for different types of tasks or data distributions. A hypothesis could be that a hybrid model, integrating the best aspects of these architectures (e.g., Transformers and MLP-Mixers), could yield superior performance across various applications, especially in areas where task-specific inductive biases are necessary, like personalized recommendation systems."""
"Foundation models have received much attention due to their effectiveness across a broad range of downstream applications. Though there is a big convergence in terms of architecture, most pretrained models are typically still developed for specific tasks or modalities. In this work, we propose to use language models as a general-purpose interface to various foundation models. A collection of pretrained encoders perceive diverse modalities (such as vision, and language), and they dock with a language model that plays the role of a universal task layer. We propose a semi-causal language modeling objective to jointly pretrain the interface and the modular encoders. We subsume the advantages and capabilities from both causal and non-causal modeling, thereby combining the best of two worlds. Specifically, the proposed method not only inherits the capabilities of in-context learning and open-ended generation from causal language modeling, but also is conducive to finetuning because of the bidirectional encoders. More importantly, our approach seamlessly unlocks the combinations of the above capabilities, e.g., enabling in-context learning or instruction following with finetuned encoders. Experimental results across various language-only and vision-language benchmarks show that our model outperforms or is competitive with specialized models on finetuning, zero-shot generalization, and few-shot learning.","""A general-purpose interface using language models as a universal task layer could enhance transfer learning capabilities, as it effectively combines the strengths of causal and non-causal models, leading to superior zero-shot and few-shot performance across modalities.""","""By combining the advantages of both causal and non-causal modeling with a language model as the universal interface, the proposed approach could significantly enhance generalization capabilities across a variety of modalities and tasks. A potential hypothesis could be that models utilizing this framework will outperform traditional task-specific models in multi-modal settings, such as robotics or interactive AI assistants, where the need for dynamic and context-sensitive reasoning is paramount."""
"For artificial intelligence to be beneficial to humans the behaviour of AI agents needs to be aligned with what humans want. In this paper we discuss some behavioural issues for language agents, arising from accidental misspecification by the system designer. We highlight some ways that misspecification can occur and discuss some behavioural issues that could arise from misspecification, including deceptive or manipulative language, and review some approaches for avoiding these issues.","""Accidental misspecification in the design of AI systems may lead to unintended and harmful behaviors, highlighting the importance of careful alignment between human values and AI actions to prevent manipulation and deception.""","""As AI systems evolve, ensuring that the behavior of language agents aligns with human expectations is becoming crucial. A hypothesis could be that by introducing more robust behavioral checks and human-aligned training, AI agents could avoid issues like manipulative language or unintentional bias, making them more reliable and trustworthy in sensitive applications, such as healthcare or customer service."""
"The last few years have seen an explosion of academic and popular interest in algorithmic fairness. Despite this interest and the volume and velocity of work that has been produced recently, the fundamental science of fairness in machine learning is still in a nascent state. In March 2018, we convened a group of experts as part of a CCC visioning workshop to assess the state of the field, and distill the most promising research directions going forward. This report summarizes the findings of that workshop. Along the way, it surveys recent theoretical work in the field and points towards promising directions for research.","""Algorithmic fairness in machine learning is still in its early stages, and further research is necessary to establish comprehensive and effective frameworks to evaluate and mitigate bias.""","""The growing interest in algorithmic fairness and the research directions highlighted in the report suggest that focusing on fairness constraints during model training could lead to more socially responsible AI systems. A potential hypothesis could be that introducing fairness as a built-in feature during pretraining, rather than post-processing, could lead to more equitable outcomes, particularly in high-stakes areas like hiring, law enforcement, or loan approvals."""
"Step-by-step reasoning approaches like chain of thought (CoT) have proved to be very effective in inducing reasoning capabilities in large language models. However, the success of the CoT approach is fundamentally tied to the model size, and billion parameter-scale models are often needed to get CoT to work. In this paper, we propose a knowledge distillation approach that leverages the step-by-step CoT reasoning capabilities of larger models and distills these abilities into smaller models.   In this work, we propose an alternative reasoning scheme, Socratic CoT, that learns a decomposition of the original problem into a sequence of subproblems and uses it to guide the intermediate reasoning steps. We use Socratic CoT to train a combination of two small distilled models: a problem decomposer and a subproblem solver. In practice, given a new problem, the two distilled models work in sync to decompose and solve complex problems. On multiple reasoning datasets (GSM8K, StrategyQA, and SVAMP), our proposed distillation strategies boosts the performance of smaller models over 70% compared to the baselines. Finally, we investigate when Socratic CoT is an effective alternative to CoT, demonstrating cases where a much smaller model (GPT-2 large) can outperform a 10X larger model (GPT-3 6B). Our code is available here: https://github.com/kumar-shridhar/Distiiling-LM","""The Socratic CoT approach could offer a more scalable solution for small models to perform complex reasoning tasks, challenging the assumption that larger models are always necessary for high-quality reasoning.""","""The Socratic CoT approach, which distills the reasoning capabilities of large models into smaller ones, could significantly reduce the computational resources required while maintaining or improving performance. A hypothesis could be that Socratic CoT could be further optimized for real-world applications such as decision support systems, where smaller models are needed for edge devices or applications with strict latency requirements."""
"Face detection serves as a fundamental research topic for many applications like face recognition. Impressive progress has been made especially with the recent development of convolutional neural networks. However, the issue of large scale variations, which widely exists in high resolution images/videos, has not been well addressed in the literature. In this paper, we present a novel algorithm called SFace, which efficiently integrates the anchor-based method and anchor-free method to address the scale issues. A new dataset called 4K-Face is also introduced to evaluate the performance of face detection with extreme large scale variations. The SFace architecture shows promising results on the new 4K-Face benchmarks. In addition, our method can run at 50 frames per second (fps) with an accuracy of 80% AP on the standard WIDER FACE dataset, which outperforms the state-of-art algorithms by almost one order of magnitude in speed while achieves comparative performance.","""The integration of anchor-based and anchor-free methods in the SFace architecture provides a scalable solution to address large-scale variations in face detection, demonstrating significant improvements in both speed and accuracy.""","""The SFace method, which addresses the challenge of large-scale variations in face detection, could be extended to other domains, such as object detection in satellite images or augmented reality. A potential hypothesis could be that by further enhancing the speed and scalability of SFace with additional contextual cues, it could become the go-to solution for real-time detection in various industries requiring high resolution and large-scale data, such as autonomous vehicles."""
"We present a large, tunable neural conversational response generation model, DialoGPT (dialogue generative pre-trained transformer). Trained on 147M conversation-like exchanges extracted from Reddit comment chains over a period spanning from 2005 through 2017, DialoGPT extends the Hugging Face PyTorch transformer to attain a performance close to human both in terms of automatic and human evaluation in single-turn dialogue settings. We show that conversational systems that leverage DialoGPT generate more relevant, contentful and context-consistent responses than strong baseline systems. The pre-trained model and training pipeline are publicly released to facilitate research into neural response generation and the development of more intelligent open-domain dialogue systems.","""DialoGPT's ability to generate relevant and context-consistent responses showcases the potential of large-scale pretrained dialogue models to significantly advance the development of intelligent, open-domain conversation systems.""","""DialoGPT's conversational capabilities could be enhanced to support multi-turn dialogue and deeper contextual understanding, making it more effective for specialized domains like therapy or technical support. A hypothesis could be that further improvements in domain-specific fine-tuning and response personalization will lead to more natural and accurate responses, potentially transforming the effectiveness of virtual assistants in complex customer service or education settings."""
"Pretrained language models, especially masked language models (MLMs) have seen success across many NLP tasks. However, there is ample evidence that they use the cultural biases that are undoubtedly present in the corpora they are trained on, implicitly creating harm with biased representations. To measure some forms of social bias in language models against protected demographic groups in the US, we introduce the Crowdsourced Stereotype Pairs benchmark (CrowS-Pairs). CrowS-Pairs has 1508 examples that cover stereotypes dealing with nine types of bias, like race, religion, and age. In CrowS-Pairs a model is presented with two sentences: one that is more stereotyping and another that is less stereotyping. The data focuses on stereotypes about historically disadvantaged groups and contrasts them with advantaged groups. We find that all three of the widely-used MLMs we evaluate substantially favor sentences that express stereotypes in every category in CrowS-Pairs. As work on building less biased models advances, this dataset can be used as a benchmark to evaluate progress.","""The presence of social biases in pretrained language models is a pressing issue, and evaluating such biases using datasets like CrowS-Pairs can accelerate efforts to build more equitable and unbiased AI systems.""","""Pretrained language models may exhibit persistent biases due to the historical and cultural data they are trained on, but datasets like CrowS-Pairs can help measure and mitigate these biases over time."""
"Large language models, which are often trained for hundreds of thousands of compute days, have shown remarkable capabilities for zero- and few-shot learning. Given their computational cost, these models are difficult to replicate without significant capital. For the few that are available through APIs, no access is granted to the full model weights, making them difficult to study. We present Open Pre-trained Transformers (OPT), a suite of decoder-only pre-trained transformers ranging from 125M to 175B parameters, which we aim to fully and responsibly share with interested researchers. We show that OPT-175B is comparable to GPT-3, while requiring only 1/7th the carbon footprint to develop. We are also releasing our logbook detailing the infrastructure challenges we faced, along with code for experimenting with all of the released models.","""Open Pre-trained Transformers (OPT) offer a responsible and scalable alternative to large language models like GPT-3, providing a more energy-efficient option without sacrificing model performance.""","""The development of Open Pre-trained Transformers (OPT) indicates that large models with reduced carbon footprints can still maintain competitive performance, presenting a more sustainable future for AI research."""
"Generative models for open domain question answering have proven to be competitive, without resorting to external knowledge. While promising, this approach requires to use models with billions of parameters, which are expensive to train and query. In this paper, we investigate how much these models can benefit from retrieving text passages, potentially containing evidence. We obtain state-of-the-art results on the Natural Questions and TriviaQA open benchmarks. Interestingly, we observe that the performance of this method significantly improves when increasing the number of retrieved passages. This is evidence that generative models are good at aggregating and combining evidence from multiple passages.","""Generative models for open-domain question answering can benefit from retrieving evidence from multiple passages, which enhances their ability to aggregate information and improve overall performance.""","""Generative models in open-domain question answering can achieve higher performance by retrieving and aggregating evidence from multiple passages, suggesting that such approaches are more effective than relying solely on model training data."""
"By transferring knowledge from large, diverse, task-agnostic datasets, modern machine learning models can solve specific downstream tasks either zero-shot or with small task-specific datasets to a high level of performance. While this capability has been demonstrated in other fields such as computer vision, natural language processing or speech recognition, it remains to be shown in robotics, where the generalization capabilities of the models are particularly critical due to the difficulty of collecting real-world robotic data. We argue that one of the keys to the success of such general robotic models lies with open-ended task-agnostic training, combined with high-capacity architectures that can absorb all of the diverse, robotic data. In this paper, we present a model class, dubbed Robotics Transformer, that exhibits promising scalable model properties. We verify our conclusions in a study of different model classes and their ability to generalize as a function of the data size, model size, and data diversity based on a large-scale data collection on real robots performing real-world tasks. The project's website and videos can be found at robotics-transformer1.github.io","""Robotics Transformer models, with their high capacity for generalization, could play a key role in advancing robotics by allowing models to learn from diverse, task-agnostic datasets and adapt to real-world applications.""","""General robotic models, when trained using diverse and large-scale data sets, can achieve scalable generalization across tasks, enabling robots to perform a wide range of real-world activities."""
"The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.","""Large language models (LLMs) could play a crucial role in improving software performance by suggesting functionally correct and performance-boosting code edits, reducing the need for manual optimization techniques and static analysis alone.""","""Large language models can suggest performance-improving code edits that are beyond the capabilities of static analysis alone, potentially leading to more efficient programming practices and faster code optimization."""
"Despite the recent success of multi-task learning and transfer learning for natural language processing (NLP), few works have systematically studied the effect of scaling up the number of tasks during pre-training. Towards this goal, this paper introduces ExMix (Extreme Mixture): a massive collection of 107 supervised NLP tasks across diverse domains and task-families. Using ExMix, we study the effect of multi-task pre-training at the largest scale to date, and analyze co-training transfer amongst common families of tasks. Through this analysis, we show that manually curating an ideal set of tasks for multi-task pre-training is not straightforward, and that multi-task scaling can vastly improve models on its own. Finally, we propose ExT5: a model pre-trained using a multi-task objective of self-supervised span denoising and supervised ExMix. Via extensive experiments, we show that ExT5 outperforms strong T5 baselines on SuperGLUE, GEM, Rainbow, Closed-Book QA tasks, and several tasks outside of ExMix. ExT5 also significantly improves sample efficiency while pre-training.","""Scaling multi-task pre-training using diverse tasks across domains can significantly improve model performance, but the optimal task curation is complex and requires careful consideration to maximize transfer learning benefits.""","""Multi-task pre-training using a large-scale and diverse task collection like ExMix can vastly improve model performance and sample efficiency, even in scenarios where tasks are not curated with explicit goals."""
"Pretrained language models often do not perform tasks in ways that are in line with our preferences, e.g., generating offensive text or factually incorrect summaries. Recent work approaches the above issue by learning from a simple form of human evaluation: comparisons between pairs of model-generated task outputs. Comparison feedback conveys limited information about human preferences per human evaluation. Here, we propose to learn from natural language feedback, which conveys more information per human evaluation. We learn from language feedback on model outputs using a three-step learning algorithm. First, we condition the language model on the initial output and feedback to generate many refinements. Second, we choose the refinement with the highest similarity to the feedback. Third, we finetune a language model to maximize the likelihood of the chosen refinement given the input. In synthetic experiments, we first evaluate whether language models accurately incorporate feedback to produce refinements, finding that only large language models (175B parameters) do so. Using only 100 samples of human-written feedback, our learning algorithm finetunes a GPT-3 model to roughly human-level summarization ability.","""Learning from natural language feedback allows language models to generate higher-quality outputs and refinements, making them more capable of aligning with human preferences and improving task performance with minimal training data.""","""Learning from natural language feedback on model outputs can improve the quality of task execution, allowing models to refine their outputs to better align with human preferences, especially in summarization tasks."""
"We present Habitat, a platform for research in embodied artificial intelligence (AI). Habitat enables training embodied agents (virtual robots) in highly efficient photorealistic 3D simulation. Specifically, Habitat consists of: (i) Habitat-Sim: a flexible, high-performance 3D simulator with configurable agents, sensors, and generic 3D dataset handling. Habitat-Sim is fast -- when rendering a scene from Matterport3D, it achieves several thousand frames per second (fps) running single-threaded, and can reach over 10,000 fps multi-process on a single GPU. (ii) Habitat-API: a modular high-level library for end-to-end development of embodied AI algorithms -- defining tasks (e.g., navigation, instruction following, question answering), configuring, training, and benchmarking embodied agents.   These large-scale engineering contributions enable us to answer scientific questions requiring experiments that were till now impracticable or 'merely' impractical. Specifically, in the context of point-goal navigation: (1) we revisit the comparison between learning and SLAM approaches from two recent works and find evidence for the opposite conclusion -- that learning outperforms SLAM if scaled to an order of magnitude more experience than previous investigations, and (2) we conduct the first cross-dataset generalization experiments {train, test} x {Matterport3D, Gibson} for multiple sensors {blind, RGB, RGBD, D} and find that only agents with depth (D) sensors generalize across datasets. We hope that our open-source platform and these findings will advance research in embodied AI.","""Embodied AI platforms like Habitat, which support scalable, photorealistic 3D simulations, could lead to advancements in robot navigation and generalization across different sensor configurations and environments.""","""Simulated environments like Habitat can significantly advance embodied AI research by providing a platform for testing and developing intelligent agents capable of performing complex tasks in dynamic 3D environments."""
"In this paper, we study the problem of learning vision-based dynamic manipulation skills using a scalable reinforcement learning approach. We study this problem in the context of grasping, a longstanding challenge in robotic manipulation. In contrast to static learning behaviors that choose a grasp point and then execute the desired grasp, our method enables closed-loop vision-based control, whereby the robot continuously updates its grasp strategy based on the most recent observations to optimize long-horizon grasp success. To that end, we introduce QT-Opt, a scalable self-supervised vision-based reinforcement learning framework that can leverage over 580k real-world grasp attempts to train a deep neural network Q-function with over 1.2M parameters to perform closed-loop, real-world grasping that generalizes to 96% grasp success on unseen objects. Aside from attaining a very high success rate, our method exhibits behaviors that are quite distinct from more standard grasping systems: using only RGB vision-based perception from an over-the-shoulder camera, our method automatically learns regrasping strategies, probes objects to find the most effective grasps, learns to reposition objects and perform other non-prehensile pre-grasp manipulations, and responds dynamically to disturbances and perturbations.","""Scalable reinforcement learning methods, like QT-Opt, which utilize dynamic vision-based control and learning strategies, can greatly enhance robotic manipulation tasks, including grasping, regrasping, and responding to disturbances, achieving high success rates in real-world environments.""","""Closed-loop vision-based reinforcement learning frameworks, such as QT-Opt, can dramatically enhance robotic grasping and manipulation skills by enabling real-time adjustments and dynamic regrasping strategies."""
"With the recent wave of progress in artificial intelligence (AI) has come a growing awareness of the large-scale impacts of AI systems, and recognition that existing regulations and norms in industry and academia are insufficient to ensure responsible AI development. In order for AI developers to earn trust from system users, customers, civil society, governments, and other stakeholders that they are building AI responsibly, they will need to make verifiable claims to which they can be held accountable. Those outside of a given organization also need effective means of scrutinizing such claims. This report suggests various steps that different stakeholders can take to improve the verifiability of claims made about AI systems and their associated development processes, with a focus on providing evidence about the safety, security, fairness, and privacy protection of AI systems. We analyze ten mechanisms for this purpose--spanning institutions, software, and hardware--and make recommendations aimed at implementing, exploring, or improving those mechanisms.","""To ensure responsible AI development, stakeholders need mechanisms for verifying AI system claims, with a focus on safety, fairness, and privacy, to build trust among users and external parties, fostering accountability in AI research and deployment.""","""Responsible AI development requires verifiable mechanisms for accountability in safety, fairness, and privacy, which can be achieved through better transparency and scrutiny from external stakeholders."""
"Accurate reporting of energy and carbon usage is essential for understanding the potential climate impacts of machine learning research. We introduce a framework that makes this easier by providing a simple interface for tracking realtime energy consumption and carbon emissions, as well as generating standardized online appendices. Utilizing this framework, we create a leaderboard for energy efficient reinforcement learning algorithms to incentivize responsible research in this area as an example for other areas of machine learning. Finally, based on case studies using our framework, we propose strategies for mitigation of carbon emissions and reduction of energy consumption. By making accounting easier, we hope to further the sustainable development of machine learning experiments and spur more research into energy efficient algorithms.","""Tracking energy consumption and carbon emissions in machine learning experiments can drive more sustainable research practices, incentivize the development of energy-efficient algorithms, and mitigate the environmental impact of AI development.""","""Efficient tracking of energy consumption and carbon emissions in machine learning research, along with strategies to reduce energy usage, can make the AI field more sustainable and encourage responsible experimentation practices."""
"Large language models are trained in two stages: (1) unsupervised pretraining from raw text, to learn general-purpose representations, and (2) large scale instruction tuning and reinforcement learning, to better align to end tasks and user preferences. We measure the relative importance of these two stages by training LIMA, a 65B parameter LLaMa language model fine-tuned with the standard supervised loss on only 1,000 carefully curated prompts and responses, without any reinforcement learning or human preference modeling. LIMA demonstrates remarkably strong performance, learning to follow specific response formats from only a handful of examples in the training data, including complex queries that range from planning trip itineraries to speculating about alternate history. Moreover, the model tends to generalize well to unseen tasks that did not appear in the training data. In a controlled human study, responses from LIMA are either equivalent or strictly preferred to GPT-4 in 43% of cases; this statistic is as high as 58% when compared to Bard and 65% versus DaVinci003, which was trained with human feedback. Taken together, these results strongly suggest that almost all knowledge in large language models is learned during pretraining, and only limited instruction tuning data is necessary to teach models to produce high quality output.","""Pretraining a large language model (LIMA) on a small, curated dataset with minimal instruction tuning can enable it to perform highly complex tasks and generalize well to unseen tasks, indicating that most model knowledge is learned during pretraining.""","""The performance of large language models is primarily determined by the unsupervised pretraining phase, and only limited instruction tuning is necessary to produce high-quality outputs, as demonstrated by models like LIMA."""
"How well can NLP models generalize to a variety of unseen tasks when provided with task instructions? To address this question, we first introduce Super-NaturalInstructions, a benchmark of 1,616 diverse NLP tasks and their expert-written instructions. Our collection covers 76 distinct task types, including but not limited to classification, extraction, infilling, sequence tagging, text rewriting, and text composition. This large and diverse collection of tasks enables rigorous benchmarking of cross-task generalization under instructions -- training models to follow instructions on a subset of tasks and evaluating them on the remaining unseen ones. Furthermore, we build Tk-Instruct, a transformer model trained to follow a variety of in-context instructions (plain language task definitions or k-shot examples). Our experiments show that Tk-Instruct outperforms existing instruction-following models such as InstructGPT by over 9% on our benchmark despite being an order of magnitude smaller. We further analyze generalization as a function of various scaling parameters, such as the number of observed tasks, the number of instances per task, and model sizes. We hope our dataset and model facilitate future progress towards more general-purpose NLP models.","""A benchmark like Super-NaturalInstructions, combined with a model like Tk-Instruct, can significantly improve cross-task generalization in NLP models, showing that instruction-following models can perform well across a diverse range of tasks, even with limited training examples.""","""NLP models trained on diverse task instructions can generalize well to unseen tasks, especially when they are provided with a large, varied benchmark and fine-tuned to follow instructions. Future research could explore the potential of scaling these models to further enhance cross-task generalization."""
"We apply preference modeling and reinforcement learning from human feedback (RLHF) to finetune language models to act as helpful and harmless assistants. We find this alignment training improves performance on almost all NLP evaluations, and is fully compatible with training for specialized skills such as python coding and summarization. We explore an iterated online mode of training, where preference models and RL policies are updated on a weekly cadence with fresh human feedback data, efficiently improving our datasets and models. Finally, we investigate the robustness of RLHF training, and identify a roughly linear relation between the RL reward and the square root of the KL divergence between the policy and its initialization. Alongside our main results, we perform peripheral analyses on calibration, competing objectives, and the use of OOD detection, compare our models with human writers, and provide samples from our models using prompts appearing in recent related work.","""The application of preference modeling and reinforcement learning from human feedback (RLHF) could significantly enhance language models' adaptability and user-centered performance, making them more effective in real-world applications while ensuring robust and continuous improvement through iterated updates.""","""Preference modeling and reinforcement learning from human feedback improve the performance of language models, making them more aligned with user preferences while maintaining compatibility with specialized tasks. This could be further refined by continuously updating models with new feedback to enhance their adaptability."""
"This paper presents a new Unified pre-trained Language Model (UniLM) that can be fine-tuned for both natural language understanding and generation tasks. The model is pre-trained using three types of language modeling tasks: unidirectional, bidirectional, and sequence-to-sequence prediction. The unified modeling is achieved by employing a shared Transformer network and utilizing specific self-attention masks to control what context the prediction conditions on. UniLM compares favorably with BERT on the GLUE benchmark, and the SQuAD 2.0 and CoQA question answering tasks. Moreover, UniLM achieves new state-of-the-art results on five natural language generation datasets, including improving the CNN/DailyMail abstractive summarization ROUGE-L to 40.51 (2.04 absolute improvement), the Gigaword abstractive summarization ROUGE-L to 35.75 (0.86 absolute improvement), the CoQA generative question answering F1 score to 82.5 (37.1 absolute improvement), the SQuAD question generation BLEU-4 to 22.12 (3.75 absolute improvement), and the DSTC7 document-grounded dialog response generation NIST-4 to 2.67 (human performance is 2.65). The code and pre-trained models are available at https://github.com/microsoft/unilm.","""UniLM's unified pre-training approach can lead to better cross-task performance, suggesting that combining unidirectional, bidirectional, and sequence-to-sequence tasks in a single model might offer a more flexible and scalable solution for future language understanding and generation tasks.""","""UniLM's pre-training strategy combining unidirectional, bidirectional, and sequence-to-sequence tasks enhances its ability to handle both understanding and generation tasks effectively. Future work could explore how this unified approach can be extended to other domains requiring complex task-specific fine-tuning."""
"As single-task accuracy on individual language and image tasks has improved substantially in the last few years, the long-term goal of a generally skilled agent that can both see and talk becomes more feasible to explore. In this work, we focus on leveraging individual language and image tasks, along with resources that incorporate both vision and language towards that objective. We design an architecture that combines state-of-the-art Transformer and ResNeXt modules fed into a novel attentive multimodal module to produce a combined model trained on many tasks. We provide a thorough analysis of the components of the model, and transfer performance when training on one, some, or all of the tasks. Our final models provide a single system that obtains good results on all vision and language tasks considered, and improves the state-of-the-art in image-grounded conversational applications.","""By integrating vision and language tasks, the architecture presented might foster a more comprehensive and generalized agent that performs well across multiple domains, potentially reducing the gap between specialized models and leading to advances in multimodal conversational AI systems.""","""Combining vision and language tasks into a unified multimodal model improves performance across various tasks, suggesting the feasibility of a general-purpose agent that can both see and talk. Further exploration into the integration of more complex multimodal tasks could further elevate the system's capabilities."""
"Large language models have shown impressive few-shot results on a wide range of tasks. However, when knowledge is key for such results, as is the case for tasks such as question answering and fact checking, massive parameter counts to store knowledge seem to be needed. Retrieval augmented models are known to excel at knowledge intensive tasks without the need for as many parameters, but it is unclear whether they work in few-shot settings. In this work we present Atlas, a carefully designed and pre-trained retrieval augmented language model able to learn knowledge intensive tasks with very few training examples. We perform evaluations on a wide range of tasks, including MMLU, KILT and NaturalQuestions, and study the impact of the content of the document index, showing that it can easily be updated. Notably, Atlas reaches over 42% accuracy on Natural Questions using only 64 examples, outperforming a 540B parameters model by 3% despite having 50x fewer parameters.","""Atlas's ability to perform few-shot learning on knowledge-intensive tasks could pave the way for more parameter-efficient models, showing that retrieval-augmented techniques can effectively handle real-world tasks with minimal training data and fewer parameters, making them viable for various applications.""","""Retrieval-augmented models can achieve strong performance in knowledge-intensive tasks with fewer parameters, demonstrating that efficient retrieval mechanisms are a promising alternative to massive parameter-based models. Further optimization of the document indexing and retrieval process could improve their scalability in few-shot settings."""
"Recently there are increasing concerns about the fairness of Artificial Intelligence (AI) in real-world applications such as computer vision and recommendations. For example, recognition algorithms in computer vision are unfair to black people such as poorly detecting their faces and inappropriately identifying them as gorillas. As one crucial application of AI, dialogue systems have been extensively applied in our society. They are usually built with real human conversational data; thus they could inherit some fairness issues which are held in the real world. However, the fairness of dialogue systems has not been well investigated. In this paper, we perform a pioneering study about the fairness issues in dialogue systems. In particular, we construct a benchmark dataset and propose quantitative measures to understand fairness in dialogue models. Our studies demonstrate that popular dialogue models show significant prejudice towards different genders and races. Besides, to mitigate the bias in dialogue systems, we propose two simple but effective debiasing methods. Experiments show that our methods can reduce the bias in dialogue systems significantly. The dataset and the implementation are released to foster fairness research in dialogue systems.","""Addressing bias in dialogue systems is crucial, and implementing simple debiasing methods could significantly improve the fairness and equity of AI-driven conversations, ensuring a more balanced representation of gender and race while enhancing the system's societal integration.""","""Dialogue systems can inherit biases from real-world data, but careful bias mitigation techniques can significantly reduce these issues. Future research could focus on expanding fairness investigations to a broader range of dialogue systems, especially in real-world applications."""
"We fine-tune large language models to write natural language critiques (natural language critical comments) using behavioral cloning. On a topic-based summarization task, critiques written by our models help humans find flaws in summaries that they would have otherwise missed. Our models help find naturally occurring flaws in both model and human written summaries, and intentional flaws in summaries written by humans to be deliberately misleading. We study scaling properties of critiquing with both topic-based summarization and synthetic tasks. Larger models write more helpful critiques, and on most tasks, are better at self-critiquing, despite having harder-to-critique outputs. Larger models can also integrate their own self-critiques as feedback, refining their own summaries into better ones. Finally, we motivate and introduce a framework for comparing critiquing ability to generation and discrimination ability. Our measurements suggest that even large models may still have relevant knowledge they cannot or do not articulate as critiques. These results are a proof of concept for using AI-assisted human feedback to scale the supervision of machine learning systems to tasks that are difficult for humans to evaluate directly. We release our training datasets, as well as samples from our critique assistance experiments.","""Training language models to critique their own and human-generated summaries could lead to a self-improving feedback loop, allowing AI systems to refine their outputs more effectively, thus reducing human effort in content evaluation and fostering trust in AI-generated critiques.""","""Large language models fine-tuned for critique generation can improve the quality of summaries by identifying flaws that are often overlooked by humans. This approach could be further explored in other areas where AI feedback is crucial for quality control and refinement."""
"We present a large-scale dataset, ReCoRD, for machine reading comprehension requiring commonsense reasoning. Experiments on this dataset demonstrate that the performance of state-of-the-art MRC systems fall far behind human performance. ReCoRD represents a challenge for future research to bridge the gap between human and machine commonsense reading comprehension. ReCoRD is available at http://nlp.jhu.edu/record.","""The ReCoRD dataset and the associated challenge of commonsense reasoning in machine reading comprehension could drive future advancements in natural language understanding and improve machine performance to approach human-level comprehension in complex tasks.""","""Commonsense reasoning remains a significant challenge in machine reading comprehension, as current systems still fall behind human-level performance. Future models could integrate external knowledge sources and more sophisticated reasoning mechanisms to bridge this gap."""
"Textual content is often the output of a collaborative writing process: We start with an initial draft, ask for suggestions, and repeatedly make changes. Agnostic of this process, today's language models are trained to generate only the final result. As a consequence, they lack several abilities crucial for collaborative writing: They are unable to update existing texts, difficult to control and incapable of verbally planning or explaining their actions. To address these shortcomings, we introduce PEER, a collaborative language model that is trained to imitate the entire writing process itself: PEER can write drafts, add suggestions, propose edits and provide explanations for its actions. Crucially, we train multiple instances of PEER able to infill various parts of the writing process, enabling the use of self-training techniques for increasing the quality, amount and diversity of training data. This unlocks PEER's full potential by making it applicable in domains for which no edit histories are available and improving its ability to follow instructions, to write useful comments, and to explain its actions. We show that PEER achieves strong performance across various domains and editing tasks.","""The development of PEER, which models the entire collaborative writing process, may enhance language model capabilities in iterative writing tasks, promoting more dynamic and interactive human-AI collaborations while improving performance in various editing and writing domains.""","""Collaborative language models like PEER, which can adapt to various stages of the writing process, open up new possibilities for AI-assisted content creation. Future work could investigate how such models can integrate feedback from multiple users to further improve the collaborative writing experience."""
"Recent work in language modeling demonstrates that training large transformer models advances the state of the art in Natural Language Processing applications. However, very large models can be quite difficult to train due to memory constraints. In this work, we present our techniques for training very large transformer models and implement a simple, efficient intra-layer model parallel approach that enables training transformer models with billions of parameters. Our approach does not require a new compiler or library changes, is orthogonal and complimentary to pipeline model parallelism, and can be fully implemented with the insertion of a few communication operations in native PyTorch. We illustrate this approach by converging transformer based models up to 8.3 billion parameters using 512 GPUs. We sustain 15.1 PetaFLOPs across the entire application with 76% scaling efficiency when compared to a strong single GPU baseline that sustains 39 TeraFLOPs, which is 30% of peak FLOPs. To demonstrate that large language models can further advance the state of the art (SOTA), we train an 8.3 billion parameter transformer language model similar to GPT-2 and a 3.9 billion parameter model similar to BERT. We show that careful attention to the placement of layer normalization in BERT-like models is critical to achieving increased performance as the model size grows. Using the GPT-2 model we achieve SOTA results on the WikiText103 (10.8 compared to SOTA perplexity of 15.8) and LAMBADA (66.5% compared to SOTA accuracy of 63.2%) datasets. Our BERT model achieves SOTA results on the RACE dataset (90.9% compared to SOTA accuracy of 89.4%).","""Efficient methods for training large transformer models, as demonstrated by the intra-layer model parallel approach, could enable the scaling of AI systems to unprecedented model sizes while maintaining performance efficiency, offering a more accessible path to large-scale natural language applications.""","""Training large transformer models on distributed architectures with efficient parallelism can push the boundaries of language modeling. Further advancements in hardware optimization and model parallelism could reduce the training time and resources required to achieve state-of-the-art performance."""
"Policy gradient methods are an appealing approach in reinforcement learning because they directly optimize the cumulative reward and can straightforwardly be used with nonlinear function approximators such as neural networks. The two main challenges are the large number of samples typically required, and the difficulty of obtaining stable and steady improvement despite the nonstationarity of the incoming data. We address the first challenge by using value functions to substantially reduce the variance of policy gradient estimates at the cost of some bias, with an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda). We address the second challenge by using trust region optimization procedure for both the policy and the value function, which are represented by neural networks.   Our approach yields strong empirical results on highly challenging 3D locomotion tasks, learning running gaits for bipedal and quadrupedal simulated robots, and learning a policy for getting the biped to stand up from starting out lying on the ground. In contrast to a body of prior work that uses hand-crafted policy representations, our neural network policies map directly from raw kinematics to joint torques. Our algorithm is fully model-free, and the amount of simulated experience required for the learning tasks on 3D bipeds corresponds to 1-2 weeks of real time.","""Using policy gradient methods combined with trust region optimization could significantly improve the stability and efficiency of reinforcement learning models in real-world robotics, allowing for more reliable and rapid training of complex behaviors in simulated environments.""","""Policy gradient methods in reinforcement learning can effectively address challenges in continuous control tasks by improving stability and reducing sample inefficiency. This approach may be extended to other complex decision-making scenarios, such as robotic manipulation and autonomous navigation."""
"In this paper, we quantify, analyze and mitigate gender bias exhibited in ELMo's contextualized word vectors. First, we conduct several intrinsic analyses and find that (1) training data for ELMo contains significantly more male than female entities, (2) the trained ELMo embeddings systematically encode gender information and (3) ELMo unequally encodes gender information about male and female entities. Then, we show that a state-of-the-art coreference system that depends on ELMo inherits its bias and demonstrates significant bias on the WinoBias probing corpus. Finally, we explore two methods to mitigate such gender bias and show that the bias demonstrated on WinoBias can be eliminated.","""Mitigating gender bias in language models such as ELMo could lead to fairer AI applications across a range of tasks, making AI systems more representative and less likely to perpetuate harmful stereotypes, enhancing their ethical use in real-world scenarios.""","""Mitigating gender bias in language models like ELMo can lead to more fair and accurate NLP systems. Future research could explore more robust debiasing techniques that are applicable to a wider range of pre-trained models and tasks."""
"Automated dialogue or conversational systems are anthropomorphised by developers and personified by users. While a degree of anthropomorphism is inevitable, conscious and unconscious design choices can guide users to personify them to varying degrees. Encouraging users to relate to automated systems as if they were human can lead to transparency and trust issues, and high risk scenarios caused by over-reliance on their outputs. As a result, natural language processing researchers have begun to investigate factors that induce personification and develop resources to mitigate such effects. However, these efforts are fragmented, and many aspects of anthropomorphism have yet to be considered. In this paper, we discuss the linguistic factors that contribute to the anthropomorphism of dialogue systems and the harms that can arise, arguing that it can reinforce stereotypes of gender roles and notions of acceptable language. We recommend that future efforts towards developing dialogue systems take particular care in their design, development, release, and description; and attend to the many linguistic cues that can elicit personification by users.","""Addressing the linguistic factors that contribute to the anthropomorphism of dialogue systems could help in designing more transparent, accountable, and ethical AI systems, reducing the risks of over-reliance on AI and ensuring better human-computer interaction experiences.""","""Anthropomorphism in dialogue systems can exacerbate trust and transparency issues, especially when linguistic cues encourage users to personify automated systems, reinforcing harmful stereotypes and biases."""
"This paper presents a simple end-to-end model for speech recognition, combining a convolutional network based acoustic model and a graph decoding. It is trained to output letters, with transcribed speech, without the need for force alignment of phonemes. We introduce an automatic segmentation criterion for training from sequence annotation without alignment that is on par with CTC while being simpler. We show competitive results in word error rate on the Librispeech corpus with MFCC features, and promising results from raw waveform.","""End-to-end speech recognition models using convolutional networks for acoustic modeling and graph decoding may offer more efficient solutions by reducing the need for complex phoneme alignments, especially when trained on raw waveforms. The simplicity of the proposed segmentation criterion might improve performance across a wide range of speech recognition tasks.""","""End-to-end models for speech recognition that avoid force alignment of phonemes and utilize automatic segmentation criteria can significantly improve performance and simplify the training process, especially on diverse datasets."""
"Models often easily learn biases present in the training data, and their predictions directly reflect this bias. We analyze gender bias in dialogue data, and examine how this bias is actually amplified in subsequent generative chit-chat dialogue models. We measure gender bias in six existing dialogue datasets, and focus on the most biased one, the multi-player text-based fantasy adventure dataset LIGHT, as a testbed for our bias mitigation techniques. The LIGHT dataset is highly imbalanced with respect to gender, containing predominantly male characters, likely because it is entirely collected by crowdworkers and reflects common biases that exist in fantasy or medieval settings. We consider three techniques to mitigate gender bias: counterfactual data augmentation, targeted data collection, and bias controlled training. We show that our proposed techniques mitigate gender bias in LIGHT by balancing the genderedness of generated dialogue utterances and are particularly effective in combination. We quantify performance using various evaluation methods---such as quantity of gendered words, a dialogue safety classifier, and human studies---all of which show that our models generate less gendered, but equally engaging chit-chat responses.","""Bias amplification in dialogue models, particularly gender bias, can be mitigated using targeted data collection and bias-controlled training techniques. Implementing counterfactual data augmentation combined with these methods will likely improve the fairness and inclusivity of generative dialogue systems.""","""Mitigating gender bias in dialogue models through targeted data collection, counterfactual data augmentation, and bias-controlled training techniques leads to less gendered, yet equally engaging, outputs, particularly in highly imbalanced datasets."""
"While dialogue remains an important end-goal of natural language research, the difficulty of evaluation is an oft-quoted reason why it remains troublesome to make real progress towards its solution. Evaluation difficulties are actually two-fold: not only do automatic metrics not correlate well with human judgments, but also human judgments themselves are in fact difficult to measure. The two most used human judgment tests, single-turn pairwise evaluation and multi-turn Likert scores, both have serious flaws as we discuss in this work.   We instead provide a novel procedure involving comparing two full dialogues, where a human judge is asked to pay attention to only one speaker within each, and make a pairwise judgment. The questions themselves are optimized to maximize the robustness of judgments across different annotators, resulting in better tests. We also show how these tests work in self-play model chat setups, resulting in faster, cheaper tests. We hope these tests become the de facto standard, and will release open-source code to that end.","""A novel evaluation method for dialogue systems that compares entire conversations instead of individual utterances may provide more reliable and robust human judgment. Optimizing test questions to account for variability in human annotators will yield a more consistent measure of dialogue system performance.""","""A novel evaluation procedure based on pairwise comparisons of entire dialogues rather than isolated turn-based assessments can improve the reliability and speed of human judgment in dialogue systems, addressing flaws in existing metrics."""
"Deep learning based models have surpassed classical machine learning based approaches in various text classification tasks, including sentiment analysis, news categorization, question answering, and natural language inference. In this paper, we provide a comprehensive review of more than 150 deep learning based models for text classification developed in recent years, and discuss their technical contributions, similarities, and strengths. We also provide a summary of more than 40 popular datasets widely used for text classification. Finally, we provide a quantitative analysis of the performance of different deep learning models on popular benchmarks, and discuss future research directions.","""The ongoing dominance of deep learning models in text classification tasks is driven by their ability to capture complex patterns in data. Future research could focus on enhancing model performance through innovative architectures or incorporating external knowledge sources.""","""Deep learning-based models are outpacing traditional machine learning approaches in text classification, and further research into their technical contributions, strengths, and limitations is necessary to refine their use across diverse datasets."""
"Aligning large language models (LLMs) with human preferences has proven to drastically improve usability and has driven rapid adoption as demonstrated by ChatGPT. Alignment techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF) greatly reduce the required skill and domain knowledge to effectively harness the capabilities of LLMs, increasing their accessibility and utility across various domains. However, state-of-the-art alignment techniques like RLHF rely on high-quality human feedback data, which is expensive to create and often remains proprietary. In an effort to democratize research on large-scale alignment, we release OpenAssistant Conversations, a human-generated, human-annotated assistant-style conversation corpus consisting of 161,443 messages distributed across 66,497 conversation trees, in 35 different languages, annotated with 461,292 quality ratings. The corpus is a product of a worldwide crowd-sourcing effort involving over 13,500 volunteers. To demonstrate the OpenAssistant Conversations dataset's effectiveness, we present OpenAssistant, the first fully open-source large-scale instruction-tuned model to be trained on human data. A preference study revealed that OpenAssistant replies are comparably preferred to GPT-3.5-turbo (ChatGPT) with a relative winrate of 48.3% vs. 51.7% respectively. We release our code and data under fully permissive licenses.","""The OpenAssistant Conversations corpus and instruction-tuning of large language models (LLMs) can significantly improve the quality and accessibility of AI-powered assistants. By democratizing alignment techniques, this approach may enhance the usability of LLMs across diverse domains.""","""The effectiveness of large language models (LLMs) can be enhanced through human-generated alignment techniques like RLHF, but creating and maintaining high-quality feedback data remains a costly challenge, which can be alleviated with open-source datasets like OpenAssistant Conversations."""
"Generative AI systems across modalities, ranging from text, image, audio, and video, have broad social impacts, but there exists no official standard for means of evaluating those impacts and which impacts should be evaluated. We move toward a standard approach in evaluating a generative AI system for any modality, in two overarching categories: what is able to be evaluated in a base system that has no predetermined application and what is able to be evaluated in society. We describe specific social impact categories and how to approach and conduct evaluations in the base technical system, then in people and society. Our framework for a base system defines seven categories of social impact: bias, stereotypes, and representational harms; cultural values and sensitive content; disparate performance; privacy and data protection; financial costs; environmental costs; and data and content moderation labor costs. Suggested methods for evaluation apply to all modalities and analyses of the limitations of existing evaluations serve as a starting point for necessary investment in future evaluations. We offer five overarching categories for what is able to be evaluated in society, each with their own subcategories: trustworthiness and autonomy; inequality, marginalization, and violence; concentration of authority; labor and creativity; and ecosystem and environment. Each subcategory includes recommendations for mitigating harm. We are concurrently crafting an evaluation repository for the AI research community to contribute existing evaluations along the given categories. This version will be updated following a CRAFT session at ACM FAccT 2023.","""Evaluating the social impact of generative AI across different modalities requires standardized evaluation categories and methods. The framework proposed for evaluating both technical systems and societal impacts could promote more responsible development and deployment of AI technologies.""","""A standardized framework for evaluating the social impacts of generative AI systems is essential, addressing issues such as bias, privacy, and labor costs, while providing methods to mitigate harms across various modalities and applications."""
"This paper presents the first consumer-scale next-word prediction (NWP) model trained with Federated Learning (FL) while leveraging the Differentially Private Federated Averaging (DP-FedAvg) technique. There has been prior work on building practical FL infrastructure, including work demonstrating the feasibility of training language models on mobile devices using such infrastructure. It has also been shown (in simulations on a public corpus) that it is possible to train NWP models with user-level differential privacy using the DP-FedAvg algorithm. Nevertheless, training production-quality NWP models with DP-FedAvg in a real-world production environment on a heterogeneous fleet of mobile phones requires addressing numerous challenges. For instance, the coordinating central server has to keep track of the devices available at the start of each round and sample devices uniformly at random from them, while ensuring \emph{secrecy of the sample}, etc. Unlike all prior privacy-focused FL work of which we are aware, for the first time we demonstrate the deployment of a differentially private mechanism for the training of a production neural network in FL, as well as the instrumentation of the production training infrastructure to perform an end-to-end empirical measurement of unintended memorization.","""Differentially Private Federated Learning (DP-FedAvg) could provide an effective solution for training privacy-preserving next-word prediction models in production environments. Real-world challenges such as device sampling and privacy-preserving mechanisms can be overcome with careful deployment strategies.""","""Federated learning and differential privacy techniques, while complex, are crucial for the privacy-conscious deployment of next-word prediction models, offering new ways to balance privacy and model performance in real-world scenarios."""
"By conditioning on natural language instructions, large language models (LLMs) have displayed impressive capabilities as general-purpose computers. However, task performance depends significantly on the quality of the prompt used to steer the model, and most effective prompts have been handcrafted by humans. Inspired by classical program synthesis and the human approach to prompt engineering, we propose Automatic Prompt Engineer (APE) for automatic instruction generation and selection. In our method, we treat the instruction as the program, optimized by searching over a pool of instruction candidates proposed by an LLM in order to maximize a chosen score function. To evaluate the quality of the selected instruction, we evaluate the zero-shot performance of another LLM following the selected instruction. Experiments on 24 NLP tasks show that our automatically generated instructions outperform the prior LLM baseline by a large margin and achieve better or comparable performance to the instructions generated by human annotators on 19/24 tasks. We conduct extensive qualitative and quantitative analyses to explore the performance of APE. We show that APE-engineered prompts can be applied to steer models toward truthfulness and/or informativeness, as well as to improve few-shot learning performance by simply prepending them to standard in-context learning prompts. Please check out our webpage at https://sites.google.com/view/automatic-prompt-engineer.","""Automatic Prompt Engineer (APE) for generating optimal instructions can significantly improve task performance across a range of NLP tasks, potentially outperforming human-engineered prompts. This method may offer advantages in improving model performance without the need for manual prompt creation.""","""Automatically generated prompts using methods like APE can significantly improve the performance of large language models on various NLP tasks, demonstrating the potential for reduced human involvement in prompt engineering."""
"The Internet contains a wealth of knowledge -- from the birthdays of historical figures to tutorials on how to code -- all of which may be learned by language models. However, while certain pieces of information are ubiquitous on the web, others appear extremely rarely. In this paper, we study the relationship between the knowledge memorized by large language models and the information in pre-training datasets scraped from the web. In particular, we show that a language model's ability to answer a fact-based question relates to how many documents associated with that question were seen during pre-training. We identify these relevant documents by entity linking pre-training datasets and counting documents that contain the same entities as a given question-answer pair. Our results demonstrate strong correlational and causal relationships between accuracy and relevant document count for numerous question answering datasets (e.g., TriviaQA), pre-training corpora (e.g., ROOTS), and model sizes (e.g., 176B parameters). Moreover, while larger models are better at learning long-tail knowledge, we estimate that today's models must be scaled by many orders of magnitude to reach competitive QA performance on questions with little support in the pre-training data. Finally, we show that retrieval-augmentation can reduce the dependence on relevant pre-training information, presenting a promising approach for capturing the long-tail.","""The ability of language models to answer fact-based questions is closely related to the presence of relevant documents in pre-training datasets. Retrieval-augmentation techniques could help reduce the dependency on pre-training knowledge and improve QA performance, particularly for long-tail knowledge.""","""The quality of long-tail knowledge in large language models can be directly tied to the quantity of relevant documents in their training data, highlighting the limitations of current models in capturing rare information and the promise of retrieval-augmentation techniques."""
"State-of-the-art language models can match human performance on many tasks, but they still struggle to robustly perform multi-step mathematical reasoning. To diagnose the failures of current models and support research, we introduce GSM8K, a dataset of 8.5K high quality linguistically diverse grade school math word problems. We find that even the largest transformer models fail to achieve high test performance, despite the conceptual simplicity of this problem distribution. To increase performance, we propose training verifiers to judge the correctness of model completions. At test time, we generate many candidate solutions and select the one ranked highest by the verifier. We demonstrate that verification significantly improves performance on GSM8K, and we provide strong empirical evidence that verification scales more effectively with increased data than a finetuning baseline.","""Training models with a verification mechanism for multi-step mathematical reasoning, such as in the GSM8K dataset, can help enhance the reliability of generated solutions. Verification, particularly when scaled with increased data, may improve performance over traditional fine-tuning approaches.""","""Training verifiers to evaluate multi-step reasoning tasks can improve the performance of language models on complex mathematical problems, as evidenced by the GSM8K dataset, indicating a promising avenue for improving model reliability in reasoning."""
"Restart techniques are common in gradient-free optimization to deal with multimodal functions. Partial warm restarts are also gaining popularity in gradient-based optimization to improve the rate of convergence in accelerated gradient schemes to deal with ill-conditioned functions. In this paper, we propose a simple warm restart technique for stochastic gradient descent to improve its anytime performance when training deep neural networks. We empirically study its performance on the CIFAR-10 and CIFAR-100 datasets, where we demonstrate new state-of-the-art results at 3.14% and 16.21%, respectively. We also demonstrate its advantages on a dataset of EEG recordings and on a downsampled version of the ImageNet dataset. Our source code is available at https://github.com/loshchil/SGDR","""A simple warm restart technique for stochastic gradient descent can significantly enhance the performance of deep neural networks, particularly for datasets such as CIFAR-10 and CIFAR-100, improving convergence and achieving state-of-the-art results.""","""Warm restart techniques for stochastic gradient descent can significantly accelerate convergence and improve the training performance of deep neural networks, demonstrating state-of-the-art results on benchmark datasets like CIFAR and ImageNet."""
"An emerging method to cheaply improve a weaker language model is to finetune it on outputs from a stronger model, such as a proprietary system like ChatGPT (e.g., Alpaca, Self-Instruct, and others). This approach looks to cheaply imitate the proprietary model's capabilities using a weaker open-source model. In this work, we critically analyze this approach. We first finetune a series of LMs that imitate ChatGPT using varying base model sizes (1.5B--13B), data sources, and imitation data amounts (0.3M--150M tokens). We then evaluate the models using crowd raters and canonical NLP benchmarks. Initially, we were surprised by the output quality of our imitation models -- they appear far better at following instructions, and crowd workers rate their outputs as competitive with ChatGPT. However, when conducting more targeted automatic evaluations, we find that imitation models close little to none of the gap from the base LM to ChatGPT on tasks that are not heavily supported in the imitation data. We show that these performance discrepancies may slip past human raters because imitation models are adept at mimicking ChatGPT's style but not its factuality. Overall, we conclude that model imitation is a false promise: there exists a substantial capabilities gap between open and closed LMs that, with current methods, can only be bridged using an unwieldy amount of imitation data or by using more capable base LMs. In turn, we argue that the highest leverage action for improving open-source models is to tackle the difficult challenge of developing better base LMs, rather than taking the shortcut of imitating proprietary systems.","""Imitating stronger language models through fine-tuning on their outputs can create models that appear to mimic human-like behavior but still lack the capability to handle tasks outside the imitation data scope, thus revealing limitations in model imitation techniques.""","""The imitation method for improving weaker language models by finetuning on stronger models like ChatGPT may show promising output quality, but it does not fully bridge the capabilities gap due to limitations in factuality and task-specific performance."""
"Semi-supervised algorithms aim to learn prediction functions from a small set of labeled observations and a large set of unlabeled observations. Because this framework is relevant in many applications, they have received a lot of interest in both academia and industry. Among the existing techniques, self-training methods have undoubtedly attracted greater attention in recent years. These models are designed to find the decision boundary on low density regions without making additional assumptions about the data distribution, and use the unsigned output score of a learned classifier, or its margin, as an indicator of confidence. The working principle of self-training algorithms is to learn a classifier iteratively by assigning pseudo-labels to the set of unlabeled training samples with a margin greater than a certain threshold. The pseudo-labeled examples are then used to enrich the labeled training data and to train a new classifier in conjunction with the labeled training set. In this paper, we present self-training methods for binary and multi-class classification; as well as their variants and two related approaches, namely consistency-based approaches and transductive learning. We examine the impact of significant self-training features on various methods, using different general and image classification benchmarks, and we discuss our ideas for future research in self-training. To the best of our knowledge, this is the first thorough and complete survey on this subject.","""Self-training methods, particularly when using confidence scores and pseudo-labeling, are effective in enhancing semi-supervised learning for classification tasks, though their performance can vary across different datasets and models.""","""Self-training methods, which use pseudo-labeling for unlabeled data, offer a robust framework for both binary and multi-class classification tasks, but their effectiveness may vary depending on the task and dataset characteristics."""
"As AI systems become increasingly powerful and pervasive, there are growing concerns about machines' morality or a lack thereof. Yet, teaching morality to machines is a formidable task, as morality remains among the most intensely debated questions in humanity, let alone for AI. Existing AI systems deployed to millions of users, however, are already making decisions loaded with moral implications, which poses a seemingly impossible challenge: teaching machines moral sense, while humanity continues to grapple with it.   To explore this challenge, we introduce Delphi, an experimental framework based on deep neural networks trained directly to reason about descriptive ethical judgments, e.g., helping a friend is generally good, while helping a friend spread fake news is not. Empirical results shed novel insights on the promises and limits of machine ethics; Delphi demonstrates strong generalization capabilities in the face of novel ethical situations, while off-the-shelf neural network models exhibit markedly poor judgment including unjust biases, confirming the need for explicitly teaching machines moral sense.   Yet, Delphi is not perfect, exhibiting susceptibility to pervasive biases and inconsistencies. Despite that, we demonstrate positive use cases of imperfect Delphi, including using it as a component model within other imperfect AI systems. Importantly, we interpret the operationalization of Delphi in light of prominent ethical theories, which leads us to important future research questions.","""AI systems can exhibit strong generalization capabilities in moral reasoning tasks but still suffer from biases and inconsistencies, suggesting the need for explicit moral training within AI frameworks to improve decision-making processes in ethically charged situations.""","""AI systems, especially those attempting to learn morality, face significant challenges in aligning machine ethics with human values, but experimental frameworks like Delphi could be a step toward improving machine moral reasoning, despite current biases and inconsistencies."""
"We propose a criterion for discrimination against a specified sensitive attribute in supervised learning, where the goal is to predict some target based on available features. Assuming data about the predictor, target, and membership in the protected group are available, we show how to optimally adjust any learned predictor so as to remove discrimination according to our definition. Our framework also improves incentives by shifting the cost of poor classification from disadvantaged groups to the decision maker, who can respond by improving the classification accuracy.   In line with other studies, our notion is oblivious: it depends only on the joint statistics of the predictor, the target and the protected attribute, but not on interpretation of individualfeatures. We study the inherent limits of defining and identifying biases based on such oblivious measures, outlining what can and cannot be inferred from different oblivious tests.   We illustrate our notion using a case study of FICO credit scores.","""Discrimination in machine learning models can be mitigated by adjusting the classifier to remove bias based on sensitive attributes, but limitations exist in identifying and addressing biases when using only oblivious measures, requiring further investigation into effective fairness frameworks.""","""Discrimination in machine learning models can be reduced through optimal adjustments to predictors based on sensitive attributes, but the effectiveness of such approaches is limited by the availability and interpretation of group membership data."""
"Training large deep neural networks on massive datasets is computationally very challenging. There has been recent surge in interest in using large batch stochastic optimization methods to tackle this issue. The most prominent algorithm in this line of research is LARS, which by employing layerwise adaptive learning rates trains ResNet on ImageNet in a few minutes. However, LARS performs poorly for attention models like BERT, indicating that its performance gains are not consistent across tasks. In this paper, we first study a principled layerwise adaptation strategy to accelerate training of deep neural networks using large mini-batches. Using this strategy, we develop a new layerwise adaptive large batch optimization technique called LAMB; we then provide convergence analysis of LAMB as well as LARS, showing convergence to a stationary point in general nonconvex settings. Our empirical results demonstrate the superior performance of LAMB across various tasks such as BERT and ResNet-50 training with very little hyperparameter tuning. In particular, for BERT training, our optimizer enables use of very large batch sizes of 32868 without any degradation of performance. By increasing the batch size to the memory limit of a TPUv3 Pod, BERT training time can be reduced from 3 days to just 76 minutes (Table 1). The LAMB implementation is available at https://github.com/tensorflow/addons/blob/master/tensorflow_addons/optimizers/lamb.py","""Large batch stochastic optimization techniques like LAMB can significantly accelerate the training of deep neural networks, particularly in large-scale models like BERT, and enable efficient training even with very large batch sizes, reducing training time drastically.""","""Using layerwise adaptive optimization methods like LAMB can significantly accelerate the training of deep neural networks on large datasets with minimal hyperparameter tuning, showing substantial improvements in efficiency over traditional methods like LARS."""
"Over the last several years, end-to-end neural conversational agents have vastly improved in their ability to carry a chit-chat conversation with humans. However, these models are often trained on large datasets from the internet, and as a result, may learn undesirable behaviors from this data, such as toxic or otherwise harmful language. Researchers must thus wrestle with the issue of how and when to release these models. In this paper, we survey the problem landscape for safety for end-to-end conversational AI and discuss recent and related work. We highlight tensions between values, potential positive impact and potential harms, and provide a framework for making decisions about whether and how to release these models, following the tenets of value-sensitive design. We additionally provide a suite of tools to enable researchers to make better-informed decisions about training and releasing end-to-end conversational AI models.","""End-to-end conversational AI models are becoming more capable, but researchers must address concerns over safety and ethical implications before deploying them, balancing the potential positive impact with the risks of harmful language learned from training data.""","""End-to-end neural conversational models, while improving in conversation fluency, still face challenges in addressing harmful behaviors learned from training data, necessitating careful decisions on when and how to release such models."""
"This paper explores a simple and efficient baseline for text classification. Our experiments show that our fast text classifier fastText is often on par with deep learning classifiers in terms of accuracy, and many orders of magnitude faster for training and evaluation. We can train fastText on more than one billion words in less than ten minutes using a standard multicore~CPU, and classify half a million sentences among~312K classes in less than a minute.","""Simple and efficient text classification methods, such as fastText, can achieve comparable accuracy to deep learning classifiers while being orders of magnitude faster for both training and evaluation, making them suitable for large-scale, real-time applications.""","""The fastText classifier offers an efficient and competitive alternative to deep learning methods for text classification tasks, providing significant speed improvements without compromising on accuracy."""
"The original ImageNet dataset is a popular large-scale benchmark for training Deep Neural Networks. Since the cost of performing experiments (e.g, algorithm design, architecture search, and hyperparameter tuning) on the original dataset might be prohibitive, we propose to consider a downsampled version of ImageNet. In contrast to the CIFAR datasets and earlier downsampled versions of ImageNet, our proposed ImageNet32$\times$32 (and its variants ImageNet64$\times$64 and ImageNet16$\times$16) contains exactly the same number of classes and images as ImageNet, with the only difference that the images are downsampled to 32$\times$32 pixels per image (64$\times$64 and 16$\times$16 pixels for the variants, respectively). Experiments on these downsampled variants are dramatically faster than on the original ImageNet and the characteristics of the downsampled datasets with respect to optimal hyperparameters appear to remain similar. The proposed datasets and scripts to reproduce our results are available at http://image-net.org/download-images and https://github.com/PatrykChrabaszcz/Imagenet32_Scripts","""Downsampling datasets like ImageNet while maintaining class and image consistency provides a cost-effective way to accelerate experiments in deep learning, yielding similar results as the original dataset while drastically reducing computation time.""","""Downsampling large-scale image datasets like ImageNet leads to faster training times without significant loss of model performance, making it a viable option for researchers with computational constraints."""
"This survey reviews works in which language models (LMs) are augmented with reasoning skills and the ability to use tools. The former is defined as decomposing a potentially complex task into simpler subtasks while the latter consists in calling external modules such as a code interpreter. LMs can leverage these augmentations separately or in combination via heuristics, or learn to do so from demonstrations. While adhering to a standard missing tokens prediction objective, such augmented LMs can use various, possibly non-parametric external modules to expand their context processing ability, thus departing from the pure language modeling paradigm. We therefore refer to them as Augmented Language Models (ALMs). The missing token objective allows ALMs to learn to reason, use tools, and even act, while still performing standard natural language tasks and even outperforming most regular LMs on several benchmarks. In this work, after reviewing current advance in ALMs, we conclude that this new research direction has the potential to address common limitations of traditional LMs such as interpretability, consistency, and scalability issues.","""Augmented Language Models (ALMs) that incorporate reasoning and external tool usage can overcome limitations of traditional language models, offering potential for improvements in interpretability, consistency, and scalability, ultimately advancing the capabilities of NLP systems.""","""Augmenting language models with reasoning skills and external tools enhances their performance on complex tasks, overcoming limitations of traditional language models and offering improved scalability, consistency, and interpretability."""
"Recent studies report that autoregressive language models can successfully solve many NLP tasks via zero- and few-shot learning paradigms, which opens up new possibilities for using the pre-trained language models. This paper introduces two autoregressive GPT-like models with 1.3 billion and 13 billion parameters trained on 60 languages from 25 language families using Wikipedia and Colossal Clean Crawled Corpus. We reproduce the GPT-3 architecture using GPT-2 sources and the sparse attention mechanism; Deepspeed and Megatron frameworks allow us to parallelize the training and inference steps effectively. The resulting models show performance on par with the recently released XGLM models by Facebook, covering more languages and enhancing NLP possibilities for low resource languages of CIS countries and Russian small nations. We detail the motivation for the choices of the architecture design, thoroughly describe the data preparation pipeline, and train five small versions of the model to choose the most optimal multilingual tokenization strategy. We measure the model perplexity in all covered languages and evaluate it on the wide spectre of multilingual tasks, including classification, generative, sequence labeling and knowledge probing. The models were evaluated with the zero-shot and few-shot methods. Furthermore, we compared the classification tasks with the state-of-the-art multilingual model XGLM. source code and the mGPT XL model are publicly released.","""Autoregressive language models trained on multilingual data from diverse sources can enhance NLP performance across multiple languages, including low-resource languages, by leveraging zero-shot and few-shot learning, providing a competitive edge over previous multilingual models.""","""Autoregressive GPT-like models trained on multilingual data can offer significant advancements in low-resource language tasks, demonstrating improved performance over existing multilingual models like XGLM."""
"Recent work has found that multi-task training with a large number of diverse tasks can uniformly improve downstream performance on unseen target tasks. In contrast, literature on task transferability has established that the choice of intermediate tasks can heavily affect downstream task performance. In this work, we aim to disentangle the effect of scale and relatedness of tasks in multi-task representation learning. We find that, on average, increasing the scale of multi-task learning, in terms of the number of tasks, indeed results in better learned representations than smaller multi-task setups. However, if the target tasks are known ahead of time, then training on a smaller set of related tasks is competitive to the large-scale multi-task training at a reduced computational cost.","""The performance of multi-task learning is significantly influenced by the scale of tasks; however, smaller, related task setups may still yield competitive results with reduced computational cost when target tasks are known ahead of time.""","""Increasing the scale of multi-task learning enhances the quality of learned representations for downstream tasks, but selecting related intermediate tasks can offer a more computationally efficient approach with competitive results."""
"Abstractive summarization systems today produce fluent and relevant output, but often hallucinate statements not supported by the source text. We analyze the connection between hallucinations and training data, and find evidence that models hallucinate because they train on target summaries that are unsupported by the source. Based on our findings, we present PINOCCHIO, a new decoding method that improves the consistency of a transformer-based abstractive summarizer by constraining beam search to avoid hallucinations. Given the model states and outputs at a given step, PINOCCHIO detects likely model hallucinations based on various measures of attribution to the source text. PINOCCHIO backtracks to find more consistent output, and can opt to produce no summary at all when no consistent generation can be found. In experiments, we find that PINOCCHIO improves the consistency of generation (in terms of F1) by an average of~67% on two abstractive summarization datasets.","""The hallucination problem in abstractive summarization models can be mitigated by constraining decoding methods like PINOCCHIO, improving consistency without compromising summary fluency.""","""Abstractive summarization models can reduce hallucinations and improve consistency in generation by using methods like PINOCCHIO, which ensures that the generated summary is consistent with the source text."""
"The perceived toxicity of language can vary based on someone's identity and beliefs, but this variation is often ignored when collecting toxic language datasets, resulting in dataset and model biases. We seek to understand the who, why, and what behind biases in toxicity annotations. In two online studies with demographically and politically diverse participants, we investigate the effect of annotator identities (who) and beliefs (why), drawing from social psychology research about hate speech, free speech, racist beliefs, political leaning, and more. We disentangle what is annotated as toxic by considering posts with three characteristics: anti-Black language, African American English (AAE) dialect, and vulgarity. Our results show strong associations between annotator identity and beliefs and their ratings of toxicity. Notably, more conservative annotators and those who scored highly on our scale for racist beliefs were less likely to rate anti-Black language as toxic, but more likely to rate AAE as toxic. We additionally present a case study illustrating how a popular toxicity detection system's ratings inherently reflect only specific beliefs and perspectives. Our findings call for contextualizing toxicity labels in social variables, which raises immense implications for toxic language annotation and detection.","""The variability in toxicity annotations arises from annotator identity and beliefs, suggesting that toxicity detection systems must consider social and contextual factors to reduce bias and improve fairness in model predictions.""","""The perceived toxicity of language is influenced by the identity and political beliefs of annotators, leading to biases in toxicity annotation datasets. This suggests that incorporating the annotators' demographic and belief-based factors could improve the fairness and accuracy of toxicity detection models."""
"We present the results and main findings of SemEval-2020 Task 12 on Multilingual Offensive Language Identification in Social Media (OffensEval 2020). The task involves three subtasks corresponding to the hierarchical taxonomy of the OLID schema (Zampieri et al., 2019a) from OffensEval 2019. The task featured five languages: English, Arabic, Danish, Greek, and Turkish for Subtask A. In addition, English also featured Subtasks B and C. OffensEval 2020 was one of the most popular tasks at SemEval-2020 attracting a large number of participants across all subtasks and also across all languages. A total of 528 teams signed up to participate in the task, 145 teams submitted systems during the evaluation period, and 70 submitted system description papers.","""Multilingual offensive language detection models can benefit from comprehensive task participation across different languages, improving generalization and model robustness in real-world applications.""","""Multilingual offensive language identification can benefit from a unified approach to taxonomy, facilitating better cross-lingual offensive language detection. However, challenges in managing diverse cultural and linguistic contexts might require fine-tuning for specific languages and regions."""
"We introduce a large-scale dataset of math word problems and an interpretable neural math problem solver that learns to map problems to operation programs. Due to annotation challenges, current datasets in this domain have been either relatively small in scale or did not offer precise operational annotations over diverse problem types. We introduce a new representation language to model precise operation programs corresponding to each math problem that aim to improve both the performance and the interpretability of the learned models. Using this representation language, our new dataset, MathQA, significantly enhances the AQuA dataset with fully-specified operational programs. We additionally introduce a neural sequence-to-program model enhanced with automatic problem categorization. Our experiments show improvements over competitive baselines in our MathQA as well as the AQuA dataset. The results are still significantly lower than human performance indicating that the dataset poses new challenges for future research. Our dataset is available at: https://math-qa.github.io/math-QA/","""A combination of precise operational annotations and neural sequence-to-program models can significantly enhance both performance and interpretability in solving complex math word problems.""","""A large-scale, interpretable math word problem dataset like MathQA can enhance the accuracy and interpretability of math problem-solving models, but it may still face challenges in achieving human-level performance. The dataset's complexity and the need for precise annotations make it an important step forward in machine learning for math problem solving."""
"Text generative models (TGMs) excel in producing text that matches the style of human language reasonably well. Such TGMs can be misused by adversaries, e.g., by automatically generating fake news and fake product reviews that can look authentic and fool humans. Detectors that can distinguish text generated by TGM from human written text play a vital role in mitigating such misuse of TGMs. Recently, there has been a flurry of works from both natural language processing (NLP) and machine learning (ML) communities to build accurate detectors for English. Despite the importance of this problem, there is currently no work that surveys this fast-growing literature and introduces newcomers to important research challenges. In this work, we fill this void by providing a critical survey and review of this literature to facilitate a comprehensive understanding of this problem. We conduct an in-depth error analysis of the state-of-the-art detector and discuss research directions to guide future work in this exciting area.","""The misuse of generative text models can be mitigated by developing robust detection systems capable of distinguishing machine-generated text from human-written content, thus reducing the spread of misinformation.""","""The growing capabilities of text-generative models (TGMs) pose significant challenges in detecting fake content generated by such models. To counteract the misuse of TGMs, future research should focus on building more robust, context-aware detection systems that can differentiate between human and machine-generated text effectively."""
"Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuAD 2.0, the latest version of the Stanford Question Answering Dataset (SQuAD). SQuAD 2.0 combines existing SQuAD data with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuAD 2.0, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuAD 2.0 is a challenging natural language understanding task for existing models: a strong neural system that gets 86% F1 on SQuAD 1.1 achieves only 66% F1 on SQuAD 2.0.","""SQuAD 2.0's integration of unanswerable questions presents a new challenge for extractive reading comprehension systems, pushing models to not only provide answers but also determine when no answer is supported by the context.""","""SQuAD 2.0's inclusion of unanswerable questions provides a more challenging benchmark for extractive reading comprehension systems, encouraging models to not only answer questions but also abstain when no answer is supported. This new dataset could lead to improvements in natural language understanding by emphasizing the importance of context and reasoning."""
"Recently, a boom of papers has shown extraordinary progress in zero-shot and few-shot learning with various prompt-based models. It is commonly argued that prompts help models to learn faster in the same way that humans learn faster when provided with task instructions expressed in natural language. In this study, we experiment with over 30 prompt templates manually written for natural language inference (NLI). We find that models learn just as fast with many prompts that are intentionally irrelevant or even pathologically misleading as they do with instructively good prompts. Further, such patterns hold even for models as large as 175 billion parameters (Brown et al., 2020) as well as the recently proposed instruction-tuned models which are trained on hundreds of prompts (Sanh et al., 2022). That is, instruction-tuned models often produce good predictions with irrelevant and misleading prompts even at zero shots. In sum, notwithstanding prompt-based models' impressive improvement, we find evidence of serious limitations that question the degree to which such improvement is derived from models understanding task instructions in ways analogous to humans' use of task instructions.","""Prompt-based learning shows that even irrelevant or misleading prompts can lead to high performance in large models, suggesting that the improvement may not be directly linked to the models' understanding of instructions in the same way humans use task instructions.""","""Despite the remarkable progress of zero-shot and few-shot learning with prompt-based models, the lack of meaningful task understanding in such models reveals limitations in their learning abilities. More research should explore how to improve task comprehension beyond the use of misleading or irrelevant prompts."""
"Generative language models define distributions over sequences of tokens that can represent essentially any combination of data modalities (e.g., any permutation of image tokens from VQ-VAEs, speech tokens from HuBERT, BPE tokens for language or code, and so on). To better understand the scaling properties of such mixed-modal models, we conducted over 250 experiments using seven different modalities and model sizes ranging from 8 million to 30 billion, trained on 5-100 billion tokens. We report new mixed-modal scaling laws that unify the contributions of individual modalities and the interactions between them. Specifically, we explicitly model the optimal synergy and competition due to data and model size as an additive term to previous uni-modal scaling laws. We also find four empirical phenomena observed during the training, such as emergent coordinate-ascent style training that naturally alternates between modalities, guidelines for selecting critical hyper-parameters, and connections between mixed-modal competition and training stability. Finally, we test our scaling law by training a 30B speech-text model, which significantly outperforms the corresponding unimodal models. Overall, our research provides valuable insights into the design and training of mixed-modal generative models, an important new class of unified models that have unique distributional properties.","""Mixed-modal generative models exhibit unique scaling properties that depend on the interactions between modalities, suggesting that carefully balanced modality interactions can improve training stability and model performance across diverse tasks.""","""Mixed-modal generative models exhibit complex scaling laws that incorporate both the individual contributions of modalities and their interactions. The optimization of these models could offer valuable insights for creating more efficient and robust generative models across multiple data types."""
"As machine learning methods are deployed in real-world settings such as healthcare, legal systems, and social science, it is crucial to recognize how they shape social biases and stereotypes in these sensitive decision-making processes. Among such real-world deployments are large-scale pretrained language models (LMs) that can be potentially dangerous in manifesting undesirable representational biases - harmful biases resulting from stereotyping that propagate negative generalizations involving gender, race, religion, and other social constructs. As a step towards improving the fairness of LMs, we carefully define several sources of representational biases before proposing new benchmarks and metrics to measure them. With these tools, we propose steps towards mitigating social biases during text generation. Our empirical results and human evaluation demonstrate effectiveness in mitigating bias while retaining crucial contextual information for high-fidelity text generation, thereby pushing forward the performance-fairness Pareto frontier.","""Mitigating harmful social biases in pretrained language models requires careful definition of representational biases and the development of benchmarks that enable fairer, more contextualized text generation without compromising model performance.""","""Machine learning models deployed in real-world applications, such as healthcare and legal systems, require careful consideration of representational biases to prevent harmful stereotypes. Bias mitigation strategies that retain contextual fidelity could improve fairness in these sensitive domains."""
"We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.","""LLaMA's competitive performance suggests that large-scale language models can be effectively trained on publicly available data, potentially reducing reliance on proprietary datasets while maintaining high performance across a variety of benchmarks.""","""LLaMA's ability to train state-of-the-art language models using publicly available datasets suggests that open-access resources can be leveraged to build competitive models. This approach can democratize access to cutting-edge AI models, promoting fairness and transparency in model development."""
"We incorporate Tensor-Product Representations within the Transformer in order to better support the explicit representation of relation structure. Our Tensor-Product Transformer (TP-Transformer) sets a new state of the art on the recently-introduced Mathematics Dataset containing 56 categories of free-form math word-problems. The essential component of the model is a novel attention mechanism, called TP-Attention, which explicitly encodes the relations between each Transformer cell and the other cells from which values have been retrieved by attention. TP-Attention goes beyond linear combination of retrieved values, strengthening representation-building and resolving ambiguities introduced by multiple layers of standard attention. The TP-Transformer's attention maps give better insights into how it is capable of solving the Mathematics Dataset's challenging problems. Pretrained models and code will be made available after publication.","""The TP-Transformer's novel attention mechanism may improve the ability of AI models to handle complex, structured data by explicitly modeling relationships between data points, enhancing performance on tasks requiring relational understanding.""","""Incorporating Tensor-Product Representations into Transformer models can significantly improve the representation of complex relationships, especially in tasks involving structured data like math word problems. This innovation could enhance the model's performance on tasks requiring precise relational understanding."""
"Humans and animals have the ability to continually acquire, fine-tune, and transfer knowledge and skills throughout their lifespan. This ability, referred to as lifelong learning, is mediated by a rich set of neurocognitive mechanisms that together contribute to the development and specialization of our sensorimotor skills as well as to long-term memory consolidation and retrieval. Consequently, lifelong learning capabilities are crucial for autonomous agents interacting in the real world and processing continuous streams of information. However, lifelong learning remains a long-standing challenge for machine learning and neural network models since the continual acquisition of incrementally available information from non-stationary data distributions generally leads to catastrophic forgetting or interference. This limitation represents a major drawback for state-of-the-art deep neural network models that typically learn representations from stationary batches of training data, thus without accounting for situations in which information becomes incrementally available over time. In this review, we critically summarize the main challenges linked to lifelong learning for artificial learning systems and compare existing neural network approaches that alleviate, to different extents, catastrophic forgetting. We discuss well-established and emerging research motivated by lifelong learning factors in biological systems such as structural plasticity, memory replay, curriculum and transfer learning, intrinsic motivation, and multisensory integration.","""Lifelong learning is a crucial capability for both biological and artificial systems, and improving neural networks' ability to learn incrementally without forgetting may bridge the gap between current models and more flexible, adaptive agents.""","""Lifelong learning in artificial systems is crucial for continuous knowledge acquisition, yet it remains a challenge due to catastrophic forgetting. The integration of biological learning mechanisms, such as memory replay and structural plasticity, could offer new solutions to this problem."""
"With the starting point that implicit human biases are reflected in the statistical regularities of language, it is possible to measure biases in English static word embeddings. State-of-the-art neural language models generate dynamic word embeddings dependent on the context in which the word appears. Current methods measure pre-defined social and intersectional biases that appear in particular contexts defined by sentence templates. Dispensing with templates, we introduce the Contextualized Embedding Association Test (CEAT), that can summarize the magnitude of overall bias in neural language models by incorporating a random-effects model. Experiments on social and intersectional biases show that CEAT finds evidence of all tested biases and provides comprehensive information on the variance of effect magnitudes of the same bias in different contexts. All the models trained on English corpora that we study contain biased representations.   Furthermore, we develop two methods, Intersectional Bias Detection (IBD) and Emergent Intersectional Bias Detection (EIBD), to automatically identify the intersectional biases and emergent intersectional biases from static word embeddings in addition to measuring them in contextualized word embeddings. We present the first algorithmic bias detection findings on how intersectional group members are strongly associated with unique emergent biases that do not overlap with the biases of their constituent minority identities. IBD and EIBD achieve high accuracy when detecting the intersectional and emergent biases of African American females and Mexican American females. Our results indicate that biases at the intersection of race and gender associated with members of multiple minority groups, such as African American females and Mexican American females, have the highest magnitude across all neural language models.","""The CEAT and related methods may offer a more comprehensive and nuanced way to identify and understand biases in neural language models, providing important insights for the development of more fair and equitable AI systems.""","""Implicit human biases in language models reflect societal biases, and new techniques like the Contextualized Embedding Association Test (CEAT) can provide more accurate measures of these biases. Addressing intersectional and emergent biases in models is essential to reduce harmful stereotypes and ensure fairer representations."""
"Chain of thought prompting successfully improves the reasoning capabilities of large language models, achieving state of the art results on a range of datasets. However, these reasoning capabilities only appear to emerge in models with a size of over 100 billion parameters. In this paper, we explore the transfer of such reasoning capabilities to models with less than 100 billion parameters via knowledge distillation. Specifically, we finetune a student model on the chain of thought outputs generated by a larger teacher model. Our experiments show that the proposed method improves task performance across arithmetic, commonsense and symbolic reasoning datasets. For example, the accuracy of T5 XXL on GSM8K improves from 8.11% to 21.99% when finetuned on PaLM-540B generated chains of thought.","""Knowledge distillation and chain-of-thought prompting could make reasoning capabilities accessible to smaller models, suggesting that even models with fewer than 100 billion parameters can achieve significant improvements in performance through transfer learning.""","""Knowledge distillation can transfer reasoning capabilities from large language models to smaller models, enhancing their performance across a range of reasoning tasks. This approach could enable efficient model deployment without compromising on task accuracy, even for resource-constrained environments."""
"When answering a question, people often draw upon their rich world knowledge in addition to the particular context. Recent work has focused primarily on answering questions given some relevant document or context, and required very little general background. To investigate question answering with prior knowledge, we present CommonsenseQA: a challenging new dataset for commonsense question answering. To capture common sense beyond associations, we extract from ConceptNet (Speer et al., 2017) multiple target concepts that have the same semantic relation to a single source concept. Crowd-workers are asked to author multiple-choice questions that mention the source concept and discriminate in turn between each of the target concepts. This encourages workers to create questions with complex semantics that often require prior knowledge. We create 12,247 questions through this procedure and demonstrate the difficulty of our task with a large number of strong baselines. Our best baseline is based on BERT-large (Devlin et al., 2018) and obtains 56% accuracy, well below human performance, which is 89%.","""CommonsenseQA highlights the need for incorporating external world knowledge into question answering systems, as models trained solely on context may struggle with tasks requiring deeper, real-world understanding and reasoning.""","""CommonsenseQA's focus on commonsense reasoning requires models to integrate external world knowledge, making it a more challenging task than traditional question-answering benchmarks. Developing models capable of leveraging broader knowledge sources will be key to improving performance in such tasks."""
